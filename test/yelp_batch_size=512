     ┌────────────────────────────────────────────────────────────────────┐
     │                        • MobaXterm 20.3 •                          │
     │            (SSH client, X-server and networking tools)             │
     │                                                                    │
     │ ➤ SSH session to root@region-3.autodl.com                          │
     │   • SSH compression : ✔                                            │
     │   • SSH-browser     : ✔                                            │
     │   • X11-forwarding  : ✘  (disabled or not supported by server)     │
     │   • DISPLAY         : 192.168.1.107:0.0                            │
     │                                                                    │
     │ ➤ For more info, ctrl+click on help or visit our website           │
     └────────────────────────────────────────────────────────────────────┘

Welcome to Ubuntu 18.04.6 LTS (GNU/Linux 5.4.0-96-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
This system has been minimized by removing packages and content that are
not required on a system that users do not log into.

To restore this content, you can run the 'unminimize' command.
Last login: Thu Jun  2 11:48:34 2022 from 127.0.0.1
+--------------------------------------------------AutoDL-------------------------------------------------------
目录说明:
╔═════════════════╦══════╦════╦═════════════════════════════════════════════════════════════════════════╗
║目录             ║名称  ║速度║说明                                                                     ║
╠═════════════════╬══════╬════╬═════════════════════════════════════════════════════════════════════════╣
║/                ║系统盘║快  ║实例关机数据不会丢失，可存放代码等。会随保存镜像一起保存。               ║
║/root/autodl-tmp ║数据盘║快  ║实例关机数据不会丢失，可存放读写IO要求高的数据。但不会随保存镜像一起保存 ║
╚═════════════════╩══════╩════╩═════════════════════════════════════════════════════════════════════════╝
CPU ：7 核心
内存：16 GB
GPU ：NVIDIA TITAN Xp, 1
存储：
  系统盘/               ：11% 2.2G/20G
  数据盘/root/autodl-tmp：0% 0/50G
+---------------------------------------------------------------------------------------------------------------
*注意:
1.系统盘较小请将大的数据存放于数据盘或网盘中，重置系统时数据盘和网盘中的数据不受影响
2.清理系统盘请参考：https://www.autodl.com/docs/qa/
root@container-f87d1190ac-f968e13b:~#
root@container-f87d1190ac-f968e13b:~#
root@container-f87d1190ac-f968e13b:~#
root@container-f87d1190ac-f968e13b:~#
root@container-f87d1190ac-f968e13b:~# cd ./CLSR
root@container-f87d1190ac-f968e13b:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2 --temp 0ath yelp
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (ynonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (ynonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (ynonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (ynonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (ynonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (ynonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Pa as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Pa as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Pa as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Pa as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Pa as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Pa as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto i

2022-06-06 13:53:14.651361: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-06 13:53:15.182095: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:276: The name tf.placeholder is deprecated. Please use tf.compat.v1.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.cead.

drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("Spars335168,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prbe removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("Spars335168,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("Spape=(177378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("Spape=(177378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("Spape=(156718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("Spape=(156718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("Spape=(156674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("Spape=(156674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("Spape=(159442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("Spape=(159442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("Spape=(155622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("Spape=(155622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("Spape=(141690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("Spape=(141690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("Spare=(85108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("Spare=(85108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecata future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calt__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1b36258rmed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosit AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflowobject at 0x7f1b362582d0>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f1b35ffcrmed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosit AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflowobject at 0x7f1b35ffcad0>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
Tensor("ExpandDims:0", shape=(1, 40), dtype=int32)
Tensor("Tile:0", shape=(1024, 40), dtype=int32)
Tensor("Const_16:0", shape=(1024,), dtype=int32)
1 (1024, 40, 64)
WARNING:tensorflow:From /root/CLSR/model.py:325: The name tf.train.exponential_decay is deprecated. Please use tial_decay instead.

WARNING:tensorflow:From /root/CLSR/model.py:326: The name tf.train.AdamOptimizer is deprecated. Please use tf.cor instead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: ad>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-06 13:53:39.365738: Model Prepared
2022-06-06 13:53:43.987736: Variables Inited
Traceback (most recent call last):
  File "main.py", line 25, in <module>
    recom.run()
  File "/root/CLSR/model.py", line 50, in run
    reses = self.trainEpoch()
  File "/root/CLSR/model.py", line 429, in trainEpoch
    uLocs, iLocs, timeLocs = self.sampleTrainBatch(batIds, self.handler.trnMat, self.handler.timeMat)
  File "/root/CLSR/model.py", line 348, in sampleTrainBatch
    neglocs = negSamp(temLabel[i], sampNum, args.item)
TypeError: negSamp() missing 1 required positional argument: 'trnPos'
root@container-f87d1190ac-f968e13b:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2 --temp 0ath yelp
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (ynonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (ynonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (ynonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (ynonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (ynonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (ynonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Pa as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Pa as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Pa as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Pa as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Pa as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Pa as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto i

2022-06-06 14:00:27.496274: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-06 14:00:27.854507: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:241: The name tf.placeholder is deprecated. Please use tf.compat.v1.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.cead.

drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("Spars335168,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prbe removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("Spars335168,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("Spape=(177378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("Spape=(177378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("Spape=(156718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("Spape=(156718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("Spape=(156674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("Spape=(156674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("Spape=(159442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("Spape=(159442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("Spape=(155622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("Spape=(155622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("Spape=(141690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("Spape=(141690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("Spare=(85108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("Spare=(85108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecata future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calt__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0323cb7rmed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosit AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflowobject at 0x7f0323cb7b90>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f0323c40rmed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosit AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflowobject at 0x7f0323c40b50>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.exponential_decay is deprecated. Please use tial_decay instead.

WARNING:tensorflow:From /root/CLSR/model.py:287: The name tf.train.AdamOptimizer is deprecated. Please use tf.cor instead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: ad>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-06 14:00:50.833717: Model Prepared
2022-06-06 14:00:53.851146: Variables Inited
2022-06-06 14:01:49.670611: Epoch 0/100, Train: Loss = 7.0886, preLoss = 2.6884
[ 0.          3.6485817   5.9280906  ... 17.602644    9.07159223
 -0.33734012]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0299 0.0299 0.07116866164187781 0.1125 0.1276
2022-06-06 14:02:41.916135: Epoch 0/100, Test: HR = 0.1876, NDCG = 0.0952
WARNING:tensorflow:From /root/CLSR/model.py:524: The name tf.train.Saver is deprecated. Please use tf.compat.v1.

2022-06-06 14:02:44.239010: Model Saved: yelp

2022-06-06 14:03:28.978770: Epoch 1/100, Train: Loss = 13.9475, preLoss = 4.4311

2022-06-06 14:04:12.986525: Epoch 2/100, Train: Loss = 19.0820, preLoss = 5.4621

2022-06-06 14:04:58.342024: Epoch 3/100, Train: Loss = 21.2050, preLoss = 4.8590
[ 0.        4.979756 31.74337  ... 15.97551   6.959296  3.027275]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0521 0.0521 0.11342991488291577 0.1736 0.1838
2022-06-06 14:05:52.103800: Epoch 3/100, Test: HR = 0.2725, NDCG = 0.1451
2022-06-06 14:05:53.620721: Model Saved: yelp

2022-06-06 14:06:40.027647: Epoch 4/100, Train: Loss = 21.3425, preLoss = 4.1017

2022-06-06 14:07:24.164117: Epoch 5/100, Train: Loss = 20.2451, preLoss = 3.3637

2022-06-06 14:08:08.784212: Epoch 6/100, Train: Loss = 18.6421, preLoss = 2.8089
[ 0.        19.840725  12.933086  ...  1.4720955 -1.9969134  4.8128357]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.07 0.07 0.14208929302895956 0.2139 0.21676565
2022-06-06 14:09:01.146973: Epoch 6/100, Test: HR = 0.3213, NDCG = 0.1766
2022-06-06 14:09:02.673230: Model Saved: yelp

2022-06-06 14:09:46.326894: Epoch 7/100, Train: Loss = 16.6819, preLoss = 2.2766

2022-06-06 14:10:29.808224: Epoch 8/100, Train: Loss = 14.5985, preLoss = 1.7838

2022-06-06 14:11:13.588190: Epoch 9/100, Train: Loss = 12.6198, preLoss = 1.4279
[ 0.         7.336231  17.838776  ...  8.466014  -1.5491527  1.6366636]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0805 0.0805 0.15515900952892242 0.2277 0.2331
2022-06-06 14:12:04.278614: Epoch 9/100, Test: HR = 0.3481, NDCG = 0.1940
2022-06-06 14:12:05.716114: Model Saved: yelp

2022-06-06 14:12:50.560820: Epoch 10/100, Train: Loss = 10.7509, preLoss = 1.1114

2022-06-06 14:13:35.029057: Epoch 11/100, Train: Loss = 9.0476, preLoss = 0.8748

2022-06-06 14:14:19.937732: Epoch 12/100, Train: Loss = 7.6158, preLoss = 0.7166
[ 0.          2.7715373   5.0342913  ... 10.650671   -0.351869525
  1.9575748 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0697 0.0697 0.15751637967873283 0.2443 0.2396
2022-06-06 14:15:11.883224: Epoch 12/100, Test: HR = 0.3715, NDCG = 0.1984
2022-06-06 14:15:13.385414: Model Saved: yelp

2022-06-06 14:15:57.251424: Epoch 13/100, Train: Loss = 6.3719, preLoss = 0.5607

2022-06-06 14:16:42.303623: Epoch 14/100, Train: Loss = 5.3638, preLoss = 0.4647

2022-06-06 14:17:26.561620: Epoch 15/100, Train: Loss = 4.5217, preLoss = 0.3837
[ 0.          1.0390886  -0.33281446 ... -1.4809139  -1.264563 51
  3.5538568 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0776 0.0776 0.16769837609413585 0.2557 0.2530
2022-06-06 14:18:18.894740: Epoch 15/100, Test: HR = 0.3879, NDCG = 0.2102
2022-06-06 14:18:20.371119: Model Saved: yelp

2022-06-06 14:19:04.593041: Epoch 16/100, Train: Loss = 3.8320, preLoss = 0.3198

2022-06-06 14:19:50.315245: Epoch 17/100, Train: Loss = 3.2864, preLoss = 0.2863

2022-06-06 14:20:36.207240: Epoch 18/100, Train: Loss = 2.8233, preLoss = 0.2421
[ 0.         -0.01498884  7.3230305  ...  1.2738894  -0.205962069
  0.5373045 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.081 0.081 0.17129254089000584 0.2629 0.261914
2022-06-06 14:21:27.175555: Epoch 18/100, Test: HR = 0.4084, NDCG = 0.2181
2022-06-06 14:21:28.709371: Model Saved: yelp

2022-06-06 14:22:13.498062: Epoch 19/100, Train: Loss = 2.4552, preLoss = 0.2184

2022-06-06 14:22:59.547582: Epoch 20/100, Train: Loss = 2.1607, preLoss = 0.1990

2022-06-06 14:23:44.579186: Epoch 21/100, Train: Loss = 1.9046, preLoss = 0.1752
[ 0.         -0.2195468   4.750978   ...  0.66770744 -0.092824275
  0.37485164]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0914 0.0914 0.18411686576827727 0.2751 0.2766
2022-06-06 14:24:38.010254: Epoch 21/100, Test: HR = 0.4213, NDCG = 0.2314
2022-06-06 14:24:39.494724: Model Saved: yelp

2022-06-06 14:25:24.931179: Epoch 22/100, Train: Loss = 1.7070, preLoss = 0.1681

2022-06-06 14:26:11.124077: Epoch 23/100, Train: Loss = 1.5405, preLoss = 0.1575

2022-06-06 14:26:56.485532: Epoch 24/100, Train: Loss = 1.3971, preLoss = 0.1459
[ 0.         -0.39359474  1.9549141  ... -3.1132607  -1.560487456
  1.2154241 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0954 0.0954 0.20001076830686879 0.3046 0.2939
2022-06-06 14:27:48.637975: Epoch 24/100, Test: HR = 0.4553, NDCG = 0.2487
2022-06-06 14:27:50.208744: Model Saved: yelp

2022-06-06 14:28:35.945327: Epoch 25/100, Train: Loss = 1.2787, preLoss = 0.1403

2022-06-06 14:29:19.459165: Epoch 26/100, Train: Loss = 1.1797, preLoss = 0.1389

2022-06-06 14:30:03.590342: Epoch 27/100, Train: Loss = 1.1003, preLoss = 0.1369
[ 0.          0.3467664   0.85114133 ... -0.8444643  -0.313406530
  0.43935478]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0992 0.0992 0.20941854749353725 0.3136 0.3038
2022-06-06 14:30:54.632384: Epoch 27/100, Test: HR = 0.4646, NDCG = 0.2582
2022-06-06 14:30:56.218973: Model Saved: yelp

2022-06-06 14:31:41.517873: Epoch 28/100, Train: Loss = 1.0339, preLoss = 0.1368

2022-06-06 14:32:29.140676: Epoch 29/100, Train: Loss = 0.9724, preLoss = 0.1290

2022-06-06 14:33:14.899280: Epoch 30/100, Train: Loss = 0.9167, preLoss = 0.1281
[ 0.         -0.40194947  1.0113233  ... -0.57080877 -0.266714548
 -0.03782481]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0788 0.0788 0.21095422550006965 0.33 0.303498
2022-06-06 14:34:05.822380: Epoch 30/100, Test: HR = 0.4876, NDCG = 0.2618
2022-06-06 14:34:07.353770: Model Saved: yelp

2022-06-06 14:34:51.584563: Epoch 31/100, Train: Loss = 0.8734, preLoss = 0.1268

2022-06-06 14:35:36.848513: Epoch 32/100, Train: Loss = 0.8304, preLoss = 0.1233

2022-06-06 14:36:21.660131: Epoch 33/100, Train: Loss = 0.7949, preLoss = 0.1237
[ 0.         -0.40412015  0.77966845 ... -7.5919666   0.043631242
 -0.41132632]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1082 0.1082 0.23217779648608308 0.3506 0.3202
2022-06-06 14:37:12.224891: Epoch 33/100, Test: HR = 0.4998, NDCG = 0.2805
2022-06-06 14:37:13.852842: Model Saved: yelp

2022-06-06 14:37:57.839480: Epoch 34/100, Train: Loss = 0.7611, preLoss = 0.1218

2022-06-06 14:38:43.674005: Epoch 35/100, Train: Loss = 0.7293, preLoss = 0.1186

2022-06-06 14:39:28.433402: Epoch 36/100, Train: Loss = 0.6984, preLoss = 0.1173
[ 0.          0.43066308  0.81650245 ... -3.3133073  -0.174949943
  0.30273625]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.084 0.084 0.21681996004317625 0.3499 0.312893
2022-06-06 14:40:19.618568: Epoch 36/100, Test: HR = 0.5119, NDCG = 0.2692
2022-06-06 14:40:21.194239: Model Saved: yelp

2022-06-06 14:41:06.563888: Epoch 37/100, Train: Loss = 0.6743, preLoss = 0.1167

2022-06-06 14:41:52.676782: Epoch 38/100, Train: Loss = 0.6554, preLoss = 0.1189

2022-06-06 14:42:36.260662: Epoch 39/100, Train: Loss = 0.6327, preLoss = 0.1163
[ 0.          0.53308636  0.6092953  ... -1.1670417  -0.061152124
  0.33892816]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1036 0.1036 0.21890764662571682 0.3393 0.3150
2022-06-06 14:43:26.341107: Epoch 39/100, Test: HR = 0.5056, NDCG = 0.2730
2022-06-06 14:43:27.966356: Model Saved: yelp

2022-06-06 14:44:12.878923: Epoch 40/100, Train: Loss = 0.6117, preLoss = 0.1146

2022-06-06 14:44:57.836116: Epoch 41/100, Train: Loss = 0.5935, preLoss = 0.1126

2022-06-06 14:45:42.355293: Epoch 42/100, Train: Loss = 0.5759, preLoss = 0.1111
[ 0.          0.35780424  0.2884205  ... -0.8021029  -0.000964174
  0.20941465]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1317 0.1317 0.24596811969886148 0.3508 0.3417
2022-06-06 14:46:36.187321: Epoch 42/100, Test: HR = 0.5116, NDCG = 0.2982
2022-06-06 14:46:37.842951: Model Saved: yelp

2022-06-06 14:47:21.663813: Epoch 43/100, Train: Loss = 0.5581, preLoss = 0.1080

2022-06-06 14:48:06.130785: Epoch 44/100, Train: Loss = 0.5440, preLoss = 0.1073

2022-06-06 14:48:50.585135: Epoch 45/100, Train: Loss = 0.5294, preLoss = 0.1082
[ 0.         -0.10816189  0.45910138 ...  3.3999677  -0.308772746
  1.1201653 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1288 0.1288 0.25020165002460665 0.3653 0.3419
2022-06-06 14:49:43.982269: Epoch 45/100, Test: HR = 0.5264, NDCG = 0.3024
2022-06-06 14:49:45.761976: Model Saved: yelp

2022-06-06 14:50:31.398576: Epoch 46/100, Train: Loss = 0.5124, preLoss = 0.1042

2022-06-06 14:51:16.012274: Epoch 47/100, Train: Loss = 0.5063, preLoss = 0.1078

2022-06-06 14:52:00.836931: Epoch 48/100, Train: Loss = 0.4896, preLoss = 0.1027
[ 0.          0.02776334  0.3741566  ...  1.8649111  -0.348909056
  0.38519588]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1355 0.1355 0.25561034433041163 0.3697 0.3462
2022-06-06 14:52:54.524732: Epoch 48/100, Test: HR = 0.5258, NDCG = 0.3067
2022-06-06 14:52:56.392319: Model Saved: yelp

2022-06-06 14:53:41.764750: Epoch 49/100, Train: Loss = 0.4806, preLoss = 0.1048

2022-06-06 14:54:27.081068: Epoch 50/100, Train: Loss = 0.4730, preLoss = 0.1046

2022-06-06 14:55:11.235762: Epoch 51/100, Train: Loss = 0.4656, preLoss = 0.1045
[ 0.          0.61724055 -0.08155754 ...  1.4284858  -0.52579= 76
  0.8050425 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1429 0.1429 0.2619710829612781 0.3716 0.35477
2022-06-06 14:56:02.786774: Epoch 51/100, Test: HR = 0.5287, NDCG = 0.3132
2022-06-06 14:56:04.614526: Model Saved: yelp

2022-06-06 14:56:51.125883: Epoch 52/100, Train: Loss = 0.4545, preLoss = 0.1026

2022-06-06 14:57:35.506910: Epoch 53/100, Train: Loss = 0.4435, preLoss = 0.0989

2022-06-06 14:58:19.464110: Epoch 54/100, Train: Loss = 0.4334, preLoss = 0.0977
[ 0.          0.27156922 -0.45866507 ...  1.1839929  -0.310975373
  0.5863451 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1288 0.1288 0.2581066734742133 0.378 0.351356
2022-06-06 14:59:08.822457: Epoch 54/100, Test: HR = 0.5355, NDCG = 0.3092
2022-06-06 14:59:10.491343: Model Saved: yelp

2022-06-06 14:59:57.567075: Epoch 55/100, Train: Loss = 0.4289, preLoss = 0.1000

2022-06-06 15:00:42.801084: Epoch 56/100, Train: Loss = 0.4234, preLoss = 0.1005

2022-06-06 15:01:27.387023: Epoch 57/100, Train: Loss = 0.4108, preLoss = 0.0952
[ 0.          0.05630382 -0.4021105  ...  0.8180877   0.074454253
  0.4602586 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1737 0.1737 0.2858903504602097 0.3922 0.37831
2022-06-06 15:02:18.032219: Epoch 57/100, Test: HR = 0.5355, NDCG = 0.3326
2022-06-06 15:02:19.746153: Model Saved: yelp

2022-06-06 15:03:05.817239: Epoch 58/100, Train: Loss = 0.4063, preLoss = 0.0969

2022-06-06 15:03:51.532063: Epoch 59/100, Train: Loss = 0.3996, preLoss = 0.0964

2022-06-06 15:04:35.505447: Epoch 60/100, Train: Loss = 0.3941, preLoss = 0.0961
[ 0.          0.05887131 -0.21517037 ...  1.3406681   0.032166873
  0.16321269]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1813 0.1813 0.2921000296573392 0.401         4 0.3839218293579946 0.7215
2022-06-06 15:05:25.047686: Epoch 60/100, Test: HR = 0.5515, NDCG = 0.3413
2022-06-06 15:05:26.666487: Model Saved: yelp

2022-06-06 15:06:10.298614: Epoch 61/100, Train: Loss = 0.3861, preLoss = 0.0929

2022-06-06 15:06:55.603391: Epoch 62/100, Train: Loss = 0.3808, preLoss = 0.0932

2022-06-06 15:07:39.959711: Epoch 63/100, Train: Loss = 0.3752, preLoss = 0.0928
[ 0.          0.41477868 -0.2963768  ...  2.5872736  -0.8342268501
  0.5427644 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1881 0.1881 0.2972961144081813 0.405         4 0.3861096901813451 0.713
2022-06-06 15:08:31.472817: Epoch 63/100, Test: HR = 0.5555, NDCG = 0.3465
2022-06-06 15:08:33.041742: Model Saved: yelp

2022-06-06 15:09:17.268670: Epoch 64/100, Train: Loss = 0.3690, preLoss = 0.0917

2022-06-06 15:10:03.685962: Epoch 65/100, Train: Loss = 0.3660, preLoss = 0.0917

2022-06-06 15:10:49.230102: Epoch 66/100, Train: Loss = 0.3571, preLoss = 0.0880
[ 0.          0.19421706 -0.50767696 ...  2.063897   -0.713065158
  0.46151364]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1629 0.1629 0.28182163915331926 0.39         36 0.37535010042880024 0.7155
2022-06-06 15:11:42.850053: Epoch 66/100, Test: HR = 0.5586, NDCG = 0.3359
2022-06-06 15:11:44.621473: Model Saved: yelp

2022-06-06 15:12:29.933041: Epoch 67/100, Train: Loss = 0.3532, preLoss = 0.0885

2022-06-06 15:13:14.636684: Epoch 68/100, Train: Loss = 0.3469, preLoss = 0.0863

2022-06-06 15:13:58.323659: Epoch 69/100, Train: Loss = 0.3463, preLoss = 0.0881
[ 0.          0.3026506  -0.48499215 ...  0.8075991  -0.570798278
  0.20742565]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1569 0.1569 0.27570781266977995 0.39         59 0.372393004295314 0.7306
2022-06-06 15:14:48.264477: Epoch 69/100, Test: HR = 0.5613, NDCG = 0.3297
2022-06-06 15:14:49.968463: Model Saved: yelp

2022-06-06 15:15:36.214010: Epoch 70/100, Train: Loss = 0.3416, preLoss = 0.0872

2022-06-06 15:16:22.288165: Epoch 71/100, Train: Loss = 0.3373, preLoss = 0.0857

2022-06-06 15:17:07.619218: Epoch 72/100, Train: Loss = 0.3323, preLoss = 0.0856
[ 0.          0.3972773  -0.4387932  ...  0.82189214 -0.521471482
  0.4290715 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1427 0.1427 0.2729962381725726 0.390         7 0.36980422410520347 0.7241
2022-06-06 15:18:00.252402: Epoch 72/100, Test: HR = 0.5591, NDCG = 0.3282
2022-06-06 15:18:02.024407: Model Saved: yelp

2022-06-06 15:18:47.376385: Epoch 73/100, Train: Loss = 0.3292, preLoss = 0.0861

2022-06-06 15:19:32.368340: Epoch 74/100, Train: Loss = 0.3246, preLoss = 0.0840

2022-06-06 15:20:16.813832: Epoch 75/100, Train: Loss = 0.3212, preLoss = 0.0844
[ 0.          0.46385238 -0.34475875 ...  0.9611732  -0.555283982
  0.32334676]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1544 0.1544 0.2841921177768625 0.399         5 0.3793686640143212 0.726
2022-06-06 15:21:07.897192: Epoch 75/100, Test: HR = 0.5652, NDCG = 0.3388
2022-06-06 15:21:09.585131: Model Saved: yelp

2022-06-06 15:21:53.491587: Epoch 76/100, Train: Loss = 0.3196, preLoss = 0.0845

2022-06-06 15:22:39.585082: Epoch 77/100, Train: Loss = 0.3162, preLoss = 0.0835

2022-06-06 15:23:25.430949: Epoch 78/100, Train: Loss = 0.3125, preLoss = 0.0828
[ 0.          0.619164   -0.5423606  ... -0.16157782 -0.3508759720
  0.30761904]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1668 0.1668 0.2936180596273035 0.412         2 0.38501088236124276 0.7279
2022-06-06 15:24:17.024333: Epoch 78/100, Test: HR = 0.5698, NDCG = 0.3453
2022-06-06 15:24:18.992143: Model Saved: yelp

2022-06-06 15:25:04.861270: Epoch 79/100, Train: Loss = 0.3086, preLoss = 0.0818

2022-06-06 15:25:50.350566: Epoch 80/100, Train: Loss = 0.3055, preLoss = 0.0818

2022-06-06 15:26:35.354674: Epoch 81/100, Train: Loss = 0.3029, preLoss = 0.0805
[ 0.          0.2425921  -0.5645291  ... -0.43218946 -0.4499646410
  0.2337338 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.173 0.173 0.2901023922901487 0.4064          0.38495860710324376 0.7299
2022-06-06 15:27:28.558116: Epoch 81/100, Test: HR = 0.5796, NDCG = 0.3471
2022-06-06 15:27:30.333249: Model Saved: yelp

2022-06-06 15:28:15.369167: Epoch 82/100, Train: Loss = 0.3024, preLoss = 0.0828

2022-06-06 15:29:00.067679: Epoch 83/100, Train: Loss = 0.2982, preLoss = 0.0797

2022-06-06 15:29:46.095138: Epoch 84/100, Train: Loss = 0.2982, preLoss = 0.0810
[ 0.          0.34766424 -0.46112514 ... -0.29820484 -0.5647448801
  0.33013058]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1658 0.1658 0.29130789569461046 0.41         28 0.385155208779626 0.7324
2022-06-06 15:30:38.622058: Epoch 84/100, Test: HR = 0.5810, NDCG = 0.3470
2022-06-06 15:30:40.330092: Model Saved: yelp

2022-06-06 15:31:25.662218: Epoch 85/100, Train: Loss = 0.2948, preLoss = 0.0806

2022-06-06 15:32:10.159734: Epoch 86/100, Train: Loss = 0.2930, preLoss = 0.0800

2022-06-06 15:32:56.968832: Epoch 87/100, Train: Loss = 0.2895, preLoss = 0.0791
[ 0.          0.35329312 -0.37959152 ...  0.3305403  -0.6747608804
  0.42626002]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.151 0.151 0.2897805956234139 0.423 0         .3792182357869118 0.7299
2022-06-06 15:33:46.931032: Epoch 87/100, Test: HR = 0.5819, NDCG = 0.3419
2022-06-06 15:33:48.713431: Model Saved: yelp

2022-06-06 15:34:31.697739: Epoch 88/100, Train: Loss = 0.2854, preLoss = 0.0777

2022-06-06 15:35:15.035886: Epoch 89/100, Train: Loss = 0.2835, preLoss = 0.0773

2022-06-06 15:36:00.656291: Epoch 90/100, Train: Loss = 0.2836, preLoss = 0.0780
[ 0.          0.3293394  -0.34611732 ...  0.46979517 -0.656505358
  0.4120311 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1299 0.1299 0.28031594067557875 0.42         08 0.3712235543226764 0.7318
2022-06-06 15:36:53.105835: Epoch 90/100, Test: HR = 0.5811, NDCG = 0.3331
2022-06-06 15:36:54.862929: Model Saved: yelp

2022-06-06 15:37:40.879411: Epoch 91/100, Train: Loss = 0.2803, preLoss = 0.0772

2022-06-06 15:38:26.742273: Epoch 92/100, Train: Loss = 0.2813, preLoss = 0.0798

2022-06-06 15:39:13.719655: Epoch 93/100, Train: Loss = 0.2754, preLoss = 0.0753
[ 0.          0.23659176 -0.40540636 ...  0.5150246  -0.568462279
  0.36768842]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.131 0.131 0.2757811878269743 0.4078          0.3722542990210389 0.7359
2022-06-06 15:40:06.316971: Epoch 93/100, Test: HR = 0.5800, NDCG = 0.3330
2022-06-06 15:40:08.149903: Model Saved: yelp

2022-06-06 15:40:53.232349: Epoch 94/100, Train: Loss = 0.2745, preLoss = 0.0764

2022-06-06 15:41:36.864649: Epoch 95/100, Train: Loss = 0.2758, preLoss = 0.0773

2022-06-06 15:42:21.329708: Epoch 96/100, Train: Loss = 0.2724, preLoss = 0.0755
[ 0.          0.22542243 -0.36524206 ...  0.7183559  -0.5546478802
  0.3133911 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1409 0.1409 0.2828236896724382 0.415         8 0.3759280990500352 0.7362
2022-06-06 15:43:12.103009: Epoch 96/100, Test: HR = 0.5780, NDCG = 0.3362
2022-06-06 15:43:13.862948: Model Saved: yelp

2022-06-06 15:43:58.152661: Epoch 97/100, Train: Loss = 0.2698, preLoss = 0.0746

2022-06-06 15:44:42.316721: Epoch 98/100, Train: Loss = 0.2696, preLoss = 0.0750

2022-06-06 15:45:27.157620: Epoch 99/100, Train: Loss = 0.2684, preLoss = 0.0757
[ 0.          0.10033578 -0.28398347 ...  0.7468872  -0.4899520601
  0.3524381 ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1448 0.1448 0.27533086577503774 0.39         39 0.3750095148075492 0.7317
2022-06-06 15:46:18.123415: Epoch 99/100, Test: HR = 0.5766, NDCG = 0.3361
2022-06-06 15:46:19.899127: Model Saved: yelp

[ 0.          0.10048185 -0.2841509  ...  0.74681526 -0.4894202801
  0.352775  ]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1448 0.1448 0.27530787758867475 0.39         38 0.37504574902510185 0.7318
2022-06-06 15:47:11.363721: Epoch 100/100, Test: HR = 0.5767, NDCG = 0.3361
2022-06-06 15:47:13.041142: Model Saved: yelp
root@container-f87d1190ac-f968e13b:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp 0.1 --ssl_reg 1e-4 --save_path yelp --batch 512
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.Conf         igProto instead.

2022-06-06 15:53:51.928005: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-06 15:53:52.322545: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session          instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:241: The name tf.placeholder is deprecated. Please use tf.c         ompat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please          use tf.compat.v1.get_variable instead.

drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tens         or("SparseTensor/values:0", shape=(335168,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_sha         pe:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) wit         h keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tens         or("SparseTensor/values:0", shape=(335168,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_sha         pe:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Te         nsor("SparseTensor_1/values:0", shape=(177378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Te         nsor("SparseTensor_1/values:0", shape=(177378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Te         nsor("SparseTensor_2/values:0", shape=(156718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Te         nsor("SparseTensor_2/values:0", shape=(156718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Te         nsor("SparseTensor_3/values:0", shape=(156674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Te         nsor("SparseTensor_3/values:0", shape=(156674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Te         nsor("SparseTensor_4/values:0", shape=(159442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Te         nsor("SparseTensor_4/values:0", shape=(159442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Te         nsor("SparseTensor_5/values:0", shape=(155622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Te         nsor("SparseTensor_5/values:0", shape=(155622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Te         nsor("SparseTensor_6/values:0", shape=(141690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Te         nsor("SparseTensor_6/values:0", shape=(141690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Ten         sor("SparseTensor_7/values:0", shape=(85108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense         _shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Ten         sor("SparseTensor_7/values:0", shape=(85108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense         _shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Plea         se use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is          deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:         1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated a         nd will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x         7f097e8fec90>> could not be transformed and will be executed as-is. Please report this to the AutgoGrap         h team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and at         tach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dens         e object at 0x7f097e8fec90>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x         7f0983d59b10>> could not be transformed and will be executed as-is. Please report this to the AutgoGrap         h team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and at         tach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dens         e object at 0x7f0983d59b10>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.exponential_decay is deprecated. Ple         ase use tf.compat.v1.train.exponential_decay instead.

WARNING:tensorflow:From /root/CLSR/model.py:287: The name tf.train.AdamOptimizer is deprecated. Please          use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py         :1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and w         ill be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-06 15:54:15.703117: Model Prepared
2022-06-06 15:54:18.377674: Variables Inited
2022-06-06 15:55:09.507954: Epoch 0/100, Train: Loss = 5.8409, preLoss = 2.3524
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.02 0.02 0.04849767101276316 0.0777 0         .09667022690232431 0.2534
2022-06-06 15:56:01.769822: Epoch 0/100, Test: HR = 0.1401, NDCG = 0.0683
WARNING:tensorflow:From /root/CLSR/model.py:524: The name tf.train.Saver is deprecated. Please use tf.c         ompat.v1.train.Saver instead.

2022-06-06 15:56:03.758276: Model Saved: yelp

2022-06-06 15:56:45.884291: Epoch 1/100, Train: Loss = 10.9814, preLoss = 3.9973

2022-06-06 15:57:28.151676: Epoch 2/100, Train: Loss = 13.5438, preLoss = 4.1678

2022-06-06 15:58:10.307720: Epoch 3/100, Train: Loss = 14.9975, preLoss = 4.0008
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0554 0.0554 0.11612249726067794 0.17         56 0.19090786850296546 0.444
2022-06-06 15:59:03.768953: Epoch 3/100, Test: HR = 0.2810, NDCG = 0.1499
2022-06-06 15:59:05.230772: Model Saved: yelp

2022-06-06 15:59:47.635228: Epoch 4/100, Train: Loss = 15.4662, preLoss = 3.6279

2022-06-06 16:00:30.245798: Epoch 5/100, Train: Loss = 15.1180, preLoss = 2.9939

2022-06-06 16:01:13.146187: Epoch 6/100, Train: Loss = 14.3832, preLoss = 2.5541
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0758 0.0758 0.1456047608557638 0.213         9 0.22627702557344806 0.5021
2022-06-06 16:02:05.114450: Epoch 6/100, Test: HR = 0.3307, NDCG = 0.1832
2022-06-06 16:02:06.544657: Model Saved: yelp

2022-06-06 16:02:48.417740: Epoch 7/100, Train: Loss = 13.3563, preLoss = 2.1362

2022-06-06 16:03:30.852120: Epoch 8/100, Train: Loss = 12.1488, preLoss = 1.7492

2022-06-06 16:04:13.940159: Epoch 9/100, Train: Loss = 11.0020, preLoss = 1.4832
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0824 0.0824 0.16201148161192286 0.24          0.24638850778843135 0.5388
2022-06-06 16:05:05.715538: Epoch 9/100, Test: HR = 0.3691, NDCG = 0.2037
2022-06-06 16:05:07.170055: Model Saved: yelp

2022-06-06 16:05:48.734021: Epoch 10/100, Train: Loss = 9.8265, preLoss = 1.2000

2022-06-06 16:06:31.179650: Epoch 11/100, Train: Loss = 8.7193, preLoss = 0.9874

2022-06-06 16:07:12.597590: Epoch 12/100, Train: Loss = 7.7427, preLoss = 0.8541
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0955 0.0955 0.1749583099261796 0.253         1 0.26090363995409205 0.5599
2022-06-06 16:08:03.992934: Epoch 12/100, Test: HR = 0.3784, NDCG = 0.2152
2022-06-06 16:08:05.460168: Model Saved: yelp

2022-06-06 16:08:47.558285: Epoch 13/100, Train: Loss = 6.8200, preLoss = 0.6985

2022-06-06 16:09:29.918574: Epoch 14/100, Train: Loss = 6.0273, preLoss = 0.5989

2022-06-06 16:10:11.510291: Epoch 15/100, Train: Loss = 5.2905, preLoss = 0.5008
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0991 0.0991 0.1874089012750206 0.272         4 0.26997996148863546 0.5648
2022-06-06 16:11:02.580447: Epoch 15/100, Test: HR = 0.4014, NDCG = 0.2287
2022-06-06 16:11:04.002058: Model Saved: yelp

2022-06-06 16:11:46.137533: Epoch 16/100, Train: Loss = 4.6587, preLoss = 0.4286

2022-06-06 16:12:27.745416: Epoch 17/100, Train: Loss = 4.1035, preLoss = 0.3683

2022-06-06 16:13:09.375785: Epoch 18/100, Train: Loss = 3.6185, preLoss = 0.3207
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0906 0.0906 0.18463551854561377 0.27         43 0.2719894906406701 0.5835
2022-06-06 16:14:00.496083: Epoch 18/100, Test: HR = 0.4086, NDCG = 0.2279
2022-06-06 16:14:01.900173: Model Saved: yelp

2022-06-06 16:14:44.084688: Epoch 19/100, Train: Loss = 3.1981, preLoss = 0.2783

2022-06-06 16:15:26.240522: Epoch 20/100, Train: Loss = 2.8464, preLoss = 0.2497

2022-06-06 16:16:07.453512: Epoch 21/100, Train: Loss = 2.5325, preLoss = 0.2218
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0945 0.0945 0.1912414176038211 0.285         9 0.27975396038096617 0.5997
2022-06-06 16:16:57.848471: Epoch 21/100, Test: HR = 0.4217, NDCG = 0.2350
2022-06-06 16:16:59.286586: Model Saved: yelp

2022-06-06 16:17:40.331572: Epoch 22/100, Train: Loss = 2.2652, preLoss = 0.1931

2022-06-06 16:18:21.619863: Epoch 23/100, Train: Loss = 2.0300, preLoss = 0.1726

2022-06-06 16:19:03.359538: Epoch 24/100, Train: Loss = 1.8325, preLoss = 0.1574
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1046 0.1046 0.201457932588728 0.2969          0.29153929948277646 0.6153
2022-06-06 16:19:53.564965: Epoch 24/100, Test: HR = 0.4384, NDCG = 0.2470
2022-06-06 16:19:54.891994: Model Saved: yelp

2022-06-06 16:20:36.550315: Epoch 25/100, Train: Loss = 1.6650, preLoss = 0.1444

2022-06-06 16:21:18.763529: Epoch 26/100, Train: Loss = 1.5149, preLoss = 0.1343

2022-06-06 16:22:01.060748: Epoch 27/100, Train: Loss = 1.3911, preLoss = 0.1277
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1132 0.1132 0.20808499426307417 0.3          0.2980316683581155 0.618
2022-06-06 16:22:52.075282: Epoch 27/100, Test: HR = 0.4370, NDCG = 0.2523
2022-06-06 16:22:53.653570: Model Saved: yelp

2022-06-06 16:23:36.038112: Epoch 28/100, Train: Loss = 1.2816, preLoss = 0.1191

2022-06-06 16:24:18.413577: Epoch 29/100, Train: Loss = 1.1870, preLoss = 0.1126

2022-06-06 16:25:00.182300: Epoch 30/100, Train: Loss = 1.1042, preLoss = 0.1085
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0999 0.0999 0.20391202047812562 0.30         7 0.2962113920540365 0.6342
2022-06-06 16:25:50.208491: Epoch 30/100, Test: HR = 0.4503, NDCG = 0.2499
2022-06-06 16:25:51.700745: Model Saved: yelp

2022-06-06 16:26:33.117409: Epoch 31/100, Train: Loss = 1.0339, preLoss = 0.1067

2022-06-06 16:27:14.703231: Epoch 32/100, Train: Loss = 0.9666, preLoss = 0.0981

2022-06-06 16:27:56.889024: Epoch 33/100, Train: Loss = 0.9137, preLoss = 0.0985
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1154 0.1154 0.21122785349259135 0.30         83 0.3060215001094295 0.6423
2022-06-06 16:28:47.756150: Epoch 33/100, Test: HR = 0.4565, NDCG = 0.2592
2022-06-06 16:28:49.233226: Model Saved: yelp

2022-06-06 16:29:31.224321: Epoch 34/100, Train: Loss = 0.8612, preLoss = 0.0949

2022-06-06 16:30:13.226058: Epoch 35/100, Train: Loss = 0.8157, preLoss = 0.0891

2022-06-06 16:30:55.429117: Epoch 36/100, Train: Loss = 0.7756, preLoss = 0.0884
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1239 0.1239 0.2186527638275164 0.316         2 0.3147208945586944 0.6546
2022-06-06 16:31:46.376019: Epoch 36/100, Test: HR = 0.4692, NDCG = 0.2680
2022-06-06 16:31:47.930976: Model Saved: yelp

2022-06-06 16:32:29.770050: Epoch 37/100, Train: Loss = 0.7415, preLoss = 0.0876

2022-06-06 16:33:11.433248: Epoch 38/100, Train: Loss = 0.7098, preLoss = 0.0848

2022-06-06 16:33:53.608476: Epoch 39/100, Train: Loss = 0.6778, preLoss = 0.0832
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1183 0.1183 0.2235897167396257 0.326         7 0.31785743041767966 0.6586
2022-06-06 16:34:44.467081: Epoch 39/100, Test: HR = 0.4766, NDCG = 0.2721
2022-06-06 16:34:46.115546: Model Saved: yelp

2022-06-06 16:35:28.232919: Epoch 40/100, Train: Loss = 0.6568, preLoss = 0.0819

2022-06-06 16:36:09.924828: Epoch 41/100, Train: Loss = 0.6339, preLoss = 0.0805

2022-06-06 16:36:52.183910: Epoch 42/100, Train: Loss = 0.6105, preLoss = 0.0781
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1242 0.1242 0.23144422785097782 0.33         6 0.3252847144452888 0.6656
2022-06-06 16:37:42.611693: Epoch 42/100, Test: HR = 0.4879, NDCG = 0.2806
2022-06-06 16:37:44.154439: Model Saved: yelp

2022-06-06 16:38:26.269653: Epoch 43/100, Train: Loss = 0.5875, preLoss = 0.0782

2022-06-06 16:39:07.556544: Epoch 44/100, Train: Loss = 0.5720, preLoss = 0.0795

2022-06-06 16:39:49.256139: Epoch 45/100, Train: Loss = 0.5592, preLoss = 0.0797
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1436 0.1436 0.24856248302155473 0.35         68 0.3394353961389695 0.677
2022-06-06 16:40:39.833377: Epoch 45/100, Test: HR = 0.5005, NDCG = 0.2949
2022-06-06 16:40:41.352080: Model Saved: yelp

2022-06-06 16:41:23.408014: Epoch 46/100, Train: Loss = 0.5411, preLoss = 0.0751

2022-06-06 16:42:05.456337: Epoch 47/100, Train: Loss = 0.5262, preLoss = 0.0762

2022-06-06 16:42:47.423453: Epoch 48/100, Train: Loss = 0.5128, preLoss = 0.0754
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1501 0.1501 0.2570804331894296 0.367         3 0.3496869494413885 0.6926
2022-06-06 16:43:38.347975: Epoch 48/100, Test: HR = 0.5149, NDCG = 0.3050
2022-06-06 16:43:39.951474: Model Saved: yelp

2022-06-06 16:44:21.954269: Epoch 49/100, Train: Loss = 0.5012, preLoss = 0.0749

2022-06-06 16:45:03.596786: Epoch 50/100, Train: Loss = 0.4934, preLoss = 0.0738

2022-06-06 16:45:45.705260: Epoch 51/100, Train: Loss = 0.4818, preLoss = 0.0726
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1585 0.1585 0.26269129026475396 0.36         88 0.35871742702176235 0.7028
2022-06-06 16:46:36.489489: Epoch 51/100, Test: HR = 0.5319, NDCG = 0.3155
2022-06-06 16:46:38.053249: Model Saved: yelp

2022-06-06 16:47:19.954439: Epoch 52/100, Train: Loss = 0.4710, preLoss = 0.0711

2022-06-06 16:48:02.154338: Epoch 53/100, Train: Loss = 0.4632, preLoss = 0.0724

2022-06-06 16:48:43.627794: Epoch 54/100, Train: Loss = 0.4537, preLoss = 0.0703
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1514 0.1514 0.2646199185382995 0.376         7 0.3585085700448582 0.7027
2022-06-06 16:49:34.747688: Epoch 54/100, Test: HR = 0.5334, NDCG = 0.3157
2022-06-06 16:49:36.352786: Model Saved: yelp

2022-06-06 16:50:17.799388: Epoch 55/100, Train: Loss = 0.4445, preLoss = 0.0693

2022-06-06 16:51:00.065658: Epoch 56/100, Train: Loss = 0.4396, preLoss = 0.0704

2022-06-06 16:51:41.840687: Epoch 57/100, Train: Loss = 0.4324, preLoss = 0.0699
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1566 0.1566 0.26598553968119587 0.37         34 0.36209744949400446 0.7074
2022-06-06 16:52:32.198096: Epoch 57/100, Test: HR = 0.5341, NDCG = 0.3184
2022-06-06 16:52:33.779092: Model Saved: yelp

2022-06-06 16:53:15.379871: Epoch 58/100, Train: Loss = 0.4237, preLoss = 0.0682

2022-06-06 16:53:57.405844: Epoch 59/100, Train: Loss = 0.4202, preLoss = 0.0695

2022-06-06 16:54:39.854094: Epoch 60/100, Train: Loss = 0.4131, preLoss = 0.0678
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1691 0.1691 0.2760968810191909 0.379         7 0.369677843432804 0.7058
2022-06-06 16:55:30.637950: Epoch 60/100, Test: HR = 0.5355, NDCG = 0.3267
2022-06-06 16:55:32.333709: Model Saved: yelp

2022-06-06 16:56:14.429822: Epoch 61/100, Train: Loss = 0.4062, preLoss = 0.0668

2022-06-06 16:56:56.715017: Epoch 62/100, Train: Loss = 0.4024, preLoss = 0.0663

2022-06-06 16:57:38.427873: Epoch 63/100, Train: Loss = 0.3968, preLoss = 0.0662
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1619 0.1619 0.2754387931509265 0.385         5 0.3690815924948959 0.7107
2022-06-06 16:58:28.916897: Epoch 63/100, Test: HR = 0.5437, NDCG = 0.3270
2022-06-06 16:58:30.623454: Model Saved: yelp

2022-06-06 16:59:12.375051: Epoch 64/100, Train: Loss = 0.3941, preLoss = 0.0657

2022-06-06 16:59:54.148749: Epoch 65/100, Train: Loss = 0.3858, preLoss = 0.0642

2022-06-06 17:00:35.798287: Epoch 66/100, Train: Loss = 0.3835, preLoss = 0.0642
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1648 0.1648 0.2793134120852116 0.390         9 0.3724719886196396 0.716
2022-06-06 17:01:26.070385: Epoch 66/100, Test: HR = 0.5445, NDCG = 0.3292
2022-06-06 17:01:27.720391: Model Saved: yelp

2022-06-06 17:02:08.960231: Epoch 67/100, Train: Loss = 0.3798, preLoss = 0.0658

2022-06-06 17:02:50.258181: Epoch 68/100, Train: Loss = 0.3734, preLoss = 0.0637

2022-06-06 17:03:31.587961: Epoch 69/100, Train: Loss = 0.3715, preLoss = 0.0643
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1661 0.1661 0.2797078011829806 0.393         9 0.372303613259106 0.7164
2022-06-06 17:04:22.164583: Epoch 69/100, Test: HR = 0.5476, NDCG = 0.3297
2022-06-06 17:04:23.825085: Model Saved: yelp

2022-06-06 17:05:06.061525: Epoch 70/100, Train: Loss = 0.3675, preLoss = 0.0625

2022-06-06 17:05:47.837804: Epoch 71/100, Train: Loss = 0.3628, preLoss = 0.0630

2022-06-06 17:06:29.675933: Epoch 72/100, Train: Loss = 0.3600, preLoss = 0.0629
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1599 0.1599 0.27685649796065187 0.39         04 0.3704675042491355 0.7148
2022-06-06 17:07:19.509027: Epoch 72/100, Test: HR = 0.5480, NDCG = 0.3283
2022-06-06 17:07:21.144850: Model Saved: yelp

2022-06-06 17:08:03.036780: Epoch 73/100, Train: Loss = 0.3579, preLoss = 0.0628

2022-06-06 17:08:45.303332: Epoch 74/100, Train: Loss = 0.3542, preLoss = 0.0628

2022-06-06 17:09:26.736105: Epoch 75/100, Train: Loss = 0.3517, preLoss = 0.0622
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1616 0.1616 0.28138553345603273 0.39         74 0.37400877780875325 0.7192
2022-06-06 17:10:17.651667: Epoch 75/100, Test: HR = 0.5523, NDCG = 0.3319
2022-06-06 17:10:19.256090: Model Saved: yelp

2022-06-06 17:11:00.822256: Epoch 76/100, Train: Loss = 0.3484, preLoss = 0.0607

2022-06-06 17:11:42.170698: Epoch 77/100, Train: Loss = 0.3450, preLoss = 0.0615

2022-06-06 17:12:23.530920: Epoch 78/100, Train: Loss = 0.3457, preLoss = 0.0634
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1631 0.1631 0.28474335982181714 0.40         09 0.3764781932003915 0.7193
2022-06-06 17:13:13.876690: Epoch 78/100, Test: HR = 0.5547, NDCG = 0.3350
2022-06-06 17:13:15.547556: Model Saved: yelp

2022-06-06 17:13:56.811752: Epoch 79/100, Train: Loss = 0.3421, preLoss = 0.0612

2022-06-06 17:14:38.424448: Epoch 80/100, Train: Loss = 0.3402, preLoss = 0.0610

2022-06-06 17:15:19.995288: Epoch 81/100, Train: Loss = 0.3369, preLoss = 0.0598
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1668 0.1668 0.2833035772630399 0.395         2 0.37598924265625683 0.7153
2022-06-06 17:16:10.206806: Epoch 81/100, Test: HR = 0.5567, NDCG = 0.3360
2022-06-06 17:16:12.042057: Model Saved: yelp

2022-06-06 17:16:53.889829: Epoch 82/100, Train: Loss = 0.3341, preLoss = 0.0606

2022-06-06 17:17:36.023036: Epoch 83/100, Train: Loss = 0.3348, preLoss = 0.0605

2022-06-06 17:18:17.650609: Epoch 84/100, Train: Loss = 0.3308, preLoss = 0.0610
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1641 0.1641 0.2821734552094588 0.395         7 0.3749102687589022 0.7166
2022-06-06 17:19:07.638998: Epoch 84/100, Test: HR = 0.5571, NDCG = 0.3348
2022-06-06 17:19:09.277398: Model Saved: yelp

2022-06-06 17:19:50.816341: Epoch 85/100, Train: Loss = 0.3293, preLoss = 0.0588

2022-06-06 17:20:33.183870: Epoch 86/100, Train: Loss = 0.3299, preLoss = 0.0601

2022-06-06 17:21:15.450211: Epoch 87/100, Train: Loss = 0.3260, preLoss = 0.0595
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1711 0.1711 0.2854578482519551 0.395         1 0.3795138540019341 0.7197
2022-06-06 17:22:06.144339: Epoch 87/100, Test: HR = 0.5596, NDCG = 0.3392
2022-06-06 17:22:07.917844: Model Saved: yelp

2022-06-06 17:22:49.288349: Epoch 88/100, Train: Loss = 0.3258, preLoss = 0.0605

2022-06-06 17:23:31.547437: Epoch 89/100, Train: Loss = 0.3211, preLoss = 0.0589

2022-06-06 17:24:13.733705: Epoch 90/100, Train: Loss = 0.3205, preLoss = 0.0585
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1721 0.1721 0.2858171653680957 0.395         3 0.3814828634944395 0.7257
2022-06-06 17:25:04.242458: Epoch 90/100, Test: HR = 0.5615, NDCG = 0.3400
2022-06-06 17:25:05.950072: Model Saved: yelp

2022-06-06 17:25:47.368411: Epoch 91/100, Train: Loss = 0.3173, preLoss = 0.0586

2022-06-06 17:26:28.638359: Epoch 92/100, Train: Loss = 0.3164, preLoss = 0.0575

2022-06-06 17:27:10.125089: Epoch 93/100, Train: Loss = 0.3157, preLoss = 0.0583
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1732 0.1732 0.2864867762466031 0.398         9 0.3808567184950595 0.7251
2022-06-06 17:28:00.775716: Epoch 93/100, Test: HR = 0.5621, NDCG = 0.3397
2022-06-06 17:28:02.578673: Model Saved: yelp

2022-06-06 17:28:44.652757: Epoch 94/100, Train: Loss = 0.3142, preLoss = 0.0572

2022-06-06 17:29:26.861463: Epoch 95/100, Train: Loss = 0.3115, preLoss = 0.0569

2022-06-06 17:30:09.113815: Epoch 96/100, Train: Loss = 0.3123, preLoss = 0.0581
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1744 0.1744 0.28765725072220855 0.40         07 0.38199502896565973 0.7261
2022-06-06 17:31:00.137009: Epoch 96/100, Test: HR = 0.5640, NDCG = 0.3411
2022-06-06 17:31:01.857010: Model Saved: yelp

2022-06-06 17:31:43.926248: Epoch 97/100, Train: Loss = 0.3154, preLoss = 0.0590

2022-06-06 17:32:25.501482: Epoch 98/100, Train: Loss = 0.3104, preLoss = 0.0565

2022-06-06 17:33:07.404413: Epoch 99/100, Train: Loss = 0.3109, preLoss = 0.0584
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1742 0.1742 0.28508484573139514 0.39         6 0.3817302876273802 0.7287
2022-06-06 17:33:58.143312: Epoch 99/100, Test: HR = 0.5632, NDCG = 0.3399
2022-06-06 17:33:59.950296: Model Saved: yelp

epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1743 0.1743 0.2850669574832497 0.395         8 0.38179448045656056 0.7287
2022-06-06 17:34:51.229391: Epoch 100/100, Test: HR = 0.5634, NDCG = 0.3400
2022-06-06 17:34:52.863958: Model Saved: yelp
root@container-f87d1190ac-f968e13b:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp 0.1 --ssl_reg 1e-4 --save_path yelp --batch 512 --epoch 120
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.Conf         igProto instead.

2022-06-06 17:35:10.150359: Start
^CTraceback (most recent call last):
  File "main.py", line 20, in <module>
    handler.LoadData()
  File "/root/CLSR/DataHandler.py", line 93, in LoadData
    self.user_sequence = pickle.load(fs)
KeyboardInterrupt
root@container-f87d1190ac-f968e13b:~/CLSR#
root@container-f87d1190ac-f968e13b:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp 0.1 --ssl_reg 1e-4 --save_path yelp --batch 512 --epoch 120 --load_path yelp
usage: main.py [-h] [--lr LR] [--batch BATCH] [--reg REG] [--epoch EPOCH]
               [--graphNum GRAPHNUM] [--decay DECAY] [--save_path SAVE_PATH]
               [--latdim LATDIM] [--rank RANK] [--memosize MEMOSIZE]
               [--sampNum SAMPNUM] [--sslNum SSLNUM]
               [--query_vector_dim QUERY_VECTOR_DIM]
               [--num_attention_heads NUM_ATTENTION_HEADS]
               [--hyperNum HYPERNUM] [--gnn_layer GNN_LAYER] [--trnNum TRNNUM]
               [--load_model LOAD_MODEL] [--shoot SHOOT] [--data DATA]
               [--target TARGET] [--deep_layer DEEP_LAYER] [--mult MULT]
               [--keepRate KEEPRATE] [--slot SLOT]
               [--graphSampleN GRAPHSAMPLEN] [--divSize DIVSIZE]
               [--tstEpoch TSTEPOCH] [--subUsrSize SUBUSRSIZE]
               [--subUsrDcy SUBUSRDCY] [--leaky LEAKY] [--hyperReg HYPERREG]
               [--temp TEMP] [--ssl_reg SSL_REG] [--percent PERCENT]
               [--batch_size BATCH_SIZE] [--pos_length POS_LENGTH]
main.py: error: unrecognized arguments: --load_path yelp
root@container-f87d1190ac-f968e13b:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp 0.1 --ssl_reg 1e-4 --save_path yelp --batch 512 --epoch 120 --load_model yelp
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.Conf         igProto instead.

2022-06-06 17:35:28.640441: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-06 17:35:29.016702: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session          instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:241: The name tf.placeholder is deprecated. Please use tf.c         ompat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please          use tf.compat.v1.get_variable instead.

drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tens         or("SparseTensor/values:0", shape=(335168,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_sha         pe:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) wit         h keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tens         or("SparseTensor/values:0", shape=(335168,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_sha         pe:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Te         nsor("SparseTensor_1/values:0", shape=(177378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Te         nsor("SparseTensor_1/values:0", shape=(177378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Te         nsor("SparseTensor_2/values:0", shape=(156718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Te         nsor("SparseTensor_2/values:0", shape=(156718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Te         nsor("SparseTensor_3/values:0", shape=(156674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Te         nsor("SparseTensor_3/values:0", shape=(156674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Te         nsor("SparseTensor_4/values:0", shape=(159442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Te         nsor("SparseTensor_4/values:0", shape=(159442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Te         nsor("SparseTensor_5/values:0", shape=(155622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Te         nsor("SparseTensor_5/values:0", shape=(155622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Te         nsor("SparseTensor_6/values:0", shape=(141690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Te         nsor("SparseTensor_6/values:0", shape=(141690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Ten         sor("SparseTensor_7/values:0", shape=(85108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense         _shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Ten         sor("SparseTensor_7/values:0", shape=(85108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense         _shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Plea         se use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is          deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:         1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated a         nd will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x         7fc4c14fec50>> could not be transformed and will be executed as-is. Please report this to the AutgoGrap         h team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and at         tach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dens         e object at 0x7fc4c14fec50>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x         7fc4c2936510>> could not be transformed and will be executed as-is. Please report this to the AutgoGrap         h team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and at         tach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dens         e object at 0x7fc4c2936510>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.exponential_decay is deprecated. Ple         ase use tf.compat.v1.train.exponential_decay instead.

WARNING:tensorflow:From /root/CLSR/model.py:287: The name tf.train.AdamOptimizer is deprecated. Please          use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py         :1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and w         ill be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-06 17:35:52.210952: Model Prepared
WARNING:tensorflow:From /root/CLSR/model.py:529: The name tf.train.Saver is deprecated. Please use tf.c         ompat.v1.train.Saver instead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.p         y:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and wil         l be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2022-06-06 17:35:54.226556: Model Loaded
2022-06-06 17:36:46.602502: Epoch 100/120, Train: Loss = 0.3097, preLoss = 0.0570

2022-06-06 17:37:27.607154: Epoch 101/120, Train: Loss = 0.3076, preLoss = 0.0579

2022-06-06 17:38:07.798967: Epoch 102/120, Train: Loss = 0.3067, preLoss = 0.0576
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1773 0.1773 0.2865573410616064 0.394         8 0.38414348198550624 0.7313
2022-06-06 17:38:56.983385: Epoch 102/120, Test: HR = 0.5633, NDCG = 0.3417
2022-06-06 17:38:58.609243: Model Saved: yelp

2022-06-06 17:39:39.251264: Epoch 103/120, Train: Loss = 0.3042, preLoss = 0.0569

2022-06-06 17:40:20.377464: Epoch 104/120, Train: Loss = 0.3034, preLoss = 0.0570

2022-06-06 17:41:01.558121: Epoch 105/120, Train: Loss = 0.3036, preLoss = 0.0563
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1816 0.1816 0.2900840000963181 0.396         6 0.3874732319177835 0.7318
2022-06-06 17:41:51.480583: Epoch 105/120, Test: HR = 0.5661, NDCG = 0.3456
2022-06-06 17:41:53.147497: Model Saved: yelp

2022-06-06 17:42:34.286075: Epoch 106/120, Train: Loss = 0.3029, preLoss = 0.0568

2022-06-06 17:43:15.210506: Epoch 107/120, Train: Loss = 0.3068, preLoss = 0.0574

2022-06-06 17:43:56.010193: Epoch 108/120, Train: Loss = 0.3030, preLoss = 0.0569
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.184 0.184 0.2900455805427095 0.3937          0.3884421494643953 0.7317
2022-06-06 17:44:46.548267: Epoch 108/120, Test: HR = 0.5667, NDCG = 0.3468
2022-06-06 17:44:47.907799: Model Saved: yelp

2022-06-06 17:45:29.358354: Epoch 109/120, Train: Loss = 0.2979, preLoss = 0.0559

2022-06-06 17:46:10.300500: Epoch 110/120, Train: Loss = 0.2984, preLoss = 0.0559

2022-06-06 17:46:50.894005: Epoch 111/120, Train: Loss = 0.3008, preLoss = 0.0563
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1812 0.1812 0.2897487135615273 0.396         1 0.3880514907056269 0.7345
2022-06-06 17:47:40.735159: Epoch 111/120, Test: HR = 0.5692, NDCG = 0.3464
2022-06-06 17:47:42.196512: Model Saved: yelp

2022-06-06 17:48:22.643988: Epoch 112/120, Train: Loss = 0.2986, preLoss = 0.0554

2022-06-06 17:49:03.456737: Epoch 113/120, Train: Loss = 0.2973, preLoss = 0.0560

2022-06-06 17:49:44.498932: Epoch 114/120, Train: Loss = 0.2973, preLoss = 0.0566
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1829 0.1829 0.28889797074209816 0.39         22 0.3887512758541606 0.7347
2022-06-06 17:50:34.916426: Epoch 114/120, Test: HR = 0.5707, NDCG = 0.3474
2022-06-06 17:50:36.385869: Model Saved: yelp

2022-06-06 17:51:17.549981: Epoch 115/120, Train: Loss = 0.2968, preLoss = 0.0557

2022-06-06 17:51:58.336320: Epoch 116/120, Train: Loss = 0.2946, preLoss = 0.0562

2022-06-06 17:52:39.248087: Epoch 117/120, Train: Loss = 0.2959, preLoss = 0.0550
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1817 0.1817 0.2902641914438602 0.396         7 0.38898466421291 0.7364
2022-06-06 17:53:27.894973: Epoch 117/120, Test: HR = 0.5718, NDCG = 0.3475
2022-06-06 17:53:29.380842: Model Saved: yelp

2022-06-06 17:54:10.379156: Epoch 118/120, Train: Loss = 0.2977, preLoss = 0.0572

2022-06-06 17:54:50.854034: Epoch 119/120, Train: Loss = 0.2933, preLoss = 0.0557

epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1832 0.1832 0.2912983494537907 0.397         7 0.39003889743141784 0.7378
2022-06-06 17:55:39.406734: Epoch 120/120, Test: HR = 0.5703, NDCG = 0.3477
2022-06-06 17:55:40.741109: Model Saved: yelp
root@container-f87d1190ac-f968e13b:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp 0.1 --ssl_reg 1e-4 --save_path yelp --batch 512 --epoch 150 --load_model yelp
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.Conf         igProto instead.

2022-06-06 18:03:47.785872: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-06 18:03:48.170210: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session          instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:241: The name tf.placeholder is deprecated. Please use tf.c         ompat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please          use tf.compat.v1.get_variable instead.

drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tens         or("SparseTensor/values:0", shape=(335168,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_sha         pe:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) wit         h keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tens         or("SparseTensor/values:0", shape=(335168,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_sha         pe:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Te         nsor("SparseTensor_1/values:0", shape=(177378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Te         nsor("SparseTensor_1/values:0", shape=(177378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Te         nsor("SparseTensor_2/values:0", shape=(156718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Te         nsor("SparseTensor_2/values:0", shape=(156718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Te         nsor("SparseTensor_3/values:0", shape=(156674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Te         nsor("SparseTensor_3/values:0", shape=(156674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Te         nsor("SparseTensor_4/values:0", shape=(159442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Te         nsor("SparseTensor_4/values:0", shape=(159442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Te         nsor("SparseTensor_5/values:0", shape=(155622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Te         nsor("SparseTensor_5/values:0", shape=(155622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Te         nsor("SparseTensor_6/values:0", shape=(141690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Te         nsor("SparseTensor_6/values:0", shape=(141690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Ten         sor("SparseTensor_7/values:0", shape=(85108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense         _shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Ten         sor("SparseTensor_7/values:0", shape=(85108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense         _shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Plea         se use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is          deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:         1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated a         nd will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x         7ff2a36d2c50>> could not be transformed and will be executed as-is. Please report this to the AutgoGrap         h team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and at         tach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dens         e object at 0x7ff2a36d2c50>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x         7ff2a2ea3510>> could not be transformed and will be executed as-is. Please report this to the AutgoGrap         h team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and at         tach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dens         e object at 0x7ff2a2ea3510>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.exponential_decay is deprecated. Ple         ase use tf.compat.v1.train.exponential_decay instead.

WARNING:tensorflow:From /root/CLSR/model.py:287: The name tf.train.AdamOptimizer is deprecated. Please          use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py         :1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and w         ill be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-06 18:04:11.941347: Model Prepared
WARNING:tensorflow:From /root/CLSR/model.py:529: The name tf.train.Saver is deprecated. Please use tf.c         ompat.v1.train.Saver instead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.p         y:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and wil         l be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2022-06-06 18:04:13.795839: Model Loaded
2022-06-06 18:05:04.450769: Epoch 118/150, Train: Loss = 0.2952, preLoss = 0.0559

2022-06-06 18:05:46.031036: Epoch 119/150, Train: Loss = 0.2941, preLoss = 0.0561

2022-06-06 18:06:27.513714: Epoch 120/150, Train: Loss = 0.2966, preLoss = 0.0556
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1844 0.1844 0.291841680563166 0.3979          0.3905753154455103 0.7378
2022-06-06 18:07:19.575856: Epoch 120/150, Test: HR = 0.5703, NDCG = 0.3483
2022-06-06 18:07:21.428884: Model Saved: yelp

2022-06-06 18:08:02.732927: Epoch 121/150, Train: Loss = 0.2931, preLoss = 0.0555

2022-06-06 18:08:44.048727: Epoch 122/150, Train: Loss = 0.2926, preLoss = 0.0556

2022-06-06 18:09:24.796383: Epoch 123/150, Train: Loss = 0.2901, preLoss = 0.0547
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1838 0.1838 0.2919428163164171 0.398         4 0.3909061262867195 0.7389
2022-06-06 18:10:15.982051: Epoch 123/150, Test: HR = 0.5709, NDCG = 0.3485
2022-06-06 18:10:17.426289: Model Saved: yelp

2022-06-06 18:10:58.698944: Epoch 124/150, Train: Loss = 0.2914, preLoss = 0.0543

2022-06-06 18:11:40.170152: Epoch 125/150, Train: Loss = 0.2926, preLoss = 0.0546

2022-06-06 18:12:21.519569: Epoch 126/150, Train: Loss = 0.2930, preLoss = 0.0556
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1814 0.1814 0.2885210366012147 0.392         6 0.3891643712191033 0.7372
2022-06-06 18:13:12.459361: Epoch 126/150, Test: HR = 0.5711, NDCG = 0.3472
2022-06-06 18:13:13.869729: Model Saved: yelp

2022-06-06 18:13:55.396901: Epoch 127/150, Train: Loss = 0.2904, preLoss = 0.0549

2022-06-06 18:14:36.754751: Epoch 128/150, Train: Loss = 0.2911, preLoss = 0.0551

2022-06-06 18:15:17.829493: Epoch 129/150, Train: Loss = 0.2917, preLoss = 0.0559
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1829 0.1829 0.2903338763810739 0.395         1 0.39029205811361595 0.7382
2022-06-06 18:16:07.441639: Epoch 129/150, Test: HR = 0.5698, NDCG = 0.3477
2022-06-06 18:16:08.847739: Model Saved: yelp

2022-06-06 18:16:49.960632: Epoch 130/150, Train: Loss = 0.2913, preLoss = 0.0557

2022-06-06 18:17:30.295821: Epoch 131/150, Train: Loss = 0.2882, preLoss = 0.0543

2022-06-06 18:18:11.517789: Epoch 132/150, Train: Loss = 0.2907, preLoss = 0.0558
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1821 0.1821 0.2905968480510948 0.396         2 0.3902154357753602 0.7381
2022-06-06 18:19:02.455269: Epoch 132/150, Test: HR = 0.5707, NDCG = 0.3479
2022-06-06 18:19:03.826480: Model Saved: yelp

2022-06-06 18:19:44.245872: Epoch 133/150, Train: Loss = 0.2885, preLoss = 0.0545

2022-06-06 18:20:24.696893: Epoch 134/150, Train: Loss = 0.2869, preLoss = 0.0554

2022-06-06 18:21:06.171821: Epoch 135/150, Train: Loss = 0.2912, preLoss = 0.0556
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1809 0.1809 0.2898002780330209 0.395         8 0.38960779674416013 0.738
2022-06-06 18:21:55.343660: Epoch 135/150, Test: HR = 0.5717, NDCG = 0.3476
2022-06-06 18:21:56.823056: Model Saved: yelp

2022-06-06 18:22:38.702684: Epoch 136/150, Train: Loss = 0.2885, preLoss = 0.0550

2022-06-06 18:23:19.701919: Epoch 137/150, Train: Loss = 0.2870, preLoss = 0.0547

2022-06-06 18:24:00.382876: Epoch 138/150, Train: Loss = 0.2898, preLoss = 0.0557
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1819 0.1819 0.290611716670083 0.3965          0.39048227048539863 0.7392
2022-06-06 18:24:48.969280: Epoch 138/150, Test: HR = 0.5713, NDCG = 0.3480
2022-06-06 18:24:50.489188: Model Saved: yelp

2022-06-06 18:25:31.333523: Epoch 139/150, Train: Loss = 0.2880, preLoss = 0.0540

2022-06-06 18:26:12.651494: Epoch 140/150, Train: Loss = 0.2854, preLoss = 0.0539

2022-06-06 18:26:54.460054: Epoch 141/150, Train: Loss = 0.2864, preLoss = 0.0540
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1817 0.1817 0.29065554035340807 0.39         69 0.39066207807968856 0.7402
2022-06-06 18:27:45.150941: Epoch 141/150, Test: HR = 0.5723, NDCG = 0.3482
2022-06-06 18:27:46.605373: Model Saved: yelp

2022-06-06 18:28:27.895847: Epoch 142/150, Train: Loss = 0.2858, preLoss = 0.0544

2022-06-06 18:29:08.949674: Epoch 143/150, Train: Loss = 0.2872, preLoss = 0.0548

2022-06-06 18:29:49.990276: Epoch 144/150, Train: Loss = 0.2865, preLoss = 0.0551
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1818 0.1818 0.29155211030396827 0.39         88 0.39084377667151343 0.7398
2022-06-06 18:30:38.229899: Epoch 144/150, Test: HR = 0.5721, NDCG = 0.3485
2022-06-06 18:30:39.711176: Model Saved: yelp

2022-06-06 18:31:20.309850: Epoch 145/150, Train: Loss = 0.2854, preLoss = 0.0538

2022-06-06 18:32:00.649503: Epoch 146/150, Train: Loss = 0.2887, preLoss = 0.0549

2022-06-06 18:32:40.820880: Epoch 147/150, Train: Loss = 0.2855, preLoss = 0.0551
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.183 0.183 0.2919330706170613 0.3987          0.3912628509331738 0.7399
2022-06-06 18:33:29.974384: Epoch 147/150, Test: HR = 0.5718, NDCG = 0.3488
2022-06-06 18:33:31.605697: Model Saved: yelp

2022-06-06 18:34:12.635184: Epoch 148/150, Train: Loss = 0.2876, preLoss = 0.0543

2022-06-06 18:34:53.106010: Epoch 149/150, Train: Loss = 0.2841, preLoss = 0.0540

epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1829 0.1829 0.292323410168566 0.3997          0.39132202665470284 0.74
2022-06-06 18:35:41.962809: Epoch 150/150, Test: HR = 0.5721, NDCG = 0.3489
2022-06-06 18:35:43.459679: Model Saved: yelp
root@container-f87d1190ac-f968e13b:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp 0.1 --ssl_reg 1e-4 --save_path yelp --batch 512 --lr 2e-3
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning:          Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will          be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWa         rning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, i         t will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.Conf         igProto instead.

2022-06-06 18:49:07.587480: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-06 18:49:07.971837: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session          instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:241: The name tf.placeholder is deprecated. Please use tf.c         ompat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please          use tf.compat.v1.get_variable instead.

drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tens         or("SparseTensor/values:0", shape=(335168,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_sha         pe:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) wit         h keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tens         or("SparseTensor/values:0", shape=(335168,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_sha         pe:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Te         nsor("SparseTensor_1/values:0", shape=(177378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Te         nsor("SparseTensor_1/values:0", shape=(177378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Te         nsor("SparseTensor_2/values:0", shape=(156718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Te         nsor("SparseTensor_2/values:0", shape=(156718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Te         nsor("SparseTensor_3/values:0", shape=(156674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Te         nsor("SparseTensor_3/values:0", shape=(156674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Te         nsor("SparseTensor_4/values:0", shape=(159442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Te         nsor("SparseTensor_4/values:0", shape=(159442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Te         nsor("SparseTensor_5/values:0", shape=(155622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Te         nsor("SparseTensor_5/values:0", shape=(155622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Te         nsor("SparseTensor_6/values:0", shape=(141690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Te         nsor("SparseTensor_6/values:0", shape=(141690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/den         se_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Ten         sor("SparseTensor_7/values:0", shape=(85108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense         _shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Ten         sor("SparseTensor_7/values:0", shape=(85108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense         _shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Plea         se use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is          deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:         1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated a         nd will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x         7f44791e5dd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGrap         h team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and at         tach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dens         e object at 0x7f44791e5dd0>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x         7f447919cb90>> could not be transformed and will be executed as-is. Please report this to the AutgoGrap         h team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and at         tach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dens         e object at 0x7f447919cb90>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.exponential_decay is deprecated. Ple         ase use tf.compat.v1.train.exponential_decay instead.

WARNING:tensorflow:From /root/CLSR/model.py:287: The name tf.train.AdamOptimizer is deprecated. Please          use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py         :1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and w         ill be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-06 18:49:31.298856: Model Prepared
2022-06-06 18:49:34.170444: Variables Inited
2022-06-06 18:50:25.273257: Epoch 0/100, Train: Loss = 14.5341, preLoss = 5.2620
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0212 0.0212 0.057969061477334535 0.0         954 0.11129235561241205 0.2882
2022-06-06 18:51:14.151421: Epoch 0/100, Test: HR = 0.1677, NDCG = 0.0811
WARNING:tensorflow:From /root/CLSR/model.py:524: The name tf.train.Saver is deprecated. Please use tf.c         ompat.v1.train.Saver instead.

2022-06-06 18:51:16.440984: Model Saved: yelp

2022-06-06 18:51:56.070249: Epoch 1/100, Train: Loss = 33.3430, preLoss = 10.8581

2022-06-06 18:52:36.545785: Epoch 2/100, Train: Loss = 41.8870, preLoss = 10.6346

2022-06-06 18:53:16.345337: Epoch 3/100, Train: Loss = 45.6498, preLoss = 9.1084
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0456 0.0456 0.10969637992878647 0.17         45 0.18032407412402962 0.4271
2022-06-06 18:54:06.206732: Epoch 3/100, Test: HR = 0.2763, NDCG = 0.1425
2022-06-06 18:54:07.719916: Model Saved: yelp

2022-06-06 18:54:48.477169: Epoch 4/100, Train: Loss = 45.6011, preLoss = 7.4175

2022-06-06 18:55:29.833047: Epoch 5/100, Train: Loss = 42.3391, preLoss = 5.5622

2022-06-06 18:56:11.208866: Epoch 6/100, Train: Loss = 37.6846, preLoss = 4.3093
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0538 0.0538 0.1192220893853312 0.183         7 0.19743615201583506 0.4634
2022-06-06 18:57:02.768416: Epoch 6/100, Test: HR = 0.2942, NDCG = 0.1549
2022-06-06 18:57:04.237951: Model Saved: yelp

2022-06-06 18:57:46.650828: Epoch 7/100, Train: Loss = 32.1837, preLoss = 3.1968

2022-06-06 18:58:28.583079: Epoch 8/100, Train: Loss = 26.7345, preLoss = 2.2870

2022-06-06 18:59:11.592926: Epoch 9/100, Train: Loss = 21.6864, preLoss = 1.5984
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.075 0.075 0.14308945120027083 0.2099          0.22354816872192088 0.496
2022-06-06 19:00:04.273273: Epoch 9/100, Test: HR = 0.3325, NDCG = 0.1825
2022-06-06 19:00:05.784658: Model Saved: yelp

2022-06-06 19:00:48.009518: Epoch 10/100, Train: Loss = 17.3464, preLoss = 1.1555

2022-06-06 19:01:29.748942: Epoch 11/100, Train: Loss = 13.7691, preLoss = 0.8506

2022-06-06 19:02:11.879310: Epoch 12/100, Train: Loss = 10.9278, preLoss = 0.6074
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0829 0.0829 0.1561302447263134 0.228         3 0.2421718756535402 0.5331
2022-06-06 19:03:04.786905: Epoch 12/100, Test: HR = 0.3606, NDCG = 0.1987
2022-06-06 19:03:06.208123: Model Saved: yelp

2022-06-06 19:03:48.440970: Epoch 13/100, Train: Loss = 8.7324, preLoss = 0.4858

2022-06-06 19:04:30.690407: Epoch 14/100, Train: Loss = 6.9874, preLoss = 0.3698

2022-06-06 19:05:11.352192: Epoch 15/100, Train: Loss = 5.6625, preLoss = 0.2953
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.089 0.089 0.16850220351242345 0.2465          0.2567599052348698 0.5583
2022-06-06 19:06:03.122283: Epoch 15/100, Test: HR = 0.3879, NDCG = 0.2139
2022-06-06 19:06:04.707183: Model Saved: yelp

2022-06-06 19:06:44.633850: Epoch 16/100, Train: Loss = 4.6443, preLoss = 0.2401

2022-06-06 19:07:25.736151: Epoch 17/100, Train: Loss = 3.8659, preLoss = 0.2090

2022-06-06 19:08:05.918006: Epoch 18/100, Train: Loss = 3.2469, preLoss = 0.1754
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1053 0.1053 0.19240590081039358 0.28          0.2784632960960223 0.5841
2022-06-06 19:08:55.358997: Epoch 18/100, Test: HR = 0.4144, NDCG = 0.2357
2022-06-06 19:08:57.154625: Model Saved: yelp

2022-06-06 19:09:37.330897: Epoch 19/100, Train: Loss = 2.7751, preLoss = 0.1635

2022-06-06 19:10:17.758769: Epoch 20/100, Train: Loss = 2.4045, preLoss = 0.1493

2022-06-06 19:10:59.069576: Epoch 21/100, Train: Loss = 2.1175, preLoss = 0.1432
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1252 0.1252 0.21642117581972084 0.30         52 0.304895624373204 0.6144
2022-06-06 19:11:47.967133: Epoch 21/100, Test: HR = 0.4521, NDCG = 0.2640
2022-06-06 19:11:49.438566: Model Saved: yelp

2022-06-06 19:12:29.472770: Epoch 22/100, Train: Loss = 1.8759, preLoss = 0.1335

2022-06-06 19:13:10.174930: Epoch 23/100, Train: Loss = 1.6869, preLoss = 0.1310

2022-06-06 19:13:50.657848: Epoch 24/100, Train: Loss = 1.5319, preLoss = 0.1282
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1435 0.1435 0.23571942605215648 0.32         62 0.32583491871623343 0.6386
2022-06-06 19:14:39.680215: Epoch 24/100, Test: HR = 0.4789, NDCG = 0.2856
2022-06-06 19:14:41.132912: Model Saved: yelp

2022-06-06 19:15:21.954803: Epoch 25/100, Train: Loss = 1.4005, preLoss = 0.1255

2022-06-06 19:16:02.717127: Epoch 26/100, Train: Loss = 1.2959, preLoss = 0.1248

2022-06-06 19:16:43.809834: Epoch 27/100, Train: Loss = 1.2057, preLoss = 0.1233
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1407 0.1407 0.23907808523919907 0.33         47 0.3253691756868215 0.6368
2022-06-06 19:17:33.010721: Epoch 27/100, Test: HR = 0.4759, NDCG = 0.2850
2022-06-06 19:17:34.480563: Model Saved: yelp

2022-06-06 19:18:15.531826: Epoch 28/100, Train: Loss = 1.1313, preLoss = 0.1256

2022-06-06 19:18:55.979007: Epoch 29/100, Train: Loss = 1.0653, preLoss = 0.1222

2022-06-06 19:19:35.939586: Epoch 30/100, Train: Loss = 1.0070, preLoss = 0.1226
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1337 0.1337 0.23863729599286243 0.34         32 0.3289479352193637 0.6565
2022-06-06 19:20:24.674947: Epoch 30/100, Test: HR = 0.4943, NDCG = 0.2881
2022-06-06 19:20:26.084427: Model Saved: yelp

2022-06-06 19:21:06.098682: Epoch 31/100, Train: Loss = 0.9587, preLoss = 0.1198

2022-06-06 19:21:46.337111: Epoch 32/100, Train: Loss = 0.9128, preLoss = 0.1201

2022-06-06 19:22:27.064243: Epoch 33/100, Train: Loss = 0.8728, preLoss = 0.1191
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.123 0.123 0.2507877501286148 0.3631          0.3398728865000728 0.6742
2022-06-06 19:23:15.240514: Epoch 33/100, Test: HR = 0.5092, NDCG = 0.2986
2022-06-06 19:23:16.633408: Model Saved: yelp

2022-06-06 19:23:56.568181: Epoch 34/100, Train: Loss = 0.8405, preLoss = 0.1211

2022-06-06 19:24:35.849739: Epoch 35/100, Train: Loss = 0.8065, preLoss = 0.1198

2022-06-06 19:25:15.613901: Epoch 36/100, Train: Loss = 0.7727, preLoss = 0.1145
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1332 0.1332 0.2474778475083032 0.347         9 0.34065867844320497 0.6706
2022-06-06 19:26:04.452590: Epoch 36/100, Test: HR = 0.5069, NDCG = 0.2997
2022-06-06 19:26:05.774257: Model Saved: yelp

2022-06-06 19:26:45.690051: Epoch 37/100, Train: Loss = 0.7497, preLoss = 0.1181

2022-06-06 19:27:25.770307: Epoch 38/100, Train: Loss = 0.7275, preLoss = 0.1175

2022-06-06 19:28:05.446604: Epoch 39/100, Train: Loss = 0.7067, preLoss = 0.1149
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1372 0.1372 0.26302368326665576 0.37         26 0.350838304066583 0.6776
2022-06-06 19:28:53.736933: Epoch 39/100, Test: HR = 0.5248, NDCG = 0.3124
2022-06-06 19:28:55.176786: Model Saved: yelp

2022-06-06 19:29:35.107250: Epoch 40/100, Train: Loss = 0.6859, preLoss = 0.1157

2022-06-06 19:30:14.946391: Epoch 41/100, Train: Loss = 0.6670, preLoss = 0.1157

2022-06-06 19:30:55.053951: Epoch 42/100, Train: Loss = 0.6507, preLoss = 0.1145
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1307 0.1307 0.25430146013092847 0.37         72 0.34283181430325965 0.6851
2022-06-06 19:31:43.568179: Epoch 42/100, Test: HR = 0.5298, NDCG = 0.3038
2022-06-06 19:31:45.000627: Model Saved: yelp

2022-06-06 19:32:25.705813: Epoch 43/100, Train: Loss = 0.6329, preLoss = 0.1133

2022-06-06 19:33:06.044686: Epoch 44/100, Train: Loss = 0.6196, preLoss = 0.1131

2022-06-06 19:33:46.102460: Epoch 45/100, Train: Loss = 0.6035, preLoss = 0.1112
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1379 0.1379 0.2652506347589722 0.377         7 0.35748275210018093 0.6978
2022-06-06 19:34:34.189959: Epoch 45/100, Test: HR = 0.5325, NDCG = 0.3161
2022-06-06 19:34:35.660569: Model Saved: yelp

2022-06-06 19:35:15.342065: Epoch 46/100, Train: Loss = 0.5891, preLoss = 0.1103

2022-06-06 19:35:55.679668: Epoch 47/100, Train: Loss = 0.5768, preLoss = 0.1110

2022-06-06 19:36:36.494505: Epoch 48/100, Train: Loss = 0.5650, preLoss = 0.1092
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1451 0.1451 0.25974559829137567 0.36         74 0.3548786773710613 0.6989
2022-06-06 19:37:26.046116: Epoch 48/100, Test: HR = 0.5257, NDCG = 0.3116
2022-06-06 19:37:27.520532: Model Saved: yelp

2022-06-06 19:38:07.521773: Epoch 49/100, Train: Loss = 0.5559, preLoss = 0.1100

2022-06-06 19:38:47.230239: Epoch 50/100, Train: Loss = 0.5428, preLoss = 0.1059

2022-06-06 19:39:27.385826: Epoch 51/100, Train: Loss = 0.5334, preLoss = 0.1052
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1509 0.1509 0.27870324465646557 0.39         35 0.36853021491736243 0.7066
2022-06-06 19:40:17.324834: Epoch 51/100, Test: HR = 0.5431, NDCG = 0.3274
2022-06-06 19:40:18.850220: Model Saved: yelp

2022-06-06 19:40:59.037900: Epoch 52/100, Train: Loss = 0.5264, preLoss = 0.1076

2022-06-06 19:41:38.655527: Epoch 53/100, Train: Loss = 0.5164, preLoss = 0.1058

2022-06-06 19:42:18.879789: Epoch 54/100, Train: Loss = 0.5091, preLoss = 0.1066
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1615 0.1615 0.284193116818209 0.4043          0.3710882570648046 0.706
2022-06-06 19:43:07.027923: Epoch 54/100, Test: HR = 0.5520, NDCG = 0.3323
2022-06-06 19:43:08.490666: Model Saved: yelp

2022-06-06 19:43:48.261407: Epoch 55/100, Train: Loss = 0.5006, preLoss = 0.1035

2022-06-06 19:44:29.259406: Epoch 56/100, Train: Loss = 0.4913, preLoss = 0.1044

2022-06-06 19:45:10.564504: Epoch 57/100, Train: Loss = 0.4847, preLoss = 0.1016
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1697 0.1697 0.2826099331799199 0.386         9 0.3745214216326114 0.705
2022-06-06 19:46:00.892907: Epoch 57/100, Test: HR = 0.5461, NDCG = 0.3346
2022-06-06 19:46:02.488657: Model Saved: yelp

2022-06-06 19:46:43.954373: Epoch 58/100, Train: Loss = 0.4783, preLoss = 0.1035

2022-06-06 19:47:26.748617: Epoch 59/100, Train: Loss = 0.4712, preLoss = 0.1017

2022-06-06 19:48:09.430132: Epoch 60/100, Train: Loss = 0.4634, preLoss = 0.1009
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1685 0.1685 0.28260564933129856 0.38         81 0.3762020194436991 0.7117
2022-06-06 19:48:59.161805: Epoch 60/100, Test: HR = 0.5534, NDCG = 0.3364
2022-06-06 19:49:00.682919: Model Saved: yelp

2022-06-06 19:49:41.726262: Epoch 61/100, Train: Loss = 0.4575, preLoss = 0.1005

2022-06-06 19:50:23.818119: Epoch 62/100, Train: Loss = 0.4531, preLoss = 0.0998

2022-06-06 19:51:05.748908: Epoch 63/100, Train: Loss = 0.4487, preLoss = 0.1001
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1448 0.1448 0.2740215342328924 0.391         6 0.36707753169698 0.7132
2022-06-06 19:51:58.089490: Epoch 63/100, Test: HR = 0.5532, NDCG = 0.3269
2022-06-06 19:51:59.925920: Model Saved: yelp

2022-06-06 19:52:43.141185: Epoch 64/100, Train: Loss = 0.4446, preLoss = 0.0991

2022-06-06 19:53:24.093162: Epoch 65/100, Train: Loss = 0.4389, preLoss = 0.0987

2022-06-06 19:54:05.669727: Epoch 66/100, Train: Loss = 0.4327, preLoss = 0.0972
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1458 0.1458 0.2757110002802787 0.398         8 0.36595627453524415 0.7132
2022-06-06 19:54:55.866977: Epoch 66/100, Test: HR = 0.5484, NDCG = 0.3247
2022-06-06 19:54:57.476348: Model Saved: yelp

2022-06-06 19:55:40.035915: Epoch 67/100, Train: Loss = 0.4288, preLoss = 0.0988

2022-06-06 19:56:23.237963: Epoch 68/100, Train: Loss = 0.4236, preLoss = 0.0964

2022-06-06 19:57:05.215864: Epoch 69/100, Train: Loss = 0.4204, preLoss = 0.0973
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1612 0.1612 0.2906759169881732 0.417         3 0.3787278408927051 0.7265
2022-06-06 19:57:55.892958: Epoch 69/100, Test: HR = 0.5575, NDCG = 0.3364
2022-06-06 19:57:57.528831: Model Saved: yelp

2022-06-06 19:58:39.143732: Epoch 70/100, Train: Loss = 0.4188, preLoss = 0.0961

2022-06-06 19:59:20.450180: Epoch 71/100, Train: Loss = 0.4118, preLoss = 0.0958

2022-06-06 20:00:02.337249: Epoch 72/100, Train: Loss = 0.4057, preLoss = 0.0937
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1614 0.1614 0.2898873217706053 0.412          0.38028759672904766 0.727
2022-06-06 20:00:54.416263: Epoch 72/100, Test: HR = 0.5645, NDCG = 0.3395
2022-06-06 20:00:56.184832: Model Saved: yelp

2022-06-06 20:01:38.503163: Epoch 73/100, Train: Loss = 0.4062, preLoss = 0.0957

2022-06-06 20:02:20.019491: Epoch 74/100, Train: Loss = 0.4022, preLoss = 0.0941

2022-06-06 20:02:59.978000: Epoch 75/100, Train: Loss = 0.4007, preLoss = 0.0941
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1495 0.1495 0.27786062539761786 0.39         63 0.3734789923349908 0.7276
2022-06-06 20:03:48.248097: Epoch 75/100, Test: HR = 0.5599, NDCG = 0.3315
2022-06-06 20:03:49.847659: Model Saved: yelp

2022-06-06 20:04:29.599534: Epoch 76/100, Train: Loss = 0.3933, preLoss = 0.0921

2022-06-06 20:05:09.997437: Epoch 77/100, Train: Loss = 0.3932, preLoss = 0.0945

2022-06-06 20:05:51.763377: Epoch 78/100, Train: Loss = 0.3903, preLoss = 0.0923
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1509 0.1509 0.2762646903825938 0.3913 0.37489
2022-06-06 20:06:40.692541: Epoch 78/100, Test: HR = 0.5603, NDCG = 0.3317
2022-06-06 20:06:42.318431: Model Saved: yelp

2022-06-06 20:07:22.221088: Epoch 79/100, Train: Loss = 0.3866, preLoss = 0.0927

2022-06-06 20:08:02.709969: Epoch 80/100, Train: Loss = 0.3854, preLoss = 0.0920

2022-06-06 20:08:43.028730: Epoch 81/100, Train: Loss = 0.3816, preLoss = 0.0917
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1624 0.1624 0.2829300068263204 0.3957 0.38025
2022-06-06 20:09:31.500760: Epoch 81/100, Test: HR = 0.5639, NDCG = 0.3380
2022-06-06 20:09:33.327934: Model Saved: yelp

2022-06-06 20:10:13.820771: Epoch 82/100, Train: Loss = 0.3776, preLoss = 0.0918

2022-06-06 20:10:53.809090: Epoch 83/100, Train: Loss = 0.3788, preLoss = 0.0917

2022-06-06 20:11:34.034009: Epoch 84/100, Train: Loss = 0.3758, preLoss = 0.0911
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.161 0.161 0.2815903394594876 0.3978 0.3789027
2022-06-06 20:12:22.014568: Epoch 84/100, Test: HR = 0.5675, NDCG = 0.3371
2022-06-06 20:12:23.597154: Model Saved: yelp

2022-06-06 20:13:03.368684: Epoch 85/100, Train: Loss = 0.3710, preLoss = 0.0893

2022-06-06 20:13:43.339850: Epoch 86/100, Train: Loss = 0.3687, preLoss = 0.0893

2022-06-06 20:14:23.646799: Epoch 87/100, Train: Loss = 0.3688, preLoss = 0.0896
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1595 0.1595 0.27783656319548444 0.3864 0.3799
2022-06-06 20:15:11.569374: Epoch 87/100, Test: HR = 0.5670, NDCG = 0.3374
2022-06-06 20:15:13.348427: Model Saved: yelp

2022-06-06 20:15:53.376217: Epoch 88/100, Train: Loss = 0.3692, preLoss = 0.0899

2022-06-06 20:16:33.641293: Epoch 89/100, Train: Loss = 0.3685, preLoss = 0.0906

2022-06-06 20:17:13.521580: Epoch 90/100, Train: Loss = 0.3619, preLoss = 0.0892
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1611 0.1611 0.2800914713424127 0.3883 0.38281
2022-06-06 20:18:02.022827: Epoch 90/100, Test: HR = 0.5679, NDCG = 0.3392
2022-06-06 20:18:03.719222: Model Saved: yelp

2022-06-06 20:18:44.566799: Epoch 91/100, Train: Loss = 0.3606, preLoss = 0.0888

2022-06-06 20:19:24.928177: Epoch 92/100, Train: Loss = 0.3576, preLoss = 0.0880

2022-06-06 20:20:05.315585: Epoch 93/100, Train: Loss = 0.3586, preLoss = 0.0884
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1634 0.1634 0.28121247780154496 0.386 0.38444
2022-06-06 20:20:53.357785: Epoch 93/100, Test: HR = 0.5661, NDCG = 0.3407
2022-06-06 20:20:55.061670: Model Saved: yelp

2022-06-06 20:21:35.486238: Epoch 94/100, Train: Loss = 0.3540, preLoss = 0.0878

2022-06-06 20:22:15.563005: Epoch 95/100, Train: Loss = 0.3552, preLoss = 0.0882

2022-06-06 20:22:55.701884: Epoch 96/100, Train: Loss = 0.3538, preLoss = 0.0881
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1709 0.1709 0.28503766451562845 0.3934 0.3862
2022-06-06 20:23:42.912066: Epoch 96/100, Test: HR = 0.5671, NDCG = 0.3421
2022-06-06 20:23:44.568126: Model Saved: yelp

2022-06-06 20:24:24.690147: Epoch 97/100, Train: Loss = 0.3529, preLoss = 0.0881

2022-06-06 20:25:04.413986: Epoch 98/100, Train: Loss = 0.3498, preLoss = 0.0880

2022-06-06 20:25:43.968448: Epoch 99/100, Train: Loss = 0.3504, preLoss = 0.0890
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.171 0.171 0.28786931457981074 0.3982 0.386921
2022-06-06 20:26:32.054532: Epoch 99/100, Test: HR = 0.5657, NDCG = 0.3431
2022-06-06 20:26:33.768395: Model Saved: yelp

epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1711 0.1711 0.28799230276617377 0.3984 0.3869
2022-06-06 20:27:22.015820: Epoch 100/100, Test: HR = 0.5655, NDCG = 0.3431
2022-06-06 20:27:23.575417: Model Saved: yelp
root@container-f87d1190ac-f968e13b:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2 --temp 0lr 15e-4
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (cated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (cated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (cated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (cated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (cated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (cated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Pas deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Pas deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Pas deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Pas deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Pas deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Pas deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto i

2022-06-06 21:06:06.588755: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-06 21:06:06.958928: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:241: The name tf.placeholder is deprecated. Please use tf.compat.v1.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.c

drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("Spars), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prversion.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("Spars), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("Spaat32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("Spaat32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("Spaat32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("Spaat32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("Spaat32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("Spaat32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("Spaat32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("Spaat32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("Spaat32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("Spaat32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("Spaat32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("Spaat32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("Spar32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("Spar32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecat
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calthon.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7a9db71ed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `ex output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7a9db7ibute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7a9dba8ed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `ex output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7a9dbaibute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.exponential_decay is deprecated. Please use t

WARNING:tensorflow:From /root/CLSR/model.py:287: The name tf.train.AdamOptimizer is deprecated. Please use tf.co

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: adlow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-06 21:06:29.914017: Model Prepared
2022-06-06 21:06:32.598369: Variables Inited
2022-06-06 21:07:21.991324: Epoch 0/100, Train: Loss = 9.5393, preLoss = 3.6006
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0234 0.0234 0.057473836786240226 0.0914 0.109
2022-06-06 21:08:11.183544: Epoch 0/100, Test: HR = 0.1618, NDCG = 0.0799
WARNING:tensorflow:From /root/CLSR/model.py:524: The name tf.train.Saver is deprecated. Please use tf.compat.v1.

2022-06-06 21:08:13.093575: Model Saved: yelp

2022-06-06 21:08:54.289029: Epoch 1/100, Train: Loss = 20.8422, preLoss = 7.1483

2022-06-06 21:09:34.343651: Epoch 2/100, Train: Loss = 26.2733, preLoss = 7.5764

2022-06-06 21:10:16.178464: Epoch 3/100, Train: Loss = 28.9062, preLoss = 6.8733
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0527 0.0527 0.10728136730391552 0.1624 0.1725
2022-06-06 21:11:06.531637: Epoch 3/100, Test: HR = 0.2540, NDCG = 0.1366
2022-06-06 21:11:07.891676: Model Saved: yelp

2022-06-06 21:11:48.754970: Epoch 4/100, Train: Loss = 29.3581, preLoss = 5.7778

2022-06-06 21:12:29.420713: Epoch 5/100, Train: Loss = 28.0298, preLoss = 4.5189

2022-06-06 21:13:09.950208: Epoch 6/100, Train: Loss = 25.8524, preLoss = 3.6520
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0599 0.0599 0.13026937454581755 0.1987 0.2049
2022-06-06 21:14:01.433920: Epoch 6/100, Test: HR = 0.3099, NDCG = 0.1661
2022-06-06 21:14:02.841840: Model Saved: yelp

2022-06-06 21:14:43.312731: Epoch 7/100, Train: Loss = 23.0747, preLoss = 2.8543

2022-06-06 21:15:23.700717: Epoch 8/100, Train: Loss = 20.2087, preLoss = 2.2391

2022-06-06 21:16:04.431877: Epoch 9/100, Train: Loss = 17.4658, preLoss = 1.7930
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0739 0.0739 0.1475713972027074 0.2199 0.22822
2022-06-06 21:16:55.229090: Epoch 9/100, Test: HR = 0.3382, NDCG = 0.1856
2022-06-06 21:16:56.661094: Model Saved: yelp

2022-06-06 21:17:37.125046: Epoch 10/100, Train: Loss = 14.8554, preLoss = 1.3913

2022-06-06 21:18:17.569616: Epoch 11/100, Train: Loss = 12.5326, preLoss = 1.0638

2022-06-06 21:18:58.587362: Epoch 12/100, Train: Loss = 10.5566, preLoss = 0.8541
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0706 0.0706 0.15265804613504264 0.2341 0.2370
2022-06-06 21:19:47.516716: Epoch 12/100, Test: HR = 0.3694, NDCG = 0.1962
2022-06-06 21:19:48.855108: Model Saved: yelp

2022-06-06 21:20:29.051512: Epoch 13/100, Train: Loss = 8.8503, preLoss = 0.6685

2022-06-06 21:21:09.538405: Epoch 14/100, Train: Loss = 7.4455, preLoss = 0.5562

2022-06-06 21:21:50.512159: Epoch 15/100, Train: Loss = 6.2534, preLoss = 0.4423
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0779 0.0779 0.1635827387904649 0.2463 0.24553
2022-06-06 21:22:40.537915: Epoch 15/100, Test: HR = 0.3736, NDCG = 0.2045
2022-06-06 21:22:41.967350: Model Saved: yelp

2022-06-06 21:23:22.726353: Epoch 16/100, Train: Loss = 5.2612, preLoss = 0.3641

2022-06-06 21:24:02.786059: Epoch 17/100, Train: Loss = 4.4373, preLoss = 0.2981

2022-06-06 21:24:43.186158: Epoch 18/100, Train: Loss = 3.7812, preLoss = 0.2504
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0895 0.0895 0.17645218211860406 0.2631 0.2618
2022-06-06 21:25:32.317892: Epoch 18/100, Test: HR = 0.3959, NDCG = 0.2193
2022-06-06 21:25:33.675115: Model Saved: yelp

2022-06-06 21:26:14.335094: Epoch 19/100, Train: Loss = 3.2471, preLoss = 0.2173

2022-06-06 21:26:54.375242: Epoch 20/100, Train: Loss = 2.8101, preLoss = 0.1945

2022-06-06 21:27:35.296549: Epoch 21/100, Train: Loss = 2.4540, preLoss = 0.1708
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1019 0.1019 0.19211840014040266 0.2829 0.2761
2022-06-06 21:28:25.065409: Epoch 21/100, Test: HR = 0.4176, NDCG = 0.2356
2022-06-06 21:28:26.533415: Model Saved: yelp

2022-06-06 21:29:07.436666: Epoch 22/100, Train: Loss = 2.1573, preLoss = 0.1568

2022-06-06 21:29:48.346164: Epoch 23/100, Train: Loss = 1.9129, preLoss = 0.1419

2022-06-06 21:30:29.230112: Epoch 24/100, Train: Loss = 1.7112, preLoss = 0.1339
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0966 0.0966 0.19588441247420496 0.2943 0.2843
2022-06-06 21:31:18.006218: Epoch 24/100, Test: HR = 0.4398, NDCG = 0.2429
2022-06-06 21:31:19.482921: Model Saved: yelp

2022-06-06 21:32:00.146279: Epoch 25/100, Train: Loss = 1.5462, preLoss = 0.1278

2022-06-06 21:32:40.671006: Epoch 26/100, Train: Loss = 1.4090, preLoss = 0.1219

2022-06-06 21:33:22.263415: Epoch 27/100, Train: Loss = 1.2937, preLoss = 0.1174
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1113 0.1113 0.21659010389629108 0.3186 0.3044
2022-06-06 21:34:11.055842: Epoch 27/100, Test: HR = 0.4659, NDCG = 0.2644
2022-06-06 21:34:12.455134: Model Saved: yelp

2022-06-06 21:34:52.020382: Epoch 28/100, Train: Loss = 1.1958, preLoss = 0.1146

2022-06-06 21:35:31.986209: Epoch 29/100, Train: Loss = 1.1146, preLoss = 0.1115

2022-06-06 21:36:11.952954: Epoch 30/100, Train: Loss = 1.0454, preLoss = 0.1083
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1365 0.1365 0.23077685251200697 0.3251 0.3220
2022-06-06 21:37:00.242424: Epoch 30/100, Test: HR = 0.4817, NDCG = 0.2818
2022-06-06 21:37:01.673857: Model Saved: yelp

2022-06-06 21:37:42.397527: Epoch 31/100, Train: Loss = 0.9835, preLoss = 0.1077

2022-06-06 21:38:22.692122: Epoch 32/100, Train: Loss = 0.9297, preLoss = 0.1066

2022-06-06 21:39:02.480757: Epoch 33/100, Train: Loss = 0.8788, preLoss = 0.1032
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1116 0.1116 0.21862141137210428 0.3216 0.3169
2022-06-06 21:39:51.104209: Epoch 33/100, Test: HR = 0.4983, NDCG = 0.2757
2022-06-06 21:39:52.402444: Model Saved: yelp

2022-06-06 21:40:33.036585: Epoch 34/100, Train: Loss = 0.8410, preLoss = 0.1044

2022-06-06 21:41:14.309904: Epoch 35/100, Train: Loss = 0.8026, preLoss = 0.1019

2022-06-06 21:41:54.557760: Epoch 36/100, Train: Loss = 0.7696, preLoss = 0.1006
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1288 0.1288 0.2403669828070591 0.3522 0.33192
2022-06-06 21:42:43.046363: Epoch 36/100, Test: HR = 0.5054, NDCG = 0.2900
2022-06-06 21:42:44.611019: Model Saved: yelp

2022-06-06 21:43:24.994472: Epoch 37/100, Train: Loss = 0.7373, preLoss = 0.0997

2022-06-06 21:44:05.360647: Epoch 38/100, Train: Loss = 0.7112, preLoss = 0.0993

2022-06-06 21:44:45.502355: Epoch 39/100, Train: Loss = 0.6848, preLoss = 0.0964
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.136 0.136 0.24946267891652424 0.3549 0.3403606739962824 0.6722
2022-06-06 21:45:33.937179: Epoch 39/100, Test: HR = 0.5071, NDCG = 0.2987
2022-06-06 21:45:35.397175: Model Saved: yelp

2022-06-06 21:46:16.490532: Epoch 40/100, Train: Loss = 0.6668, preLoss = 0.0980

2022-06-06 21:46:56.841940: Epoch 41/100, Train: Loss = 0.6453, preLoss = 0.0985

2022-06-06 21:47:37.502978: Epoch 42/100, Train: Loss = 0.6284, preLoss = 0.0973
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1236 0.1236 0.26118809699506 0.3914 0.3445464949225358 0.6823
2022-06-06 21:48:25.678926: Epoch 42/100, Test: HR = 0.5316, NDCG = 0.3066
2022-06-06 21:48:27.135218: Model Saved: yelp

2022-06-06 21:49:07.274247: Epoch 43/100, Train: Loss = 0.6079, preLoss = 0.0943

2022-06-06 21:49:47.001650: Epoch 44/100, Train: Loss = 0.5902, preLoss = 0.0937

2022-06-06 21:50:27.310837: Epoch 45/100, Train: Loss = 0.5765, preLoss = 0.0941
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1425 0.1425 0.25862510107982073 0.3696 0.34785617708779626 0.6782
2022-06-06 21:51:15.419873: Epoch 45/100, Test: HR = 0.5266, NDCG = 0.3095
2022-06-06 21:51:16.901615: Model Saved: yelp

2022-06-06 21:51:57.474069: Epoch 46/100, Train: Loss = 0.5656, preLoss = 0.0946

2022-06-06 21:52:38.165928: Epoch 47/100, Train: Loss = 0.5525, preLoss = 0.0920

2022-06-06 21:53:18.907985: Epoch 48/100, Train: Loss = 0.5364, preLoss = 0.0911
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1406 0.1406 0.2618734195384394 0.3768 0.35092627950144284 0.6832
2022-06-06 21:54:08.331773: Epoch 48/100, Test: HR = 0.5339, NDCG = 0.3133
2022-06-06 21:54:09.871058: Model Saved: yelp

2022-06-06 21:54:50.087314: Epoch 49/100, Train: Loss = 0.5291, preLoss = 0.0906

2022-06-06 21:55:30.349223: Epoch 50/100, Train: Loss = 0.5169, preLoss = 0.0906

2022-06-06 21:56:10.622174: Epoch 51/100, Train: Loss = 0.5050, preLoss = 0.0875
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1495 0.1495 0.266468488838139 0.3799 0.3554984089393355 0.6859
2022-06-06 21:56:59.189627: Epoch 51/100, Test: HR = 0.5392, NDCG = 0.3186
2022-06-06 21:57:00.701209: Model Saved: yelp

2022-06-06 21:57:41.161965: Epoch 52/100, Train: Loss = 0.4990, preLoss = 0.0893

2022-06-06 21:58:21.762057: Epoch 53/100, Train: Loss = 0.4884, preLoss = 0.0888

2022-06-06 21:59:02.101579: Epoch 54/100, Train: Loss = 0.4805, preLoss = 0.0873
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.157 0.157 0.27072230851085294 0.3794 0.36333106534765297 0.6979
2022-06-06 21:59:52.034289: Epoch 54/100, Test: HR = 0.5435, NDCG = 0.3245
2022-06-06 21:59:53.606097: Model Saved: yelp

2022-06-06 22:00:34.129351: Epoch 55/100, Train: Loss = 0.4730, preLoss = 0.0866

2022-06-06 22:01:14.325755: Epoch 56/100, Train: Loss = 0.4675, preLoss = 0.0875

2022-06-06 22:01:54.902598: Epoch 57/100, Train: Loss = 0.4583, preLoss = 0.0859
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1355 0.1355 0.2614173044787987 0.378 0.3558845167530677 0.7036
2022-06-06 22:02:43.588129: Epoch 57/100, Test: HR = 0.5425, NDCG = 0.3154
2022-06-06 22:02:45.114014: Model Saved: yelp

2022-06-06 22:03:25.455846: Epoch 58/100, Train: Loss = 0.4515, preLoss = 0.0854

2022-06-06 22:04:09.670339: Epoch 59/100, Train: Loss = 0.4453, preLoss = 0.0838

2022-06-06 22:04:51.363581: Epoch 60/100, Train: Loss = 0.4387, preLoss = 0.0842
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.138 0.138 0.26480390798708053 0.3806 0.35973254467059756 0.7096
2022-06-06 22:05:40.242414: Epoch 60/100, Test: HR = 0.5405, NDCG = 0.3173
2022-06-06 22:05:41.840664: Model Saved: yelp

2022-06-06 22:06:22.210663: Epoch 61/100, Train: Loss = 0.4355, preLoss = 0.0838

2022-06-06 22:07:03.389125: Epoch 62/100, Train: Loss = 0.4263, preLoss = 0.0828

2022-06-06 22:07:44.257811: Epoch 63/100, Train: Loss = 0.4219, preLoss = 0.0810
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1278 0.1278 0.2595951028932516 0.3793 0.35580302922240714 0.7127
2022-06-06 22:08:32.810918: Epoch 63/100, Test: HR = 0.5428, NDCG = 0.3132
2022-06-06 22:08:34.308910: Model Saved: yelp

2022-06-06 22:09:14.404074: Epoch 64/100, Train: Loss = 0.4200, preLoss = 0.0820

2022-06-06 22:09:54.360562: Epoch 65/100, Train: Loss = 0.4130, preLoss = 0.0817

2022-06-06 22:10:34.172631: Epoch 66/100, Train: Loss = 0.4101, preLoss = 0.0812
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1338 0.1338 0.25957615240911724 0.3696 0.358448351662645 0.7086
2022-06-06 22:11:22.820199: Epoch 66/100, Test: HR = 0.5448, NDCG = 0.3172
2022-06-06 22:11:24.406964: Model Saved: yelp

2022-06-06 22:12:04.226663: Epoch 67/100, Train: Loss = 0.4056, preLoss = 0.0807

2022-06-06 22:12:44.115526: Epoch 68/100, Train: Loss = 0.4001, preLoss = 0.0809

2022-06-06 22:13:24.695904: Epoch 69/100, Train: Loss = 0.3982, preLoss = 0.0809
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1313 0.1313 0.2658075658990257 0.3824 0.3623804672588835 0.7157
2022-06-06 22:14:13.470289: Epoch 69/100, Test: HR = 0.5478, NDCG = 0.3203
2022-06-06 22:14:15.052410: Model Saved: yelp

2022-06-06 22:14:55.481259: Epoch 70/100, Train: Loss = 0.3910, preLoss = 0.0803

2022-06-06 22:15:36.982612: Epoch 71/100, Train: Loss = 0.3922, preLoss = 0.0802

2022-06-06 22:16:17.332919: Epoch 72/100, Train: Loss = 0.3865, preLoss = 0.0793
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1398 0.1398 0.27171284636990245 0.3911 0.36540539223592616 0.7166
2022-06-06 22:17:05.221846: Epoch 72/100, Test: HR = 0.5482, NDCG = 0.3230
2022-06-06 22:17:06.821793: Model Saved: yelp

2022-06-06 22:17:47.332257: Epoch 73/100, Train: Loss = 0.3831, preLoss = 0.0784

2022-06-06 22:18:27.413111: Epoch 74/100, Train: Loss = 0.3793, preLoss = 0.0768

2022-06-06 22:19:08.212600: Epoch 75/100, Train: Loss = 0.3760, preLoss = 0.0786
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1453 0.1453 0.27448114698645504 0.3934 0.36848813775476824 0.7195
2022-06-06 22:19:55.979231: Epoch 75/100, Test: HR = 0.5486, NDCG = 0.3253
2022-06-06 22:19:57.592220: Model Saved: yelp

2022-06-06 22:20:37.992656: Epoch 76/100, Train: Loss = 0.3741, preLoss = 0.0772

2022-06-06 22:21:17.663954: Epoch 77/100, Train: Loss = 0.3697, preLoss = 0.0785

2022-06-06 22:21:57.338012: Epoch 78/100, Train: Loss = 0.3669, preLoss = 0.0768
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1466 0.1466 0.274781487220283 0.3899 0.37171805273450204 0.7245
2022-06-06 22:22:44.796542: Epoch 78/100, Test: HR = 0.5577, NDCG = 0.3298
2022-06-06 22:22:46.433123: Model Saved: yelp

2022-06-06 22:23:27.025785: Epoch 79/100, Train: Loss = 0.3664, preLoss = 0.0773

2022-06-06 22:24:06.565024: Epoch 80/100, Train: Loss = 0.3634, preLoss = 0.0761

2022-06-06 22:24:46.088554: Epoch 81/100, Train: Loss = 0.3606, preLoss = 0.0767
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.15 0.15 0.2770027457379918 0.3922 0.3735834346327218 0.725
2022-06-06 22:25:33.520524: Epoch 81/100, Test: HR = 0.5584, NDCG = 0.3317
2022-06-06 22:25:35.035973: Model Saved: yelp

2022-06-06 22:26:15.081641: Epoch 82/100, Train: Loss = 0.3605, preLoss = 0.0770

2022-06-06 22:26:54.620334: Epoch 83/100, Train: Loss = 0.3572, preLoss = 0.0765

2022-06-06 22:27:34.288146: Epoch 84/100, Train: Loss = 0.3531, preLoss = 0.0743
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1547 0.1547 0.2829148002716082 0.3961 0.3778188576785234 0.7223
2022-06-06 22:28:22.615370: Epoch 84/100, Test: HR = 0.5626, NDCG = 0.3377
2022-06-06 22:28:24.336908: Model Saved: yelp

2022-06-06 22:29:05.178298: Epoch 85/100, Train: Loss = 0.3523, preLoss = 0.0751

2022-06-06 22:29:45.553448: Epoch 86/100, Train: Loss = 0.3516, preLoss = 0.0749

2022-06-06 22:30:25.565973: Epoch 87/100, Train: Loss = 0.3475, preLoss = 0.0747
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1553 0.1553 0.28500907806449877 0.3995 0.37920265700145045 0.7235
2022-06-06 22:31:14.173925: Epoch 87/100, Test: HR = 0.5638, NDCG = 0.3390
2022-06-06 22:31:15.930532: Model Saved: yelp

2022-06-06 22:31:56.585493: Epoch 88/100, Train: Loss = 0.3487, preLoss = 0.0759

2022-06-06 22:32:37.122905: Epoch 89/100, Train: Loss = 0.3460, preLoss = 0.0741

2022-06-06 22:33:16.822407: Epoch 90/100, Train: Loss = 0.3427, preLoss = 0.0734
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.158 0.158 0.2890557064004018 0.4034 0.3823741843333613 0.7244
2022-06-06 22:34:04.560579: Epoch 90/100, Test: HR = 0.5667, NDCG = 0.3426
2022-06-06 22:34:06.187677: Model Saved: yelp

2022-06-06 22:34:46.560024: Epoch 91/100, Train: Loss = 0.3398, preLoss = 0.0731

2022-06-06 22:35:27.095661: Epoch 92/100, Train: Loss = 0.3383, preLoss = 0.0715

2022-06-06 22:36:07.112828: Epoch 93/100, Train: Loss = 0.3394, preLoss = 0.0732
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1604 0.1604 0.2912873767660356 0.4046 0.38439114385006207 0.7244
2022-06-06 22:36:55.280158: Epoch 93/100, Test: HR = 0.5683, NDCG = 0.3450
2022-06-06 22:36:56.975286: Model Saved: yelp

2022-06-06 22:37:36.602082: Epoch 94/100, Train: Loss = 0.3374, preLoss = 0.0733

2022-06-06 22:38:16.120381: Epoch 95/100, Train: Loss = 0.3358, preLoss = 0.0730

2022-06-06 22:38:56.064993: Epoch 96/100, Train: Loss = 0.3362, preLoss = 0.0726
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1604 0.1604 0.28831347888471726 0.4025 0.38228337536479834 0.7259
2022-06-06 22:39:43.865320: Epoch 96/100, Test: HR = 0.5701, NDCG = 0.3431
2022-06-06 22:39:45.532864: Model Saved: yelp

2022-06-06 22:40:24.811423: Epoch 97/100, Train: Loss = 0.3343, preLoss = 0.0735

2022-06-06 22:41:04.693327: Epoch 98/100, Train: Loss = 0.3329, preLoss = 0.0726

2022-06-06 22:41:44.373423: Epoch 99/100, Train: Loss = 0.3318, preLoss = 0.0729
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1597 0.1597 0.28287573140400896 0.3913 0.3817591612074124 0.7296
2022-06-06 22:42:32.683510: Epoch 99/100, Test: HR = 0.5702, NDCG = 0.3416
2022-06-06 22:42:34.176115: Model Saved: yelp

epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1596 0.1596 0.2827664271583877 0.3913 0.3816940001665818 0.7298
2022-06-06 22:43:23.038689: Epoch 100/100, Test: HR = 0.5703, NDCG = 0.3415
2022-06-06 22:43:24.740210: Model Saved: yelp
root@container-f87d1190ac-f968e13b:~/CLSR#
