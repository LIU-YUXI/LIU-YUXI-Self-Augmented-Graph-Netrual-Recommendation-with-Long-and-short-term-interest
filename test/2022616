     ┌────────────────────────────────────────────────────────────────────┐
     │                        • MobaXterm 20.3 •                          │
     │            (SSH client, X-server and networking tools)             │
     │                                                                    │
     │ ➤ SSH session to root@region-3.autodl.com                          │
     │   • SSH compression : ✔                                            │
     │   • SSH-browser     : ✔                                            │
     │   • X11-forwarding  : ✘  (disabled or not supported by server)     │
     │   • DISPLAY         : 192.168.1.107:0.0                            │
     │                                                                    │
     │ ➤ For more info, ctrl+click on help or visit our website           │
     └────────────────────────────────────────────────────────────────────┘

Welcome to Ubuntu 18.04.6 LTS (GNU/Linux 5.4.0-96-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
This system has been minimized by removing packages and content that are
not required on a system that users do not log into.

To restore this content, you can run the 'unminimize' command.
Last login: Wed Jun 15 23:27:34 2022 from 127.0.0.1
+--------------------------------------------------AutoDL--------------------------------------------------------+
目录说明:
╔═════════════════╦══════╦════╦═════════════════════════════════════════════════════════════════════════╗
║目录             ║名称  ║速度║说明                                                                     ║
╠═════════════════╬══════╬════╬═════════════════════════════════════════════════════════════════════════╣
║/                ║系统盘║快  ║实例关机数据不会丢失，可存放代码等。会随保存镜像一起保存。               ║
║/root/autodl-tmp ║数据盘║快  ║实例关机数据不会丢失，可存放读写IO要求高的数据。但不会随保存镜像一起保存 ║
╚═════════════════╩══════╩════╩═════════════════════════════════════════════════════════════════════════╝
CPU ：7 核心
内存：16 GB
GPU ：NVIDIA TITAN Xp, 1
存储：
  系统盘/               ：15% 2.9G/20G
  数据盘/root/autodl-tmp：0% 0/50G
+----------------------------------------------------------------------------------------------------------------+
*注意:
1.系统盘较小请将大的数据存放于数据盘或网盘中，重置系统时数据盘和网盘中的数据不受影响
2.清理系统盘请参考：https://www.autodl.com/docs/qa/
root@container-327e11a8ac-4a1523bb:~# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2 --temp  0.1 --ssl_reg 1e-7 --save_path yelp          --batch 256 --epoch 150 --load yelp
python: can't open file 'main.py': [Errno 2] No such file or directory
root@container-327e11a8ac-4a1523bb:~# cd ./CLSR
root@container-327e11a8ac-4a1523bb:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2 --temp  0.1 --ssl_reg 1e-7 --save_path          yelp --batch 256 --epoch 150 --load yelp
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2022-06-16 11:00:42.278716: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-16 11:00:42.428966: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:241: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("SparseTensor/values:0", shape=(33516         8,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be re         moved in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("SparseTensor/values:0", shape=(33516         8,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("SparseTensor_1/values:0", shape=(1         77378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("SparseTensor_1/values:0", shape=(1         77378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("SparseTensor_2/values:0", shape=(1         56718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("SparseTensor_2/values:0", shape=(1         56718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("SparseTensor_3/values:0", shape=(1         56674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("SparseTensor_3/values:0", shape=(1         56674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("SparseTensor_4/values:0", shape=(1         59442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("SparseTensor_4/values:0", shape=(1         59442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("SparseTensor_5/values:0", shape=(1         55622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("SparseTensor_5/values:0", shape=(1         55622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("SparseTensor_6/values:0", shape=(1         41690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("SparseTensor_6/values:0", shape=(1         41690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("SparseTensor_7/values:0", shape=(85         108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("SparseTensor_7/values:0", shape=(85         108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a fut         ure version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (         from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efdf0731d10>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef         df0731d10>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7efeee22b910>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ef         eee22b910>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_d         ecay instead.

WARNING:tensorflow:From /root/CLSR/model.py:287: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer ins         tead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wra         pper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-16 11:01:05.695599: Model Prepared
WARNING:tensorflow:From /root/CLSR/model.py:529: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorfl         ow.python.training.checkpoint_management) is deprecated and will be removed in a future version.
Instructions for updating:
Use standard file APIs to check for files with this prefix.
2022-06-16 11:01:09.107202: Model Loaded
2022-06-16 11:02:02.743458: Epoch 82/150, Train: Loss = 0.2345, preLoss = 0.0742

2022-06-16 11:02:45.864861: Epoch 83/150, Train: Loss = 0.2334, preLoss = 0.0751

2022-06-16 11:03:29.154699: Epoch 84/150, Train: Loss = 0.2296, preLoss = 0.0730
[ 0.          0.04693499 -0.5600691  ... -0.04204351 -0.1557848876
  0.06041965]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1729 0.1729 0.3014674746308035 0.4208 0.3982106962352689 0.7544
2022-06-16 11:04:18.984719: Epoch 84/150, Test: HR = 0.5872, NDCG = 0.3560
2022-06-16 11:04:20.503813: Model Saved: yelp

2022-06-16 11:05:03.765146: Epoch 85/150, Train: Loss = 0.2280, preLoss = 0.0732

2022-06-16 11:05:47.244827: Epoch 86/150, Train: Loss = 0.2270, preLoss = 0.0739

2022-06-16 11:06:30.242907: Epoch 87/150, Train: Loss = 0.2236, preLoss = 0.0722
[ 0.         -0.05098107 -0.6325222  ...  0.06665118 -0.1260633686
  0.03685326]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1741 0.1741 0.3079052058953703 0.4341 0.4009041570989711 0.7548
2022-06-16 11:07:20.753643: Epoch 87/150, Test: HR = 0.5910, NDCG = 0.3595
2022-06-16 11:07:22.239386: Model Saved: yelp

2022-06-16 11:08:06.125826: Epoch 88/150, Train: Loss = 0.2219, preLoss = 0.0721

2022-06-16 11:08:49.186290: Epoch 89/150, Train: Loss = 0.2202, preLoss = 0.0720

2022-06-16 11:09:35.739545: Epoch 90/150, Train: Loss = 0.2171, preLoss = 0.0705
[ 0.         -0.20399794 -0.42077154 ...  0.13974504 -0.2120484665
  0.05026042]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1694 0.1694 0.3092174051650782 0.4411 0.39867500462382427 0.7502
2022-06-16 11:10:27.094696: Epoch 90/150, Test: HR = 0.5971, NDCG = 0.3601
2022-06-16 11:10:29.716653: Model Saved: yelp

2022-06-16 11:11:12.810539: Epoch 91/150, Train: Loss = 0.2167, preLoss = 0.0715

2022-06-16 11:11:56.165272: Epoch 92/150, Train: Loss = 0.2155, preLoss = 0.0717

2022-06-16 11:12:39.924933: Epoch 93/150, Train: Loss = 0.2126, preLoss = 0.0700
[ 0.         -0.18757349 -0.37099558 ...  0.21536107 -0.1388498263
  0.04501989]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1648 0.1648 0.3085020048204714 0.4442 0.3976714154170661 0.7522
2022-06-16 11:13:29.985591: Epoch 93/150, Test: HR = 0.6029, NDCG = 0.3600
2022-06-16 11:13:32.187960: Model Saved: yelp

2022-06-16 11:14:15.942755: Epoch 94/150, Train: Loss = 0.2103, preLoss = 0.0690

2022-06-16 11:14:59.622686: Epoch 95/150, Train: Loss = 0.2100, preLoss = 0.0700

2022-06-16 11:15:42.954686: Epoch 96/150, Train: Loss = 0.2083, preLoss = 0.0695
[ 0.         -0.1354089  -0.3333862  ...  0.25611958 -0.1483540862
  0.09686773]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1634 0.1634 0.302979682659797 0.4344 0.397382550775042 0.7586
2022-06-16 11:16:32.772425: Epoch 96/150, Test: HR = 0.6034, NDCG = 0.3582
2022-06-16 11:16:35.430067: Model Saved: yelp

2022-06-16 11:17:18.317711: Epoch 97/150, Train: Loss = 0.2073, preLoss = 0.0696

2022-06-16 11:18:01.718235: Epoch 98/150, Train: Loss = 0.2076, preLoss = 0.0710

2022-06-16 11:18:44.918256: Epoch 99/150, Train: Loss = 0.2038, preLoss = 0.0682
[ 0.         -0.15507132 -0.32478043 ...  0.17656727 -0.1693218233
  0.08435403]
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1619 0.1619 0.3034351632837554 0.4389 0.3952282268145332 0.7539
2022-06-16 11:19:34.281171: Epoch 99/150, Test: HR = 0.6023, NDCG = 0.3569
2022-06-16 11:19:36.368965: Model Saved: yelp

2022-06-16 11:20:19.242925: Epoch 100/150, Train: Loss = 0.2052, preLoss = 0.0706

^CTraceback (most recent call last):40: preloss = 0.07, REGLoss = 0.13
  File "main.py", line 25, in <module>
    recom.run()
  File "/root/CLSR/model.py", line 50, in run
    reses = self.trainEpoch()
  File "/root/CLSR/model.py", line 390, in trainEpoch
    uLocs, iLocs, timeLocs = self.sampleTrainBatch(batIds, self.handler.trnMat, self.handler.timeMat, sample_num_list[s])
  File "/root/CLSR/model.py", line 315, in sampleTrainBatch
    timeLocs[cur] = timeMat[batIds[i],posloc]
  File "/root/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py", line 37, in __getitem__
    return self._get_intXint(row, col)
KeyboardInterrupt

root@container-327e11a8ac-4a1523bb:~/CLSR#
root@container-327e11a8ac-4a1523bb:~/CLSR# nvidia-smi
Thu Jun 16 11:22:01 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 495.44       Driver Version: 495.44       CUDA Version: 11.5     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA TITAN Xp     On   | 00000000:85:00.0 Off |                  N/A |
| 36%   59C    P2    79W / 250W |   4379MiB / 12196MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
+-----------------------------------------------------------------------------+
root@container-327e11a8ac-4a1523bb:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp  0.1 --ssl_reg 1e-7 --s         ave_path yelp --batch 512 --epoch 150
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2022-06-16 11:22:42.817518: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-16 11:22:42.976747: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:241: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("SparseTensor/values:0", shape=(33516         8,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be re         moved in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("SparseTensor/values:0", shape=(33516         8,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("SparseTensor_1/values:0", shape=(1         77378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("SparseTensor_1/values:0", shape=(1         77378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("SparseTensor_2/values:0", shape=(1         56718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("SparseTensor_2/values:0", shape=(1         56718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("SparseTensor_3/values:0", shape=(1         56674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("SparseTensor_3/values:0", shape=(1         56674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("SparseTensor_4/values:0", shape=(1         59442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("SparseTensor_4/values:0", shape=(1         59442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("SparseTensor_5/values:0", shape=(1         55622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("SparseTensor_5/values:0", shape=(1         55622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("SparseTensor_6/values:0", shape=(1         41690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("SparseTensor_6/values:0", shape=(1         41690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("SparseTensor_7/values:0", shape=(85         108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("SparseTensor_7/values:0", shape=(85         108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a fut         ure version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (         from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd738628310>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd         738628310>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd82c61c890>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fd         82c61c890>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_d         ecay instead.

WARNING:tensorflow:From /root/CLSR/model.py:287: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer ins         tead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wra         pper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-16 11:23:05.713643: Model Prepared
2022-06-16 11:23:09.726702: Variables Inited
2022-06-16 11:23:58.324771: Epoch 0/150, Train: Loss = 5.2499, preLoss = 2.2079
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.019 0.019 0.05221236933390601 0.086 0.10556524540552575 0.2807
2022-06-16 11:24:46.081825: Epoch 0/150, Test: HR = 0.1527, NDCG = 0.0735
WARNING:tensorflow:From /root/CLSR/model.py:524: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

2022-06-16 11:24:49.770863: Model Saved: yelp

2022-06-16 11:25:28.889422: Epoch 1/150, Train: Loss = 9.6817, preLoss = 3.6597

2022-06-16 11:26:08.246509: Epoch 2/150, Train: Loss = 11.8582, preLoss = 3.9342

2022-06-16 11:26:47.765536: Epoch 3/150, Train: Loss = 13.3694, preLoss = 3.9640
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0591 0.0591 0.12236223505943064 0.1857 0.19441201767810623 0.4433
2022-06-16 11:27:36.678011: Epoch 3/150, Test: HR = 0.2914, NDCG = 0.1564
2022-06-16 11:27:38.834804: Model Saved: yelp

2022-06-16 11:28:18.082332: Epoch 4/150, Train: Loss = 14.1243, preLoss = 3.6618

2022-06-16 11:28:57.303924: Epoch 5/150, Train: Loss = 14.1606, preLoss = 3.1699

2022-06-16 11:29:36.736386: Epoch 6/150, Train: Loss = 13.6061, preLoss = 2.6156
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0609 0.0609 0.13010108532396017 0.199 0.20962210677512594 0.4825
2022-06-16 11:30:26.412511: Epoch 6/150, Test: HR = 0.3169, NDCG = 0.1680
2022-06-16 11:30:28.293487: Model Saved: yelp

2022-06-16 11:31:07.997283: Epoch 7/150, Train: Loss = 12.8601, preLoss = 2.2523

2022-06-16 11:31:47.897397: Epoch 8/150, Train: Loss = 11.9224, preLoss = 1.9339

2022-06-16 11:32:27.066244: Epoch 9/150, Train: Loss = 10.8214, preLoss = 1.5798
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0618 0.0618 0.1409774050346341 0.2213 0.22397835568743976 0.5176
2022-06-16 11:33:15.896146: Epoch 9/150, Test: HR = 0.3421, NDCG = 0.1799
2022-06-16 11:33:18.040556: Model Saved: yelp

2022-06-16 11:33:57.612350: Epoch 10/150, Train: Loss = 9.8083, preLoss = 1.3552

2022-06-16 11:34:37.287044: Epoch 11/150, Train: Loss = 8.7724, preLoss = 1.1101

2022-06-16 11:35:17.136619: Epoch 12/150, Train: Loss = 7.7957, preLoss = 0.9261
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0753 0.0753 0.15869768004740603 0.2434 0.241961795042208 0.5393
2022-06-16 11:36:05.798841: Epoch 12/150, Test: HR = 0.3665, NDCG = 0.1985
2022-06-16 11:36:07.431223: Model Saved: yelp

2022-06-16 11:36:46.614370: Epoch 13/150, Train: Loss = 6.9390, preLoss = 0.8086

2022-06-16 11:37:26.185625: Epoch 14/150, Train: Loss = 6.1197, preLoss = 0.6650

2022-06-16 11:38:06.113227: Epoch 15/150, Train: Loss = 5.3952, preLoss = 0.5644
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0698 0.0698 0.16164340979032973 0.2522 0.24933118831692333 0.563
2022-06-16 11:38:53.959117: Epoch 15/150, Test: HR = 0.3865, NDCG = 0.2049
2022-06-16 11:38:55.656246: Model Saved: yelp

2022-06-16 11:39:35.131535: Epoch 16/150, Train: Loss = 4.7495, preLoss = 0.4865

2022-06-16 11:40:14.109324: Epoch 17/150, Train: Loss = 4.1877, preLoss = 0.4308

2022-06-16 11:40:53.691846: Epoch 18/150, Train: Loss = 3.6860, preLoss = 0.3679
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0723 0.0723 0.1688651923772235 0.2641 0.2573047681394335 0.5778
2022-06-16 11:41:41.388265: Epoch 18/150, Test: HR = 0.3997, NDCG = 0.2125
2022-06-16 11:41:42.888061: Model Saved: yelp

2022-06-16 11:42:22.583253: Epoch 19/150, Train: Loss = 3.2406, preLoss = 0.3039

2022-06-16 11:43:01.686859: Epoch 20/150, Train: Loss = 2.8638, preLoss = 0.2694

2022-06-16 11:43:40.583595: Epoch 21/150, Train: Loss = 2.5366, preLoss = 0.2425
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0972 0.0972 0.1889390717810006 0.2789 0.27569717859696335 0.5861
2022-06-16 11:44:27.984281: Epoch 21/150, Test: HR = 0.4129, NDCG = 0.2321
2022-06-16 11:44:29.532707: Model Saved: yelp

2022-06-16 11:45:08.453712: Epoch 22/150, Train: Loss = 2.2471, preLoss = 0.2106

2022-06-16 11:45:47.104821: Epoch 23/150, Train: Loss = 2.0049, preLoss = 0.1891

2022-06-16 11:46:26.217385: Epoch 24/150, Train: Loss = 1.7900, preLoss = 0.1698
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0948 0.0948 0.19128013866896532 0.2861 0.280412498261582 0.6005
2022-06-16 11:47:12.760461: Epoch 24/150, Test: HR = 0.4287, NDCG = 0.2372
2022-06-16 11:47:14.263991: Model Saved: yelp

2022-06-16 11:47:52.673434: Epoch 25/150, Train: Loss = 1.6062, preLoss = 0.1564

2022-06-16 11:48:31.242404: Epoch 26/150, Train: Loss = 1.4401, preLoss = 0.1364

2022-06-16 11:49:09.761635: Epoch 27/150, Train: Loss = 1.3058, preLoss = 0.1303
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0992 0.0992 0.1922193043404038 0.2829 0.2845728439899783 0.6078
2022-06-16 11:49:56.331191: Epoch 27/150, Test: HR = 0.4314, NDCG = 0.2400
2022-06-16 11:49:57.839233: Model Saved: yelp

2022-06-16 11:50:36.974129: Epoch 28/150, Train: Loss = 1.1854, preLoss = 0.1214

2022-06-16 11:51:15.883518: Epoch 29/150, Train: Loss = 1.0766, preLoss = 0.1096

2022-06-16 11:51:54.516532: Epoch 30/150, Train: Loss = 0.9878, preLoss = 0.1062
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0959 0.0959 0.18559576788685866 0.2766 0.28339000884357174 0.6165
2022-06-16 11:52:41.215152: Epoch 30/150, Test: HR = 0.4414, NDCG = 0.2393
2022-06-16 11:52:42.739253: Model Saved: yelp

2022-06-16 11:53:21.506032: Epoch 31/150, Train: Loss = 0.9078, preLoss = 0.1005

2022-06-16 11:54:00.327658: Epoch 32/150, Train: Loss = 0.8373, preLoss = 0.0956

2022-06-16 11:54:38.908208: Epoch 33/150, Train: Loss = 0.7738, preLoss = 0.0901
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1116 0.1116 0.20550881053053982 0.2981 0.3030947747689985 0.6397
2022-06-16 11:55:25.628086: Epoch 33/150, Test: HR = 0.4560, NDCG = 0.2569
2022-06-16 11:55:27.156078: Model Saved: yelp

2022-06-16 11:56:05.479083: Epoch 34/150, Train: Loss = 0.7165, preLoss = 0.0840

2022-06-16 11:56:43.994613: Epoch 35/150, Train: Loss = 0.6726, preLoss = 0.0851

2022-06-16 11:57:22.588384: Epoch 36/150, Train: Loss = 0.6300, preLoss = 0.0818
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1029 0.1029 0.21151778254711617 0.3159 0.3067192372388327 0.6508
2022-06-16 11:58:09.646198: Epoch 36/150, Test: HR = 0.4702, NDCG = 0.2613
2022-06-16 11:58:11.234818: Model Saved: yelp

2022-06-16 11:58:50.264934: Epoch 37/150, Train: Loss = 0.5922, preLoss = 0.0791

2022-06-16 11:59:29.479933: Epoch 38/150, Train: Loss = 0.5587, preLoss = 0.0771

2022-06-16 12:00:08.306581: Epoch 39/150, Train: Loss = 0.5299, preLoss = 0.0765
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1458 0.1458 0.24678582501332688 0.3456 0.34155335612535004 0.6751
2022-06-16 12:00:55.280641: Epoch 39/150, Test: HR = 0.5060, NDCG = 0.2988
2022-06-16 12:00:56.805936: Model Saved: yelp

2022-06-16 12:01:35.498239: Epoch 40/150, Train: Loss = 0.5018, preLoss = 0.0736

2022-06-16 12:02:14.189150: Epoch 41/150, Train: Loss = 0.4760, preLoss = 0.0705

2022-06-16 12:02:53.052913: Epoch 42/150, Train: Loss = 0.4567, preLoss = 0.0719
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1626 0.1626 0.26666961810565115 0.3648 0.35910618664932237 0.6852
2022-06-16 12:03:39.973837: Epoch 42/150, Test: HR = 0.5248, NDCG = 0.3187
2022-06-16 12:03:41.644938: Model Saved: yelp

2022-06-16 12:04:20.273811: Epoch 43/150, Train: Loss = 0.4378, preLoss = 0.0718

2022-06-16 12:04:58.915166: Epoch 44/150, Train: Loss = 0.4188, preLoss = 0.0699

2022-06-16 12:05:38.591495: Epoch 45/150, Train: Loss = 0.4031, preLoss = 0.0694
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1665 0.1665 0.2744322758537707 0.3792 0.36384780107695613 0.6903
2022-06-16 12:06:27.612539: Epoch 45/150, Test: HR = 0.5293, NDCG = 0.3232
2022-06-16 12:06:29.208414: Model Saved: yelp

2022-06-16 12:07:08.421915: Epoch 46/150, Train: Loss = 0.3885, preLoss = 0.0685

2022-06-16 12:07:47.109884: Epoch 47/150, Train: Loss = 0.3731, preLoss = 0.0658

2022-06-16 12:08:25.756408: Epoch 48/150, Train: Loss = 0.3621, preLoss = 0.0665
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1472 0.1472 0.2618923192385937 0.3721 0.35719041500511456 0.7005
2022-06-16 12:09:12.650446: Epoch 48/150, Test: HR = 0.5366, NDCG = 0.3158
2022-06-16 12:09:14.195318: Model Saved: yelp

2022-06-16 12:09:53.291568: Epoch 49/150, Train: Loss = 0.3503, preLoss = 0.0655

2022-06-16 12:10:31.964334: Epoch 50/150, Train: Loss = 0.3408, preLoss = 0.0657

2022-06-16 12:11:10.969516: Epoch 51/150, Train: Loss = 0.3311, preLoss = 0.0650
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1497 0.1497 0.2759758962481943 0.3969 0.3611983572552413 0.6954
2022-06-16 12:11:57.760924: Epoch 51/150, Test: HR = 0.5359, NDCG = 0.3210
2022-06-16 12:11:59.407867: Model Saved: yelp

2022-06-16 12:12:38.511807: Epoch 52/150, Train: Loss = 0.3216, preLoss = 0.0638

2022-06-16 12:13:17.269109: Epoch 53/150, Train: Loss = 0.3144, preLoss = 0.0644

2022-06-16 12:13:57.595441: Epoch 54/150, Train: Loss = 0.3061, preLoss = 0.0633
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1591 0.1591 0.28216988024268363 0.4003 0.37037507900674804 0.7085
2022-06-16 12:14:44.634755: Epoch 54/150, Test: HR = 0.5487, NDCG = 0.3302
2022-06-16 12:14:46.226732: Model Saved: yelp

2022-06-16 12:15:24.740860: Epoch 55/150, Train: Loss = 0.2985, preLoss = 0.0624

2022-06-16 12:16:03.611484: Epoch 56/150, Train: Loss = 0.2913, preLoss = 0.0615

2022-06-16 12:16:42.306995: Epoch 57/150, Train: Loss = 0.2859, preLoss = 0.0619
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.15 0.15 0.2837642019425091 0.4099 0.3706746861019313 0.7127
2022-06-16 12:17:28.918403: Epoch 57/150, Test: HR = 0.5543, NDCG = 0.3307
2022-06-16 12:17:30.490254: Model Saved: yelp

2022-06-16 12:18:09.699629: Epoch 58/150, Train: Loss = 0.2808, preLoss = 0.0621

2022-06-16 12:18:48.584414: Epoch 59/150, Train: Loss = 0.2738, preLoss = 0.0602

2022-06-16 12:19:27.363305: Epoch 60/150, Train: Loss = 0.2693, preLoss = 0.0604
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1526 0.1526 0.2891654788178044 0.4172 0.37713184646604386 0.7219
2022-06-16 12:20:14.615413: Epoch 60/150, Test: HR = 0.5667, NDCG = 0.3380
2022-06-16 12:20:16.183040: Model Saved: yelp

2022-06-16 12:20:54.992123: Epoch 61/150, Train: Loss = 0.2649, preLoss = 0.0604

2022-06-16 12:21:33.516274: Epoch 62/150, Train: Loss = 0.2616, preLoss = 0.0612

2022-06-16 12:22:13.062817: Epoch 63/150, Train: Loss = 0.2571, preLoss = 0.0606
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1637 0.1637 0.2940477579282989 0.4176 0.38297823991442054 0.7253
2022-06-16 12:23:00.043223: Epoch 63/150, Test: HR = 0.5742, NDCG = 0.3449
2022-06-16 12:23:01.629109: Model Saved: yelp

2022-06-16 12:23:40.365239: Epoch 64/150, Train: Loss = 0.2522, preLoss = 0.0593

2022-06-16 12:24:18.576943: Epoch 65/150, Train: Loss = 0.2473, preLoss = 0.0579

2022-06-16 12:24:57.406981: Epoch 66/150, Train: Loss = 0.2440, preLoss = 0.0578
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1763 0.1763 0.301950184709831 0.4235 0.3900406239470888 0.729
2022-06-16 12:25:44.400265: Epoch 66/150, Test: HR = 0.5749, NDCG = 0.3512
2022-06-16 12:25:46.049477: Model Saved: yelp

2022-06-16 12:26:25.968799: Epoch 67/150, Train: Loss = 0.2415, preLoss = 0.0584

2022-06-16 12:27:04.737954: Epoch 68/150, Train: Loss = 0.2377, preLoss = 0.0576

2022-06-16 12:27:43.280019: Epoch 69/150, Train: Loss = 0.2346, preLoss = 0.0572
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1851 0.1851 0.30404076535192587 0.4184 0.3941801361528899 0.7306
2022-06-16 12:28:29.654911: Epoch 69/150, Test: HR = 0.5712, NDCG = 0.3540
2022-06-16 12:28:31.240883: Model Saved: yelp

2022-06-16 12:29:09.793018: Epoch 70/150, Train: Loss = 0.2310, preLoss = 0.0563

2022-06-16 12:29:48.337920: Epoch 71/150, Train: Loss = 0.2294, preLoss = 0.0572

2022-06-16 12:30:26.629931: Epoch 72/150, Train: Loss = 0.2260, preLoss = 0.0562
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1933 0.1933 0.3095909579668615 0.4236 0.40129716184414044 0.7398
2022-06-16 12:31:13.381828: Epoch 72/150, Test: HR = 0.5823, NDCG = 0.3616
2022-06-16 12:31:15.042484: Model Saved: yelp

2022-06-16 12:31:53.204070: Epoch 73/150, Train: Loss = 0.2237, preLoss = 0.0562

2022-06-16 12:32:31.461785: Epoch 74/150, Train: Loss = 0.2224, preLoss = 0.0570

2022-06-16 12:33:10.095806: Epoch 75/150, Train: Loss = 0.2190, preLoss = 0.0555
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1884 0.1884 0.30935866211255814 0.4235 0.40255789389145313 0.7451
2022-06-16 12:33:56.655663: Epoch 75/150, Test: HR = 0.5838, NDCG = 0.3618
2022-06-16 12:33:58.382487: Model Saved: yelp

2022-06-16 12:34:37.481420: Epoch 76/150, Train: Loss = 0.2171, preLoss = 0.0554

2022-06-16 12:35:16.761088: Epoch 77/150, Train: Loss = 0.2158, preLoss = 0.0560

2022-06-16 12:35:56.115682: Epoch 78/150, Train: Loss = 0.2130, preLoss = 0.0548
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1841 0.1841 0.30912196215015264 0.4305 0.4003434616066565 0.7475
2022-06-16 12:36:42.884340: Epoch 78/150, Test: HR = 0.5852, NDCG = 0.3595
2022-06-16 12:36:44.626404: Model Saved: yelp

2022-06-16 12:37:24.306632: Epoch 79/150, Train: Loss = 0.2124, preLoss = 0.0558

2022-06-16 12:38:04.304632: Epoch 80/150, Train: Loss = 0.2088, preLoss = 0.0539

2022-06-16 12:38:43.362636: Epoch 81/150, Train: Loss = 0.2071, preLoss = 0.0537
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1845 0.1845 0.3081396326729079 0.4255 0.402296005497025 0.7496
2022-06-16 12:39:30.512450: Epoch 81/150, Test: HR = 0.5919, NDCG = 0.3625
2022-06-16 12:39:32.305459: Model Saved: yelp

2022-06-16 12:40:11.869414: Epoch 82/150, Train: Loss = 0.2072, preLoss = 0.0552

2022-06-16 12:40:50.894818: Epoch 83/150, Train: Loss = 0.2049, preLoss = 0.0543

2022-06-16 12:41:30.366426: Epoch 84/150, Train: Loss = 0.2041, preLoss = 0.0548
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1906 0.1906 0.31279937843977734 0.429 0.40631492257071133 0.7498
2022-06-16 12:42:17.512115: Epoch 84/150, Test: HR = 0.5967, NDCG = 0.3677
2022-06-16 12:42:19.240309: Model Saved: yelp

2022-06-16 12:42:58.558768: Epoch 85/150, Train: Loss = 0.2019, preLoss = 0.0538

2022-06-16 12:43:37.545473: Epoch 86/150, Train: Loss = 0.2007, preLoss = 0.0537

2022-06-16 12:44:17.548127: Epoch 87/150, Train: Loss = 0.2006, preLoss = 0.0547
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1891 0.1891 0.3161046727293302 0.4364 0.40755435309694305 0.7508
2022-06-16 12:45:04.997677: Epoch 87/150, Test: HR = 0.5978, NDCG = 0.3689
2022-06-16 12:45:07.835059: Model Saved: yelp

2022-06-16 12:45:47.128675: Epoch 88/150, Train: Loss = 0.1977, preLoss = 0.0529

2022-06-16 12:46:26.391762: Epoch 89/150, Train: Loss = 0.1961, preLoss = 0.0524

2022-06-16 12:47:05.726053: Epoch 90/150, Train: Loss = 0.1961, preLoss = 0.0534
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1928 0.1928 0.31917383552252365 0.4394 0.4096794095862147 0.7512
2022-06-16 12:47:53.251043: Epoch 90/150, Test: HR = 0.5992, NDCG = 0.3714
2022-06-16 12:47:55.031637: Model Saved: yelp

2022-06-16 12:48:34.699175: Epoch 91/150, Train: Loss = 0.1956, preLoss = 0.0538

2022-06-16 12:49:14.059110: Epoch 92/150, Train: Loss = 0.1942, preLoss = 0.0533

2022-06-16 12:49:53.654023: Epoch 93/150, Train: Loss = 0.1924, preLoss = 0.0524
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1978 0.1978 0.3220176017670328 0.4407 0.4126359287262451 0.7532
2022-06-16 12:50:40.902716: Epoch 93/150, Test: HR = 0.6008, NDCG = 0.3743
2022-06-16 12:50:42.715256: Model Saved: yelp

2022-06-16 12:51:21.705263: Epoch 94/150, Train: Loss = 0.1918, preLoss = 0.0526

2022-06-16 12:52:00.769449: Epoch 95/150, Train: Loss = 0.1913, preLoss = 0.0529

2022-06-16 12:52:40.216715: Epoch 96/150, Train: Loss = 0.1898, preLoss = 0.0521
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1995 0.1995 0.32189460326589925 0.4387 0.41315150462744793 0.7529
2022-06-16 12:53:27.955038: Epoch 96/150, Test: HR = 0.5998, NDCG = 0.3746
2022-06-16 12:53:29.803187: Model Saved: yelp

2022-06-16 12:54:08.670524: Epoch 97/150, Train: Loss = 0.1893, preLoss = 0.0523

2022-06-16 12:54:47.981437: Epoch 98/150, Train: Loss = 0.1883, preLoss = 0.0520

2022-06-16 12:55:26.896937: Epoch 99/150, Train: Loss = 0.1865, preLoss = 0.0509
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2021 0.2021 0.3219680584323475 0.4366 0.413990985799596 0.7533
2022-06-16 12:56:14.262443: Epoch 99/150, Test: HR = 0.5997, NDCG = 0.3754
2022-06-16 12:56:16.073882: Model Saved: yelp

2022-06-16 12:56:55.228291: Epoch 100/150, Train: Loss = 0.1855, preLoss = 0.0507

2022-06-16 12:57:34.473475: Epoch 101/150, Train: Loss = 0.1855, preLoss = 0.0512

2022-06-16 12:58:13.787842: Epoch 102/150, Train: Loss = 0.1839, preLoss = 0.0503
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.201 0.201 0.3230888630997326 0.4402 0.4144185058002261 0.7546
2022-06-16 12:59:00.927114: Epoch 102/150, Test: HR = 0.6026, NDCG = 0.3762
2022-06-16 12:59:02.758923: Model Saved: yelp

2022-06-16 12:59:42.016466: Epoch 103/150, Train: Loss = 0.1842, preLoss = 0.0512

2022-06-16 13:00:21.776674: Epoch 104/150, Train: Loss = 0.1837, preLoss = 0.0513

2022-06-16 13:01:00.950008: Epoch 105/150, Train: Loss = 0.1837, preLoss = 0.0518
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1984 0.1984 0.3224382379483361 0.4422 0.4132792575877531 0.7551
2022-06-16 13:01:48.014556: Epoch 105/150, Test: HR = 0.6018, NDCG = 0.3747
2022-06-16 13:01:49.877167: Model Saved: yelp

2022-06-16 13:02:28.832301: Epoch 106/150, Train: Loss = 0.1813, preLoss = 0.0499

2022-06-16 13:03:08.324501: Epoch 107/150, Train: Loss = 0.1820, preLoss = 0.0511

2022-06-16 13:03:47.944235: Epoch 108/150, Train: Loss = 0.1815, preLoss = 0.0511
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1988 0.1988 0.322818346454926 0.4423 0.41374904492482895 0.7551
2022-06-16 13:04:35.334201: Epoch 108/150, Test: HR = 0.6026, NDCG = 0.3754
2022-06-16 13:04:37.058574: Model Saved: yelp

2022-06-16 13:05:16.497171: Epoch 109/150, Train: Loss = 0.1815, preLoss = 0.0515

2022-06-16 13:05:55.823924: Epoch 110/150, Train: Loss = 0.1804, preLoss = 0.0508

2022-06-16 13:06:34.829491: Epoch 111/150, Train: Loss = 0.1800, preLoss = 0.0509
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1984 0.1984 0.32543628361725374 0.4477 0.4147697867488383 0.7562
2022-06-16 13:07:21.605097: Epoch 111/150, Test: HR = 0.6032, NDCG = 0.3763
2022-06-16 13:07:23.467225: Model Saved: yelp

2022-06-16 13:08:02.862787: Epoch 112/150, Train: Loss = 0.1790, preLoss = 0.0503

2022-06-16 13:08:42.168705: Epoch 113/150, Train: Loss = 0.1782, preLoss = 0.0499

2022-06-16 13:09:21.294739: Epoch 114/150, Train: Loss = 0.1785, preLoss = 0.0506
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1999 0.1999 0.3250118288093881 0.4464 0.41471960032844957 0.756
2022-06-16 13:10:08.260567: Epoch 114/150, Test: HR = 0.6018, NDCG = 0.3759
2022-06-16 13:10:10.117600: Model Saved: yelp

2022-06-16 13:10:49.517122: Epoch 115/150, Train: Loss = 0.1780, preLoss = 0.0504

2022-06-16 13:11:28.540508: Epoch 116/150, Train: Loss = 0.1765, preLoss = 0.0493

2022-06-16 13:12:07.970302: Epoch 117/150, Train: Loss = 0.1777, preLoss = 0.0508
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1995 0.1995 0.3257570543794955 0.4481 0.41533738501324996 0.7572
2022-06-16 13:12:54.982961: Epoch 117/150, Test: HR = 0.6029, NDCG = 0.3765
2022-06-16 13:12:57.534892: Model Saved: yelp

2022-06-16 13:13:36.858294: Epoch 118/150, Train: Loss = 0.1760, preLoss = 0.0494

2022-06-16 13:14:16.002933: Epoch 119/150, Train: Loss = 0.1753, preLoss = 0.0490

2022-06-16 13:14:55.384445: Epoch 120/150, Train: Loss = 0.1770, preLoss = 0.0511
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1976 0.1976 0.32463153713443327 0.4474 0.4149267310923501 0.759
2022-06-16 13:15:42.696322: Epoch 120/150, Test: HR = 0.6036, NDCG = 0.3758
2022-06-16 13:15:45.187283: Model Saved: yelp

2022-06-16 13:16:24.344077: Epoch 121/150, Train: Loss = 0.1756, preLoss = 0.0499

2022-06-16 13:17:03.202082: Epoch 122/150, Train: Loss = 0.1758, preLoss = 0.0504

2022-06-16 13:17:42.396954: Epoch 123/150, Train: Loss = 0.1750, preLoss = 0.0499
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1994 0.1994 0.3248309617870453 0.4459 0.4159253700459226 0.7604
2022-06-16 13:18:28.975528: Epoch 123/150, Test: HR = 0.6025, NDCG = 0.3761
2022-06-16 13:18:30.890144: Model Saved: yelp

2022-06-16 13:19:10.385079: Epoch 124/150, Train: Loss = 0.1747, preLoss = 0.0499

2022-06-16 13:19:49.358709: Epoch 125/150, Train: Loss = 0.1749, preLoss = 0.0502

2022-06-16 13:20:28.472122: Epoch 126/150, Train: Loss = 0.1738, preLoss = 0.0494
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.201 0.201 0.32561752038946185 0.4463 0.41679855824006246 0.761
2022-06-16 13:21:15.344409: Epoch 126/150, Test: HR = 0.6031, NDCG = 0.3771
2022-06-16 13:21:17.244620: Model Saved: yelp

2022-06-16 13:21:57.015798: Epoch 127/150, Train: Loss = 0.1747, preLoss = 0.0505

2022-06-16 13:22:36.131223: Epoch 128/150, Train: Loss = 0.1742, preLoss = 0.0502

2022-06-16 13:23:15.420414: Epoch 129/150, Train: Loss = 0.1733, preLoss = 0.0496
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.201 0.201 0.32415986077632297 0.4432 0.41619396651133755 0.7601
2022-06-16 13:24:02.449988: Epoch 129/150, Test: HR = 0.6031, NDCG = 0.3767
2022-06-16 13:24:04.340078: Model Saved: yelp

2022-06-16 13:24:43.704293: Epoch 130/150, Train: Loss = 0.1725, preLoss = 0.0490

2022-06-16 13:25:22.888289: Epoch 131/150, Train: Loss = 0.1732, preLoss = 0.0499

2022-06-16 13:26:01.809340: Epoch 132/150, Train: Loss = 0.1726, preLoss = 0.0495
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2005 0.2005 0.32331751343112647 0.4415 0.41581257770091906 0.7595
2022-06-16 13:26:49.106293: Epoch 132/150, Test: HR = 0.6034, NDCG = 0.3766
2022-06-16 13:26:51.027312: Model Saved: yelp

2022-06-16 13:27:30.273850: Epoch 133/150, Train: Loss = 0.1731, preLoss = 0.0502

2022-06-16 13:28:10.102430: Epoch 134/150, Train: Loss = 0.1714, preLoss = 0.0487

2022-06-16 13:28:50.136468: Epoch 135/150, Train: Loss = 0.1716, preLoss = 0.0491
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1981 0.1981 0.3225896059866757 0.4424 0.4148798111627795 0.7599
2022-06-16 13:29:37.091988: Epoch 135/150, Test: HR = 0.6038, NDCG = 0.3756
2022-06-16 13:29:38.903204: Model Saved: yelp

2022-06-16 13:30:18.345092: Epoch 136/150, Train: Loss = 0.1713, preLoss = 0.0490

2022-06-16 13:30:57.914939: Epoch 137/150, Train: Loss = 0.1718, preLoss = 0.0496

^CTraceback (most recent call last):0: preloss = 0.05, REGLoss = 0.12
  File "main.py", line 25, in <module>
    recom.run()
  File "/root/CLSR/model.py", line 50, in run
    reses = self.trainEpoch()
  File "/root/CLSR/model.py", line 390, in trainEpoch
    uLocs, iLocs, timeLocs = self.sampleTrainBatch(batIds, self.handler.trnMat, self.handler.timeMat, sample_num_list[s])
  File "/root/CLSR/model.py", line 316, in sampleTrainBatch
    timeLocs[cur+temlen//2] = timeMat[batIds[i],negloc]
  File "/root/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py", line 33, in __getitem__
    row, col = self._validate_indices(key)
  File "/root/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py", line 140, in _validate_indices
    if isintlike(col):
  File "/root/miniconda3/lib/python3.7/site-packages/scipy/sparse/sputils.py", line 204, in isintlike
    if np.ndim(x) != 0:
  File "<__array_function__ internals>", line 6, in ndim
KeyboardInterrupt

root@container-327e11a8ac-4a1523bb:~/CLSR#
root@container-327e11a8ac-4a1523bb:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data gowalla --reg 1e-2          --temp  0.1 --ssl_reg 1e-6          --save_path gowalla --batch 512 --epoch 150
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2022-06-16 13:31:38.047132: Start
tstInt [None None None ... None None None]
tstStat [False False False ... False False False] 50821
tstUsrs [    7     8    21 ... 50804 50805 50812] 10000
trnMat   (0, 1) 28.0
  (0, 2)        2.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        14.0
  (0, 7)        2.0
  (0, 8)        2.0
  (0, 9)        1.0
  (0, 10)       1.0
  (0, 11)       1.0
  (0, 12)       1.0
  (0, 13)       1.0
  (0, 14)       1.0
  (0, 15)       1.0
  (0, 16)       2.0
  (0, 17)       1.0
  (0, 18)       1.0
  (0, 19)       1.0
  (0, 20)       1.0
  (0, 21)       1.0
  (0, 22)       1.0
  (0, 23)       1.0
  (0, 24)       1.0
  (0, 25)       1.0
  :     :
  (50818, 23821)        1.0
  (50818, 29714)        1.0
  (50818, 35622)        1.0
  (50818, 43630)        1.0
  (50818, 43633)        1.0
  (50818, 46220)        1.0
  (50818, 46221)        1.0
  (50818, 46222)        2.0
  (50818, 46223)        1.0
  (50818, 46224)        1.0
  (50818, 46225)        1.0
  (50818, 46226)        1.0
  (50818, 46227)        1.0
  (50818, 46553)        1.0
  (50818, 50352)        1.0
  (50818, 54931)        1.0
  (50818, 55993)        1.0
  (50819, 22327)        1.0
  (50819, 29134)        1.0
  (50819, 52671)        2.0
  (50819, 54088)        1.0
  (50820, 21819)        1.0
  (50820, 21941)        16.0
  (50820, 21947)        1.0
  (50820, 30477)        1.0   (9, 50874)        1.0
  (9, 51528)    1.0
  (48, 50873)   1.0
  (48, 51030)   1.0
  (48, 51303)   1.0
  (48, 53520)   1.0
  (48, 54006)   1.0
  (48, 54007)   1.0
  (48, 54008)   1.0
  (48, 54009)   1.0
  (48, 54010)   1.0
  (48, 54011)   1.0
  (48, 54012)   1.0
  (48, 54013)   2.0
  (73, 50873)   1.0
  (73, 50881)   2.0
  (73, 55017)   1.0
  (78, 55162)   3.0
  (78, 55186)   1.0
  (110, 50933)  1.0
  (110, 51029)  1.0
  (110, 51030)  1.0
  (110, 51286)  1.0
  (110, 52475)  1.0
  (110, 52476)  1.0
  :     :
  (88947, 40374)        1.0
  (89107, 3156) 1.0
  (89324, 2639) 2.0
  (89342, 4886) 2.0
  (89446, 1877) 1.0
  (89560, 3378) 1.0
  (89750, 4886) 2.0
  (90073, 3321) 1.0
  (90073, 4886) 1.0
  (90105, 38240)        1.0
  (90791, 15519)        2.0
  (91224, 15908)        1.0
  (91323, 2954) 1.0
  (91404, 3156) 1.0
  (91404, 3184) 1.0
  (91693, 2954) 1.0
  (92541, 3156) 3.0
  (92541, 3195) 1.0
  (93146, 3365) 1.0
  (94518, 40374)        1.0
  (94736, 5592) 2.0
  (94806, 3422) 1.0
  (94983, 38240)        1.0
  (95806, 3422) 1.0
  (95806, 15987)        1.0   (18, 52475)       1.0
  (18, 52476)   1.0
  (78, 50929)   1.0
  (78, 55149)   1.0
  (78, 55171)   1.0
  (78, 55185)   1.0
  (90, 54260)   1.0
  (90, 54262)   1.0
  (90, 55952)   2.0
  (110, 51028)  1.0
  (110, 51031)  1.0
  (110, 51035)  1.0
  (110, 51303)  1.0
  (110, 51312)  1.0
  (110, 52490)  1.0
  (110, 53023)  1.0
  (110, 54006)  1.0
  (110, 54011)  1.0
  (110, 54012)  1.0
  (110, 55844)  1.0
  (110, 55941)  2.0
  (110, 57052)  1.0
  (110, 57700)  1.0
  (110, 57704)  1.0
  (110, 57807)  1.0
  :     :
  (88749, 3360) 1.0
  (88749, 12984)        1.0
  (89217, 3378) 1.0
  (89654, 17810)        1.0
  (89993, 3360) 2.0
  (89994, 3360) 1.0
  (90091, 2954) 1.0
  (90252, 3214) 5.0
  (90285, 3360) 1.0
  (90542, 4560) 1.0
  (90665, 3360) 1.0
  (91037, 37874)        1.0
  (91063, 10551)        1.0
  (91171, 12984)        1.0
  (91273, 3214) 2.0
  (91273, 3557) 1.0
  (91693, 2954) 1.0
  (92541, 3156) 1.0
  (93146, 3365) 1.0
  (93967, 3565) 1.0
  (94572, 3214) 2.0
  (95239, 3319) 1.0
  (95423, 3360) 1.0
  (95434, 3360) 2.0
  (96764, 4417) 1.0   (0, 1)    8
  (0, 2)        9
  (0, 3)        10
  (0, 4)        10
  (0, 5)        10
  (0, 6)        8
  (0, 7)        8
  (0, 8)        10
  (0, 9)        10
  (0, 10)       10
  (0, 11)       10
  (0, 12)       10
  (0, 13)       10
  (0, 14)       10
  (0, 15)       10
  (0, 16)       10
  (0, 17)       10
  (0, 18)       10
  (0, 19)       10
  (0, 20)       10
  (0, 21)       10
  (0, 22)       10
  (0, 23)       10
  (0, 24)       10
  (0, 25)       10
  :     :
  (50818, 55993)        10
  (50818, 35622)        10
  (50818, 23821)        10
  (50818, 23820)        10
  (50818, 46553)        10
  (50818, 43630)        10
  (50818, 43633)        10
  (50818, 54931)        10
  (50818, 46226)        10
  (50818, 50352)        10
  (50818, 46227)        10
  (50818, 46221)        10
  (50818, 46223)        10
  (50818, 46222)        10
  (50818, 46220)        10
  (50818, 46224)        10
  (50818, 46225)        10
  (50819, 54088)        10
  (50819, 52671)        10
  (50819, 22327)        10
  (50819, 29134)        10
  (50820, 21941)        10
  (50820, 21947)        10
  (50820, 21819)        10
  (50820, 30477)        10
[29224 57406 57337 ...    61    62  8944]
2022-06-16 13:31:38.330240: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 50821 ITEM 57440
WARNING:tensorflow:From /root/CLSR/model.py:241: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(1816, 2), dtype=int64), values=Tensor("SparseTensor/values:0", shape=(1816,),          dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be re         moved in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(1816, 2), dtype=int64), values=Tensor("SparseTensor/values:0", shape=(1816,),          dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(1384, 2), dtype=int64), values=Tensor("SparseTensor_1/values:0", shape=(138         4,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(1384, 2), dtype=int64), values=Tensor("SparseTensor_1/values:0", shape=(138         4,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(2226, 2), dtype=int64), values=Tensor("SparseTensor_2/values:0", shape=(222         6,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(2226, 2), dtype=int64), values=Tensor("SparseTensor_2/values:0", shape=(222         6,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(23372, 2), dtype=int64), values=Tensor("SparseTensor_3/values:0", shape=(23         372,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(23372, 2), dtype=int64), values=Tensor("SparseTensor_3/values:0", shape=(23         372,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(103982, 2), dtype=int64), values=Tensor("SparseTensor_4/values:0", shape=(1         03982,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(103982, 2), dtype=int64), values=Tensor("SparseTensor_4/values:0", shape=(1         03982,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(242438, 2), dtype=int64), values=Tensor("SparseTensor_5/values:0", shape=(2         42438,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(242438, 2), dtype=int64), values=Tensor("SparseTensor_5/values:0", shape=(2         42438,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(390062, 2), dtype=int64), values=Tensor("SparseTensor_6/values:0", shape=(3         90062,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(390062, 2), dtype=int64), values=Tensor("SparseTensor_6/values:0", shape=(3         90062,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(420046, 2), dtype=int64), values=Tensor("SparseTensor_7/values:0", shape=(4         20046,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(420046, 2), dtype=int64), values=Tensor("SparseTensor_7/values:0", shape=(4         20046,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a fut         ure version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (         from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f9c7e96cbd0>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f9         c7e96cbd0>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(50821, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f9c7e911c10>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f9         c7e911c10>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(57440, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_d         ecay instead.

WARNING:tensorflow:From /root/CLSR/model.py:287: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer ins         tead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wra         pper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-16 13:32:00.075345: Model Prepared
2022-06-16 13:32:03.336401: Variables Inited
2022-06-16 13:32:59.155553: Epoch 0/150, Train: Loss = 3.7943, preLoss = 1.3284
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0167 0.0167 0.046439696206069775 0.0769 0.09322493162052048 0.2475
2022-06-16 13:33:56.282173: Epoch 0/150, Test: HR = 0.1362, NDCG = 0.0654
WARNING:tensorflow:From /root/CLSR/model.py:524: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

2022-06-16 13:33:58.290711: Model Saved: gowalla

2022-06-16 13:34:44.061576: Epoch 1/150, Train: Loss = 6.4591, preLoss = 2.1348

2022-06-16 13:35:29.728757: Epoch 2/150, Train: Loss = 8.5676, preLoss = 2.6180

2022-06-16 13:36:15.478006: Epoch 3/150, Train: Loss = 10.7038, preLoss = 2.6786
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0329 0.0329 0.08102389574513781 0.1295 0.1445815651745714 0.3593
2022-06-16 13:37:13.683504: Epoch 3/150, Test: HR = 0.2157, NDCG = 0.1087
2022-06-16 13:37:15.378600: Model Saved: gowalla

2022-06-16 13:38:00.902945: Epoch 4/150, Train: Loss = 12.3486, preLoss = 2.6831

2022-06-16 13:38:46.350775: Epoch 5/150, Train: Loss = 12.9877, preLoss = 2.4155

2022-06-16 13:39:32.052959: Epoch 6/150, Train: Loss = 12.8172, preLoss = 2.0532
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0544 0.0544 0.12200439581231347 0.1903 0.2010344378423366 0.4738
2022-06-16 13:40:29.848812: Epoch 6/150, Test: HR = 0.3037, NDCG = 0.1583
2022-06-16 13:40:31.558302: Model Saved: gowalla

2022-06-16 13:41:16.682231: Epoch 7/150, Train: Loss = 12.1976, preLoss = 1.7447

2022-06-16 13:42:02.133701: Epoch 8/150, Train: Loss = 11.2429, preLoss = 1.4346

2022-06-16 13:42:47.251628: Epoch 9/150, Train: Loss = 10.1772, preLoss = 1.1957
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0782 0.0782 0.15701098983569586 0.237 0.244173486946712 0.5478
2022-06-16 13:43:45.734597: Epoch 9/150, Test: HR = 0.3640, NDCG = 0.1979
2022-06-16 13:43:47.349626: Model Saved: gowalla

2022-06-16 13:44:32.631478: Epoch 10/150, Train: Loss = 9.1146, preLoss = 1.0081

2022-06-16 13:45:17.574961: Epoch 11/150, Train: Loss = 8.0177, preLoss = 0.8105

2022-06-16 13:46:02.962440: Epoch 12/150, Train: Loss = 6.9931, preLoss = 0.6659
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0807 0.0807 0.16534731125621446 0.252 0.2661637352172049 0.6091
2022-06-16 13:47:00.378631: Epoch 12/150, Test: HR = 0.4084, NDCG = 0.2156
2022-06-16 13:47:01.957236: Model Saved: gowalla

2022-06-16 13:47:47.221132: Epoch 13/150, Train: Loss = 6.0670, preLoss = 0.5600

2022-06-16 13:48:32.787603: Epoch 14/150, Train: Loss = 5.2381, preLoss = 0.4591

2022-06-16 13:49:18.205371: Epoch 15/150, Train: Loss = 4.5463, preLoss = 0.4075
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0842 0.0842 0.18521238961903314 0.2857 0.2879409936753796 0.6482
2022-06-16 13:50:14.623090: Epoch 15/150, Test: HR = 0.4518, NDCG = 0.2386
2022-06-16 13:50:16.240482: Model Saved: gowalla

2022-06-16 13:51:01.509211: Epoch 16/150, Train: Loss = 3.9420, preLoss = 0.3447

2022-06-16 13:51:46.404134: Epoch 17/150, Train: Loss = 3.4116, preLoss = 0.2911

2022-06-16 13:52:31.641075: Epoch 18/150, Train: Loss = 2.9710, preLoss = 0.2575
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0944 0.0944 0.20139915031803016 0.3104 0.31091917605476976 0.695
2022-06-16 13:53:28.231402: Epoch 18/150, Test: HR = 0.4890, NDCG = 0.2589
2022-06-16 13:53:29.909637: Model Saved: gowalla

2022-06-16 13:54:15.679184: Epoch 19/150, Train: Loss = 2.5862, preLoss = 0.2220

2022-06-16 13:55:00.738432: Epoch 20/150, Train: Loss = 2.2725, preLoss = 0.2046

2022-06-16 13:55:45.809985: Epoch 21/150, Train: Loss = 2.0051, preLoss = 0.1822
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.108 0.108 0.22308107998307436 0.3377 0.3341857555974594 0.7308
2022-06-16 13:56:42.301351: Epoch 21/150, Test: HR = 0.5078, NDCG = 0.2780
2022-06-16 13:56:43.929206: Model Saved: gowalla

2022-06-16 13:57:29.350297: Epoch 22/150, Train: Loss = 1.7809, preLoss = 0.1664

2022-06-16 13:58:15.544676: Epoch 23/150, Train: Loss = 1.5959, preLoss = 0.1550

2022-06-16 13:59:00.803031: Epoch 24/150, Train: Loss = 1.4369, preLoss = 0.1444
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1156 0.1156 0.24130180876784288 0.3674 0.3515114214892307 0.7541
2022-06-16 13:59:56.697091: Epoch 24/150, Test: HR = 0.5474, NDCG = 0.2993
2022-06-16 13:59:58.479989: Model Saved: gowalla

2022-06-16 14:00:43.866277: Epoch 25/150, Train: Loss = 1.3054, preLoss = 0.1401

2022-06-16 14:01:29.205999: Epoch 26/150, Train: Loss = 1.1934, preLoss = 0.1343

2022-06-16 14:02:14.889424: Epoch 27/150, Train: Loss = 1.0972, preLoss = 0.1267
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1346 0.1346 0.27097742780666256 0.4066 0.3811601941952228 0.7886
2022-06-16 14:03:10.725324: Epoch 27/150, Test: HR = 0.5985, NDCG = 0.3331
2022-06-16 14:03:12.396374: Model Saved: gowalla

2022-06-16 14:03:57.578080: Epoch 28/150, Train: Loss = 1.0156, preLoss = 0.1223

2022-06-16 14:04:43.134036: Epoch 29/150, Train: Loss = 0.9523, preLoss = 0.1241

2022-06-16 14:05:28.667866: Epoch 30/150, Train: Loss = 0.8888, preLoss = 0.1183
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1518 0.1518 0.28975394214820654 0.4258 0.4030498480965695 0.8219
2022-06-16 14:06:25.108859: Epoch 30/150, Test: HR = 0.6138, NDCG = 0.3503
2022-06-16 14:06:26.796193: Model Saved: gowalla

2022-06-16 14:07:11.736020: Epoch 31/150, Train: Loss = 0.8350, preLoss = 0.1152

2022-06-16 14:07:56.503833: Epoch 32/150, Train: Loss = 0.7888, preLoss = 0.1149

2022-06-16 14:08:41.806458: Epoch 33/150, Train: Loss = 0.7462, preLoss = 0.1118
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1614 0.1614 0.3099856451365177 0.4519 0.41908474161612064 0.8349
2022-06-16 14:09:37.784201: Epoch 33/150, Test: HR = 0.6321, NDCG = 0.3676
2022-06-16 14:09:39.430991: Model Saved: gowalla

2022-06-16 14:10:25.129651: Epoch 34/150, Train: Loss = 0.7075, preLoss = 0.1075

2022-06-16 14:11:10.664451: Epoch 35/150, Train: Loss = 0.6756, preLoss = 0.1080

2022-06-16 14:11:55.508729: Epoch 36/150, Train: Loss = 0.6470, preLoss = 0.1068
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1735 0.1735 0.3251918916582774 0.4712 0.4330355610939182 0.8467
2022-06-16 14:12:51.745822: Epoch 36/150, Test: HR = 0.6530, NDCG = 0.3836
2022-06-16 14:12:53.532376: Model Saved: gowalla

2022-06-16 14:13:38.827554: Epoch 37/150, Train: Loss = 0.6216, preLoss = 0.1058

2022-06-16 14:14:24.271937: Epoch 38/150, Train: Loss = 0.5970, preLoss = 0.1041

2022-06-16 14:15:09.858433: Epoch 39/150, Train: Loss = 0.5731, preLoss = 0.1018
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1895 0.1895 0.34326678403577493 0.4882 0.4494650040229755 0.857
2022-06-16 14:16:05.988721: Epoch 39/150, Test: HR = 0.6720, NDCG = 0.4024
2022-06-16 14:16:07.694608: Model Saved: gowalla

2022-06-16 14:16:54.139121: Epoch 40/150, Train: Loss = 0.5528, preLoss = 0.1011

2022-06-16 14:17:39.747781: Epoch 41/150, Train: Loss = 0.5338, preLoss = 0.1000

2022-06-16 14:18:25.030732: Epoch 42/150, Train: Loss = 0.5192, preLoss = 0.1017
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2053 0.2053 0.35484138058382625 0.4997 0.46021907360006115 0.867
2022-06-16 14:19:20.876872: Epoch 42/150, Test: HR = 0.6813, NDCG = 0.4133
2022-06-16 14:19:22.584116: Model Saved: gowalla

2022-06-16 14:20:07.873940: Epoch 43/150, Train: Loss = 0.5011, preLoss = 0.0984

2022-06-16 14:20:53.021072: Epoch 44/150, Train: Loss = 0.4871, preLoss = 0.0982

2022-06-16 14:21:38.140564: Epoch 45/150, Train: Loss = 0.4703, preLoss = 0.0937
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2107 0.2107 0.36899502363227726 0.5226 0.4703596622561803 0.8739
2022-06-16 14:22:34.605636: Epoch 45/150, Test: HR = 0.6983, NDCG = 0.4257
2022-06-16 14:22:36.453649: Model Saved: gowalla

2022-06-16 14:23:21.590862: Epoch 46/150, Train: Loss = 0.4584, preLoss = 0.0940

2022-06-16 14:24:06.548833: Epoch 47/150, Train: Loss = 0.4463, preLoss = 0.0930

2022-06-16 14:24:52.457498: Epoch 48/150, Train: Loss = 0.4332, preLoss = 0.0902
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2091 0.2091 0.3683715517125408 0.5209 0.47468123986214095 0.8903
2022-06-16 14:25:49.086640: Epoch 48/150, Test: HR = 0.6967, NDCG = 0.4253
2022-06-16 14:25:50.914506: Model Saved: gowalla

2022-06-16 14:26:36.320672: Epoch 49/150, Train: Loss = 0.4251, preLoss = 0.0917

2022-06-16 14:27:21.468586: Epoch 50/150, Train: Loss = 0.4140, preLoss = 0.0900

2022-06-16 14:28:06.884207: Epoch 51/150, Train: Loss = 0.4033, preLoss = 0.0879
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2024 0.2024 0.36922383613278226 0.5249 0.4763378892601707 0.8969
2022-06-16 14:29:02.648071: Epoch 51/150, Test: HR = 0.7027, NDCG = 0.4268
2022-06-16 14:29:04.369137: Model Saved: gowalla

2022-06-16 14:29:49.364642: Epoch 52/150, Train: Loss = 0.3938, preLoss = 0.0863

2022-06-16 14:30:34.511251: Epoch 53/150, Train: Loss = 0.3885, preLoss = 0.0883

2022-06-16 14:31:19.555016: Epoch 54/150, Train: Loss = 0.3800, preLoss = 0.0866
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2228 0.2228 0.378839347128756 0.5305 0.4813181740627521 0.8864
2022-06-16 14:32:15.238905: Epoch 54/150, Test: HR = 0.7027, NDCG = 0.4346
2022-06-16 14:32:17.039165: Model Saved: gowalla

2022-06-16 14:33:01.995429: Epoch 55/150, Train: Loss = 0.3711, preLoss = 0.0840

2022-06-16 14:33:47.314243: Epoch 56/150, Train: Loss = 0.3663, preLoss = 0.0855

2022-06-16 14:34:32.439733: Epoch 57/150, Train: Loss = 0.3588, preLoss = 0.0837
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2304 0.2304 0.390544922898448 0.5449 0.49216227643781174 0.8955
2022-06-16 14:35:27.972941: Epoch 57/150, Test: HR = 0.7207, NDCG = 0.4478
2022-06-16 14:35:29.766309: Model Saved: gowalla

2022-06-16 14:36:14.362132: Epoch 58/150, Train: Loss = 0.3510, preLoss = 0.0816

2022-06-16 14:36:59.290984: Epoch 59/150, Train: Loss = 0.3456, preLoss = 0.0812

2022-06-16 14:37:44.522938: Epoch 60/150, Train: Loss = 0.3391, preLoss = 0.0798
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2284 0.2284 0.392749753630531 0.5478 0.496456503185156 0.9049
2022-06-16 14:38:40.443661: Epoch 60/150, Test: HR = 0.7300, NDCG = 0.4520
2022-06-16 14:38:42.239953: Model Saved: gowalla

2022-06-16 14:39:26.718827: Epoch 61/150, Train: Loss = 0.3354, preLoss = 0.0807

2022-06-16 14:40:11.827695: Epoch 62/150, Train: Loss = 0.3289, preLoss = 0.0786

2022-06-16 14:40:57.030264: Epoch 63/150, Train: Loss = 0.3244, preLoss = 0.0784
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2401 0.2401 0.4086495329885666 0.5701 0.505956686555884 0.9042
2022-06-16 14:41:52.184362: Epoch 63/150, Test: HR = 0.7476, NDCG = 0.4661
2022-06-16 14:41:53.945496: Model Saved: gowalla

2022-06-16 14:42:39.189632: Epoch 64/150, Train: Loss = 0.3208, preLoss = 0.0788

2022-06-16 14:43:24.082016: Epoch 65/150, Train: Loss = 0.3156, preLoss = 0.0773

2022-06-16 14:44:08.992686: Epoch 66/150, Train: Loss = 0.3113, preLoss = 0.0768
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2492 0.2492 0.41657696607870076 0.5766 0.5146415645174084 0.9116
2022-06-16 14:45:03.940536: Epoch 66/150, Test: HR = 0.7584, NDCG = 0.4756
2022-06-16 14:45:06.562470: Model Saved: gowalla

2022-06-16 14:45:51.487374: Epoch 67/150, Train: Loss = 0.3073, preLoss = 0.0760

2022-06-16 14:46:36.189311: Epoch 68/150, Train: Loss = 0.3032, preLoss = 0.0752

2022-06-16 14:47:20.908538: Epoch 69/150, Train: Loss = 0.3002, preLoss = 0.0752
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2528 0.2528 0.4217466823059411 0.5791 0.5207795160844225 0.9168
2022-06-16 14:48:17.272918: Epoch 69/150, Test: HR = 0.7648, NDCG = 0.4821
2022-06-16 14:48:19.063553: Model Saved: gowalla

2022-06-16 14:49:04.177214: Epoch 70/150, Train: Loss = 0.2949, preLoss = 0.0728

2022-06-16 14:49:49.357680: Epoch 71/150, Train: Loss = 0.2925, preLoss = 0.0733

2022-06-16 14:50:34.463717: Epoch 72/150, Train: Loss = 0.2894, preLoss = 0.0732
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2597 0.2597 0.4285432612979891 0.5887 0.5239319791470751 0.9169
2022-06-16 14:51:30.212921: Epoch 72/150, Test: HR = 0.7584, NDCG = 0.4835
2022-06-16 14:51:32.114475: Model Saved: gowalla

2022-06-16 14:52:17.213350: Epoch 73/150, Train: Loss = 0.2868, preLoss = 0.0732

2022-06-16 14:53:02.015664: Epoch 74/150, Train: Loss = 0.2832, preLoss = 0.0722

2022-06-16 14:53:47.297241: Epoch 75/150, Train: Loss = 0.2811, preLoss = 0.0724
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2614 0.2614 0.43368487213154366 0.5971 0.5270935018588846 0.9186
2022-06-16 14:54:42.990198: Epoch 75/150, Test: HR = 0.7617, NDCG = 0.4870
2022-06-16 14:54:44.861052: Model Saved: gowalla

2022-06-16 14:55:30.009497: Epoch 76/150, Train: Loss = 0.2770, preLoss = 0.0705

2022-06-16 14:56:14.860379: Epoch 77/150, Train: Loss = 0.2757, preLoss = 0.0713

2022-06-16 14:57:00.245705: Epoch 78/150, Train: Loss = 0.2726, preLoss = 0.0703
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2641 0.2641 0.4350732389605687 0.5985 0.5278563882462316 0.9185
2022-06-16 14:57:55.679151: Epoch 78/150, Test: HR = 0.7535, NDCG = 0.4855
2022-06-16 14:57:57.516250: Model Saved: gowalla

2022-06-16 14:58:42.388860: Epoch 79/150, Train: Loss = 0.2696, preLoss = 0.0694

2022-06-16 14:59:27.513015: Epoch 80/150, Train: Loss = 0.2685, preLoss = 0.0701

2022-06-16 15:00:12.914120: Epoch 81/150, Train: Loss = 0.2648, preLoss = 0.0682
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2667 0.2667 0.42885717430241205 0.5827 0.5260976253761646 0.9169
2022-06-16 15:01:08.674741: Epoch 81/150, Test: HR = 0.7514, NDCG = 0.4837
2022-06-16 15:01:10.663657: Model Saved: gowalla

2022-06-16 15:01:56.338382: Epoch 82/150, Train: Loss = 0.2645, preLoss = 0.0696

2022-06-16 15:02:41.577084: Epoch 83/150, Train: Loss = 0.2629, preLoss = 0.0696

2022-06-16 15:03:26.286824: Epoch 84/150, Train: Loss = 0.2607, preLoss = 0.0690
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.257 0.257 0.4258837336186687 0.5846 0.5246564904502267 0.9249
2022-06-16 15:04:21.794048: Epoch 84/150, Test: HR = 0.7536, NDCG = 0.4808
2022-06-16 15:04:23.616177: Model Saved: gowalla

2022-06-16 15:05:08.515833: Epoch 85/150, Train: Loss = 0.2585, preLoss = 0.0683

2022-06-16 15:05:53.751687: Epoch 86/150, Train: Loss = 0.2562, preLoss = 0.0674

2022-06-16 15:06:38.585346: Epoch 87/150, Train: Loss = 0.2559, preLoss = 0.0685
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.263 0.263 0.4277310815320101 0.5814 0.5272931760986391 0.9235
2022-06-16 15:07:33.954114: Epoch 87/150, Test: HR = 0.7595, NDCG = 0.4854
2022-06-16 15:07:35.858286: Model Saved: gowalla

2022-06-16 15:08:20.900612: Epoch 88/150, Train: Loss = 0.2550, preLoss = 0.0689

2022-06-16 15:09:06.247518: Epoch 89/150, Train: Loss = 0.2525, preLoss = 0.0677

2022-06-16 15:09:50.965936: Epoch 90/150, Train: Loss = 0.2501, preLoss = 0.0665
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.263 0.263 0.4271712706928513 0.5818 0.5266066720840241 0.9237
2022-06-16 15:10:46.637669: Epoch 90/150, Test: HR = 0.7561, NDCG = 0.4839
2022-06-16 15:10:48.360266: Model Saved: gowalla

2022-06-16 15:11:32.892282: Epoch 91/150, Train: Loss = 0.2502, preLoss = 0.0678

2022-06-16 15:12:18.071283: Epoch 92/150, Train: Loss = 0.2486, preLoss = 0.0672

2022-06-16 15:13:03.158608: Epoch 93/150, Train: Loss = 0.2449, preLoss = 0.0645
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2637 0.2637 0.42986546660346375 0.5871 0.5280812237338144 0.9245
2022-06-16 15:13:58.525432: Epoch 93/150, Test: HR = 0.7616, NDCG = 0.4866
2022-06-16 15:14:00.252172: Model Saved: gowalla

2022-06-16 15:14:45.827318: Epoch 94/150, Train: Loss = 0.2440, preLoss = 0.0647

2022-06-16 15:15:31.172836: Epoch 95/150, Train: Loss = 0.2444, preLoss = 0.0661

2022-06-16 15:16:16.368524: Epoch 96/150, Train: Loss = 0.2425, preLoss = 0.0652
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2666 0.2666 0.43311561357017825 0.5902 0.5306619342735109 0.9259
2022-06-16 15:17:12.914297: Epoch 96/150, Test: HR = 0.7572, NDCG = 0.4876
2022-06-16 15:17:14.810882: Model Saved: gowalla

2022-06-16 15:18:00.709386: Epoch 97/150, Train: Loss = 0.2417, preLoss = 0.0652

2022-06-16 15:18:45.991706: Epoch 98/150, Train: Loss = 0.2408, preLoss = 0.0652

2022-06-16 15:19:31.517060: Epoch 99/150, Train: Loss = 0.2394, preLoss = 0.0647
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2682 0.2682 0.43709298619181736 0.596 0.5330189112047434 0.926
2022-06-16 15:20:27.803102: Epoch 99/150, Test: HR = 0.7630, NDCG = 0.4914
2022-06-16 15:20:29.641908: Model Saved: gowalla

2022-06-16 15:21:14.627869: Epoch 100/150, Train: Loss = 0.2388, preLoss = 0.0649

2022-06-16 15:21:59.783436: Epoch 101/150, Train: Loss = 0.2372, preLoss = 0.0641

2022-06-16 15:22:44.211956: Epoch 102/150, Train: Loss = 0.2361, preLoss = 0.0637
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2709 0.2709 0.43794927274425927 0.5952 0.5348883945932211 0.9267
2022-06-16 15:23:38.894566: Epoch 102/150, Test: HR = 0.7672, NDCG = 0.4941
2022-06-16 15:23:40.635872: Model Saved: gowalla

2022-06-16 15:24:12.540122: Step 13/20: preloss = 0.07, REGLoss = 0.17                                                                                  2022-06-16 15:24:14.812866: Step 14/20: preloss = 0.06, REGLoss = 0.17                                                                                  2022-06-16 15:24:25.455337: Epoch 103/150, Train: Loss = 0.2357, preLoss = 0.0640

2022-06-16 15:25:10.082679: Epoch 104/150, Train: Loss = 0.2330, preLoss = 0.0620

2022-06-16 15:25:54.924142: Epoch 105/150, Train: Loss = 0.2345, preLoss = 0.0642
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2679 0.2679 0.43837758418128797 0.5977 0.5344600585164866 0.9266
2022-06-16 15:26:50.331101: Epoch 105/150, Test: HR = 0.7708, NDCG = 0.4947
2022-06-16 15:26:52.221527: Model Saved: gowalla

2022-06-16 15:27:36.740933: Epoch 106/150, Train: Loss = 0.2320, preLoss = 0.0623

2022-06-16 15:28:21.447551: Epoch 107/150, Train: Loss = 0.2323, preLoss = 0.0632

2022-06-16 15:29:06.160568: Epoch 108/150, Train: Loss = 0.2308, preLoss = 0.0624
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2717 0.2717 0.4402884974529222 0.5992 0.5361546352508981 0.927
2022-06-16 15:30:00.994764: Epoch 108/150, Test: HR = 0.7725, NDCG = 0.4967
2022-06-16 15:30:02.702880: Model Saved: gowalla

2022-06-16 15:30:47.348574: Epoch 109/150, Train: Loss = 0.2305, preLoss = 0.0626

2022-06-16 15:31:31.824808: Epoch 110/150, Train: Loss = 0.2298, preLoss = 0.0626

2022-06-16 15:32:16.489725: Epoch 111/150, Train: Loss = 0.2285, preLoss = 0.0617
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2727 0.2727 0.4408972848469361 0.6 0.5365183646092473 0.9265
2022-06-16 15:33:11.647228: Epoch 111/150, Test: HR = 0.7729, NDCG = 0.4973
2022-06-16 15:33:13.539647: Model Saved: gowalla

2022-06-16 15:33:58.457087: Epoch 112/150, Train: Loss = 0.2286, preLoss = 0.0623

2022-06-16 15:34:43.114507: Epoch 113/150, Train: Loss = 0.2275, preLoss = 0.0617

2022-06-16 15:35:28.371130: Epoch 114/150, Train: Loss = 0.2274, preLoss = 0.0621
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2748 0.2748 0.44272233966034036 0.6018 0.5377792926626378 0.9264
2022-06-16 15:36:23.376634: Epoch 114/150, Test: HR = 0.7726, NDCG = 0.4985
2022-06-16 15:36:25.231627: Model Saved: gowalla

2022-06-16 15:37:10.198665: Epoch 115/150, Train: Loss = 0.2279, preLoss = 0.0630

2022-06-16 15:37:54.667323: Epoch 116/150, Train: Loss = 0.2265, preLoss = 0.0621

^CTraceback (most recent call last):20: preloss = 0.06, REGLoss = 0.16
  File "main.py", line 25, in <module>
    recom.run()
  File "/root/CLSR/model.py", line 50, in run
    reses = self.trainEpoch()
  File "/root/CLSR/model.py", line 391, in trainEpoch
    suLocs, siLocs = self.sampleSslBatch(batIds, self.handler.subadj)
  File "/root/CLSR/model.py", line 338, in sampleSslBatch
    posset = np.reshape(np.argwhere(temLabel[k][i]!=0), [-1])
  File "<__array_function__ internals>", line 2, in reshape
KeyboardInterrupt

root@container-327e11a8ac-4a1523bb:~/CLSR#
root@container-327e11a8ac-4a1523bb:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp  0.1 --ssl_reg 1e-6 --s         ave_path yelp --batch 512 --epoch 150
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2022-06-16 16:22:45.035925: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-16 16:22:45.114762: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:240: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be re         moved in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a fut         ure version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (         from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f26280dcc10>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2         6280dcc10>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f26dc816990>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f2         6dc816990>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:285: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_d         ecay instead.

WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer ins         tead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wra         pper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-16 16:23:07.616338: Model Prepared
2022-06-16 16:23:10.704802: Variables Inited
2022-06-16 16:24:04.808241: Epoch 0/150, Train: Loss = 5.2530, preLoss = 2.2075
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0179 0.0179 0.044482255804958376 0.0715 0.09382755647147337 0.2514
2022-06-16 16:24:54.890895: Epoch 0/150, Test: HR = 0.1355, NDCG = 0.0648
WARNING:tensorflow:From /root/CLSR/model.py:523: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

2022-06-16 16:25:00.807892: Model Saved: yelp

2022-06-16 16:25:43.123447: Epoch 1/150, Train: Loss = 9.7333, preLoss = 3.7311

2022-06-16 16:26:24.746131: Epoch 2/150, Train: Loss = 11.9041, preLoss = 3.8994

2022-06-16 16:27:07.560774: Epoch 3/150, Train: Loss = 13.4371, preLoss = 3.9518
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0833 0.0833 0.14769170628809006 0.2114 0.21618814314014068 0.4563
2022-06-16 16:27:58.627552: Epoch 3/150, Test: HR = 0.3110, NDCG = 0.1796
2022-06-16 16:28:01.987107: Model Saved: yelp

2022-06-16 16:28:45.098657: Epoch 4/150, Train: Loss = 14.1539, preLoss = 3.6954

2022-06-16 16:29:27.538621: Epoch 5/150, Train: Loss = 14.1481, preLoss = 3.1730

2022-06-16 16:30:10.676881: Epoch 6/150, Train: Loss = 13.5742, preLoss = 2.6070
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0624 0.0624 0.1360442296384804 0.2104 0.21358689900441946 0.4872
2022-06-16 16:31:01.190451: Epoch 6/150, Test: HR = 0.3240, NDCG = 0.1725
2022-06-16 16:31:02.942111: Model Saved: yelp

2022-06-16 16:31:44.871419: Epoch 7/150, Train: Loss = 12.6901, preLoss = 2.1394

2022-06-16 16:32:27.070409: Epoch 8/150, Train: Loss = 11.7949, preLoss = 1.8926

2022-06-16 16:33:08.649568: Epoch 9/150, Train: Loss = 10.7654, preLoss = 1.6017
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0707 0.0707 0.15866620863690184 0.246 0.23792624044934654 0.5272
2022-06-16 16:33:58.479744: Epoch 9/150, Test: HR = 0.3684, NDCG = 0.1981
2022-06-16 16:34:00.311386: Model Saved: yelp

2022-06-16 16:34:41.977839: Epoch 10/150, Train: Loss = 9.6677, preLoss = 1.2993

2022-06-16 16:35:23.995675: Epoch 11/150, Train: Loss = 8.6661, preLoss = 1.0962

2022-06-16 16:36:05.269035: Epoch 12/150, Train: Loss = 7.7060, preLoss = 0.9152
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0882 0.0882 0.16969677269514105 0.25 0.2501385138042644 0.5371
2022-06-16 16:36:55.773865: Epoch 12/150, Test: HR = 0.3654, NDCG = 0.2069
2022-06-16 16:36:57.616093: Model Saved: yelp

2022-06-16 16:37:38.619521: Epoch 13/150, Train: Loss = 6.8453, preLoss = 0.7966

2022-06-16 16:38:20.655800: Epoch 14/150, Train: Loss = 6.0506, preLoss = 0.6838

2022-06-16 16:39:02.162844: Epoch 15/150, Train: Loss = 5.3056, preLoss = 0.5471
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0914 0.0914 0.18060228626423527 0.2676 0.26386836764462507 0.562
2022-06-16 16:39:51.475102: Epoch 15/150, Test: HR = 0.3962, NDCG = 0.2221
2022-06-16 16:39:53.427395: Model Saved: yelp

2022-06-16 16:40:35.772998: Epoch 16/150, Train: Loss = 4.6811, preLoss = 0.4777

2022-06-16 16:41:17.962803: Epoch 17/150, Train: Loss = 4.1044, preLoss = 0.3969

2022-06-16 16:42:00.099718: Epoch 18/150, Train: Loss = 3.6135, preLoss = 0.3475
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0972 0.0972 0.1930642552644932 0.2887 0.27608863005167766 0.5813
2022-06-16 16:42:49.177489: Epoch 18/150, Test: HR = 0.4216, NDCG = 0.2358
2022-06-16 16:42:50.887951: Model Saved: yelp

2022-06-16 16:43:32.505729: Epoch 19/150, Train: Loss = 3.1919, preLoss = 0.3124

2022-06-16 16:44:14.366308: Epoch 20/150, Train: Loss = 2.8154, preLoss = 0.2676

2022-06-16 16:44:55.974601: Epoch 21/150, Train: Loss = 2.4913, preLoss = 0.2325
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0917 0.0917 0.1905139257370381 0.287 0.276494768285277 0.5903
2022-06-16 16:45:44.071391: Epoch 21/150, Test: HR = 0.4233, NDCG = 0.2345
2022-06-16 16:45:45.680669: Model Saved: yelp

2022-06-16 16:46:26.836459: Epoch 22/150, Train: Loss = 2.2134, preLoss = 0.2070

2022-06-16 16:47:08.507313: Epoch 23/150, Train: Loss = 1.9726, preLoss = 0.1853

2022-06-16 16:47:50.293712: Epoch 24/150, Train: Loss = 1.7691, preLoss = 0.1732
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0929 0.0929 0.19490393388018057 0.2971 0.2828087973744668 0.607
2022-06-16 16:48:38.916969: Epoch 24/150, Test: HR = 0.4353, NDCG = 0.2396
2022-06-16 16:48:40.550279: Model Saved: yelp

2022-06-16 16:49:22.360403: Epoch 25/150, Train: Loss = 1.5758, preLoss = 0.1465

2022-06-16 16:50:03.686583: Epoch 26/150, Train: Loss = 1.4213, preLoss = 0.1376

2022-06-16 16:50:45.870244: Epoch 27/150, Train: Loss = 1.2834, preLoss = 0.1267
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1073 0.1073 0.20867711548216522 0.3113 0.29866743818369196 0.6263
2022-06-16 16:51:34.454894: Epoch 27/150, Test: HR = 0.4587, NDCG = 0.2564
2022-06-16 16:51:36.035193: Model Saved: yelp

2022-06-16 16:52:17.853006: Epoch 28/150, Train: Loss = 1.1626, preLoss = 0.1163

2022-06-16 16:52:59.500439: Epoch 29/150, Train: Loss = 1.0592, preLoss = 0.1099

2022-06-16 16:53:41.173694: Epoch 30/150, Train: Loss = 0.9697, preLoss = 0.1030
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1171 0.1171 0.21449024472668662 0.3142 0.30686278432128344 0.6374
2022-06-16 16:54:29.247743: Epoch 30/150, Test: HR = 0.4672, NDCG = 0.2640
2022-06-16 16:54:30.941577: Model Saved: yelp

2022-06-16 16:55:12.240615: Epoch 31/150, Train: Loss = 0.8918, preLoss = 0.0967

2022-06-16 16:55:53.849212: Epoch 32/150, Train: Loss = 0.8219, preLoss = 0.0911

2022-06-16 16:56:36.175213: Epoch 33/150, Train: Loss = 0.7632, preLoss = 0.0884
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.126 0.126 0.23050537000967541 0.3333 0.3206590384218404 0.6468
2022-06-16 16:57:24.645765: Epoch 33/150, Test: HR = 0.4888, NDCG = 0.2809
2022-06-16 16:57:26.409130: Model Saved: yelp

2022-06-16 16:58:08.205919: Epoch 34/150, Train: Loss = 0.7095, preLoss = 0.0846

2022-06-16 16:58:49.291369: Epoch 35/150, Train: Loss = 0.6638, preLoss = 0.0832

2022-06-16 16:59:31.371632: Epoch 36/150, Train: Loss = 0.6241, preLoss = 0.0824
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1152 0.1152 0.22678523001770276 0.3403 0.3185675887829238 0.6593
2022-06-16 17:00:19.852398: Epoch 36/150, Test: HR = 0.4980, NDCG = 0.2780
2022-06-16 17:00:21.488402: Model Saved: yelp

2022-06-16 17:01:03.191672: Epoch 37/150, Train: Loss = 0.5859, preLoss = 0.0793

2022-06-16 17:01:44.791729: Epoch 38/150, Train: Loss = 0.5518, preLoss = 0.0764

2022-06-16 17:02:27.179685: Epoch 39/150, Train: Loss = 0.5221, preLoss = 0.0741
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1376 0.1376 0.24897536562575662 0.3544 0.3390899350756222 0.6657
2022-06-16 17:03:16.167567: Epoch 39/150, Test: HR = 0.5099, NDCG = 0.3000
2022-06-16 17:03:17.954958: Model Saved: yelp

2022-06-16 17:04:00.003261: Epoch 40/150, Train: Loss = 0.4974, preLoss = 0.0741

2022-06-16 17:04:41.978484: Epoch 41/150, Train: Loss = 0.4753, preLoss = 0.0740

2022-06-16 17:05:24.212720: Epoch 42/150, Train: Loss = 0.4547, preLoss = 0.0729
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1473 0.1473 0.2563987758971526 0.3625 0.3475184739322469 0.6805
2022-06-16 17:06:13.556992: Epoch 42/150, Test: HR = 0.5157, NDCG = 0.3061
2022-06-16 17:06:15.234246: Model Saved: yelp

2022-06-16 17:06:57.438343: Epoch 43/150, Train: Loss = 0.4348, preLoss = 0.0706

2022-06-16 17:07:39.539222: Epoch 44/150, Train: Loss = 0.4168, preLoss = 0.0691

2022-06-16 17:08:21.618471: Epoch 45/150, Train: Loss = 0.4003, preLoss = 0.0678
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1596 0.1596 0.27098187536613616 0.3832 0.3594415425083828 0.693
2022-06-16 17:09:10.996532: Epoch 45/150, Test: HR = 0.5264, NDCG = 0.3175
2022-06-16 17:09:12.676149: Model Saved: yelp

2022-06-16 17:09:54.462826: Epoch 46/150, Train: Loss = 0.3865, preLoss = 0.0679

2022-06-16 17:10:36.175157: Epoch 47/150, Train: Loss = 0.3730, preLoss = 0.0667

2022-06-16 17:11:18.067870: Epoch 48/150, Train: Loss = 0.3605, preLoss = 0.0656
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1698 0.1698 0.2824862088715914 0.3935 0.3724126193911736 0.7058
2022-06-16 17:12:06.185521: Epoch 48/150, Test: HR = 0.5441, NDCG = 0.3316
2022-06-16 17:12:07.956753: Model Saved: yelp

2022-06-16 17:12:49.572608: Epoch 49/150, Train: Loss = 0.3511, preLoss = 0.0663

2022-06-16 17:13:31.076180: Epoch 50/150, Train: Loss = 0.3411, preLoss = 0.0658

2022-06-16 17:14:12.179947: Epoch 51/150, Train: Loss = 0.3305, preLoss = 0.0642
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1674 0.1674 0.28683276796890034 0.4009 0.37460637335015395 0.708
2022-06-16 17:15:00.223048: Epoch 51/150, Test: HR = 0.5436, NDCG = 0.3331
2022-06-16 17:15:02.005356: Model Saved: yelp

2022-06-16 17:15:43.992108: Epoch 52/150, Train: Loss = 0.3225, preLoss = 0.0642

2022-06-16 17:16:25.654066: Epoch 53/150, Train: Loss = 0.3162, preLoss = 0.0656

2022-06-16 17:17:07.628867: Epoch 54/150, Train: Loss = 0.3076, preLoss = 0.0640
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1726 0.1726 0.2884451319847989 0.4005 0.3795159534967553 0.7157
2022-06-16 17:17:56.528131: Epoch 54/150, Test: HR = 0.5548, NDCG = 0.3390
2022-06-16 17:17:58.269641: Model Saved: yelp

2022-06-16 17:18:40.773977: Epoch 55/150, Train: Loss = 0.2994, preLoss = 0.0622

2022-06-16 17:19:22.398156: Epoch 56/150, Train: Loss = 0.2924, preLoss = 0.0614

2022-06-16 17:20:04.296050: Epoch 57/150, Train: Loss = 0.2869, preLoss = 0.0616
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1794 0.1794 0.29863583986704517 0.417 0.3859071364828105 0.7182
2022-06-16 17:20:52.432428: Epoch 57/150, Test: HR = 0.5625, NDCG = 0.3467
2022-06-16 17:20:54.158841: Model Saved: yelp

2022-06-16 17:21:35.819534: Epoch 58/150, Train: Loss = 0.2813, preLoss = 0.0613

2022-06-16 17:22:18.068200: Epoch 59/150, Train: Loss = 0.2766, preLoss = 0.0616

2022-06-16 17:22:59.716927: Epoch 60/150, Train: Loss = 0.2722, preLoss = 0.0619
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1724 0.1724 0.2955310088665368 0.4166 0.38614207035312687 0.728
2022-06-16 17:23:48.523552: Epoch 60/150, Test: HR = 0.5724, NDCG = 0.3470
2022-06-16 17:23:50.268807: Model Saved: yelp

2022-06-16 17:24:31.754186: Epoch 61/150, Train: Loss = 0.2662, preLoss = 0.0601

2022-06-16 17:25:13.762714: Epoch 62/150, Train: Loss = 0.2615, preLoss = 0.0596

2022-06-16 17:25:56.105620: Epoch 63/150, Train: Loss = 0.2585, preLoss = 0.0605
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1893 0.1893 0.3075825796857781 0.4295 0.39464150308721324 0.7319
2022-06-16 17:26:44.688761: Epoch 63/150, Test: HR = 0.5758, NDCG = 0.3554
2022-06-16 17:26:46.504817: Model Saved: yelp

2022-06-16 17:27:28.477622: Epoch 64/150, Train: Loss = 0.2530, preLoss = 0.0586

2022-06-16 17:28:10.056196: Epoch 65/150, Train: Loss = 0.2504, preLoss = 0.0596

2022-06-16 17:28:52.427289: Epoch 66/150, Train: Loss = 0.2457, preLoss = 0.0579
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1783 0.1783 0.2993164895350113 0.4202 0.3900814302017778 0.7322
2022-06-16 17:29:41.199562: Epoch 66/150, Test: HR = 0.5765, NDCG = 0.3508
2022-06-16 17:29:43.117303: Model Saved: yelp

2022-06-16 17:30:25.600138: Epoch 67/150, Train: Loss = 0.2434, preLoss = 0.0587

2022-06-16 17:31:07.863124: Epoch 68/150, Train: Loss = 0.2411, preLoss = 0.0592

2022-06-16 17:31:49.895201: Epoch 69/150, Train: Loss = 0.2371, preLoss = 0.0580
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1832 0.1832 0.3071216008020401 0.4295 0.39567576983467034 0.7352
2022-06-16 17:32:40.133735: Epoch 69/150, Test: HR = 0.5823, NDCG = 0.3572
2022-06-16 17:32:41.973285: Model Saved: yelp

2022-06-16 17:33:24.423054: Epoch 70/150, Train: Loss = 0.2339, preLoss = 0.0573

2022-06-16 17:34:07.115506: Epoch 71/150, Train: Loss = 0.2316, preLoss = 0.0573

2022-06-16 17:34:49.483281: Epoch 72/150, Train: Loss = 0.2284, preLoss = 0.0565
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1885 0.1885 0.30873637963255385 0.4272 0.39917136093362665 0.7395
2022-06-16 17:35:38.966338: Epoch 72/150, Test: HR = 0.5845, NDCG = 0.3602
2022-06-16 17:35:41.292681: Model Saved: yelp

2022-06-16 17:36:23.769580: Epoch 73/150, Train: Loss = 0.2270, preLoss = 0.0573

2022-06-16 17:37:06.481292: Epoch 74/150, Train: Loss = 0.2243, preLoss = 0.0566

2022-06-16 17:37:48.202472: Epoch 75/150, Train: Loss = 0.2222, preLoss = 0.0566
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1923 0.1923 0.3126902412840747 0.4314 0.40201051030535284 0.7397
2022-06-16 17:38:36.882826: Epoch 75/150, Test: HR = 0.5858, NDCG = 0.3632
2022-06-16 17:38:38.949665: Model Saved: yelp

2022-06-16 17:39:20.906380: Epoch 76/150, Train: Loss = 0.2196, preLoss = 0.0559

2022-06-16 17:40:03.068410: Epoch 77/150, Train: Loss = 0.2173, preLoss = 0.0553

2022-06-16 17:40:44.816086: Epoch 78/150, Train: Loss = 0.2154, preLoss = 0.0552
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2059 0.2059 0.3157449412045868 0.4233 0.4083645686748414 0.7403
2022-06-16 17:41:33.725857: Epoch 78/150, Test: HR = 0.5887, NDCG = 0.3701
2022-06-16 17:41:36.049478: Model Saved: yelp

2022-06-16 17:42:17.335317: Epoch 79/150, Train: Loss = 0.2132, preLoss = 0.0547

2022-06-16 17:42:59.794405: Epoch 80/150, Train: Loss = 0.2115, preLoss = 0.0545

2022-06-16 17:43:42.366848: Epoch 81/150, Train: Loss = 0.2109, preLoss = 0.0553
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1969 0.1969 0.31572983271159033 0.4308 0.4070815039805313 0.7432
2022-06-16 17:44:30.827262: Epoch 81/150, Test: HR = 0.5945, NDCG = 0.3696
2022-06-16 17:44:33.159623: Model Saved: yelp

2022-06-16 17:45:14.695908: Epoch 82/150, Train: Loss = 0.2091, preLoss = 0.0548

2022-06-16 17:45:56.560222: Epoch 83/150, Train: Loss = 0.2073, preLoss = 0.0544

2022-06-16 17:46:37.974547: Epoch 84/150, Train: Loss = 0.2051, preLoss = 0.0535
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1947 0.1947 0.3142194907436239 0.4292 0.4066199109176395 0.7449
2022-06-16 17:47:26.328005: Epoch 84/150, Test: HR = 0.5965, NDCG = 0.3692
2022-06-16 17:47:28.737875: Model Saved: yelp

2022-06-16 17:48:10.528494: Epoch 85/150, Train: Loss = 0.2039, preLoss = 0.0535

2022-06-16 17:48:52.562954: Epoch 86/150, Train: Loss = 0.2024, preLoss = 0.0532

2022-06-16 17:49:34.066851: Epoch 87/150, Train: Loss = 0.2013, preLoss = 0.0533
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1988 0.1988 0.3179181714155915 0.4316 0.4103128655875029 0.7476
2022-06-16 17:50:22.182715: Epoch 87/150, Test: HR = 0.5974, NDCG = 0.3726
2022-06-16 17:50:24.185445: Model Saved: yelp

2022-06-16 17:51:05.226816: Epoch 88/150, Train: Loss = 0.2007, preLoss = 0.0538

2022-06-16 17:51:46.994528: Epoch 89/150, Train: Loss = 0.1986, preLoss = 0.0528

2022-06-16 17:52:28.416235: Epoch 90/150, Train: Loss = 0.1978, preLoss = 0.0530
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1928 0.1928 0.3162208023463751 0.4333 0.4077981202506169 0.7472
2022-06-16 17:53:17.025822: Epoch 90/150, Test: HR = 0.5956, NDCG = 0.3696
2022-06-16 17:53:18.874656: Model Saved: yelp

2022-06-16 17:54:00.705732: Epoch 91/150, Train: Loss = 0.1971, preLoss = 0.0532

2022-06-16 17:54:43.089028: Epoch 92/150, Train: Loss = 0.1965, preLoss = 0.0535

2022-06-16 17:55:25.028414: Epoch 93/150, Train: Loss = 0.1941, preLoss = 0.0519
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.197 0.197 0.3175635676899608 0.4331 0.4098002757843309 0.7488
2022-06-16 17:56:13.758049: Epoch 93/150, Test: HR = 0.5955, NDCG = 0.3711
2022-06-16 17:56:15.548457: Model Saved: yelp

2022-06-16 17:56:57.063303: Epoch 94/150, Train: Loss = 0.1946, preLoss = 0.0533

2022-06-16 17:57:38.591195: Epoch 95/150, Train: Loss = 0.1934, preLoss = 0.0529

2022-06-16 17:58:20.044857: Epoch 96/150, Train: Loss = 0.1925, preLoss = 0.0527
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1945 0.1945 0.3178665544172021 0.4349 0.41077021618895504 0.7518
2022-06-16 17:59:08.407073: Epoch 96/150, Test: HR = 0.6014, NDCG = 0.3728
2022-06-16 17:59:10.520968: Model Saved: yelp

2022-06-16 17:59:52.119597: Epoch 97/150, Train: Loss = 0.1916, preLoss = 0.0526

2022-06-16 18:00:33.868020: Epoch 98/150, Train: Loss = 0.1923, preLoss = 0.0540

2022-06-16 18:01:15.617556: Epoch 99/150, Train: Loss = 0.1905, preLoss = 0.0529
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1965 0.1965 0.3199826450972662 0.4388 0.41213987243352795 0.7547
2022-06-16 18:02:04.143226: Epoch 99/150, Test: HR = 0.6022, NDCG = 0.3737
2022-06-16 18:02:06.035904: Model Saved: yelp

2022-06-16 18:02:47.090336: Epoch 100/150, Train: Loss = 0.1887, preLoss = 0.0517

2022-06-16 18:03:28.325691: Epoch 101/150, Train: Loss = 0.1880, preLoss = 0.0517

2022-06-16 18:04:10.072694: Epoch 102/150, Train: Loss = 0.1881, preLoss = 0.0523
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1964 0.1964 0.32158122908045816 0.4411 0.41342165616071724 0.7554
2022-06-16 18:04:58.733521: Epoch 102/150, Test: HR = 0.6067, NDCG = 0.3760
2022-06-16 18:05:00.541591: Model Saved: yelp

2022-06-16 18:05:42.201135: Epoch 103/150, Train: Loss = 0.1868, preLoss = 0.0516

2022-06-16 18:06:23.688807: Epoch 104/150, Train: Loss = 0.1863, preLoss = 0.0517

2022-06-16 18:07:05.735058: Epoch 105/150, Train: Loss = 0.1865, preLoss = 0.0523
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1999 0.1999 0.32533701052733355 0.4455 0.415695760211068 0.7543
2022-06-16 18:07:54.556816: Epoch 105/150, Test: HR = 0.6095, NDCG = 0.3792
2022-06-16 18:07:56.511544: Model Saved: yelp

2022-06-16 18:08:38.568721: Epoch 106/150, Train: Loss = 0.1845, preLoss = 0.0509

2022-06-16 18:09:20.270406: Epoch 107/150, Train: Loss = 0.1857, preLoss = 0.0526

2022-06-16 18:10:01.968429: Epoch 108/150, Train: Loss = 0.1839, preLoss = 0.0513
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1982 0.1982 0.32412965918152425 0.4459 0.4149122631218836 0.7565
2022-06-16 18:10:50.292512: Epoch 108/150, Test: HR = 0.6096, NDCG = 0.3778
2022-06-16 18:10:52.227524: Model Saved: yelp

2022-06-16 18:11:33.732170: Epoch 109/150, Train: Loss = 0.1835, preLoss = 0.0513

2022-06-16 18:12:15.400675: Epoch 110/150, Train: Loss = 0.1828, preLoss = 0.0510

2022-06-16 18:12:57.300970: Epoch 111/150, Train: Loss = 0.1818, preLoss = 0.0505
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1988 0.1988 0.32449281448169354 0.4447 0.41574020752296326 0.7564
2022-06-16 18:13:45.441952: Epoch 111/150, Test: HR = 0.6099, NDCG = 0.3788
2022-06-16 18:13:47.267573: Model Saved: yelp

2022-06-16 18:14:28.698139: Epoch 112/150, Train: Loss = 0.1814, preLoss = 0.0505

2022-06-16 18:15:10.607388: Epoch 113/150, Train: Loss = 0.1816, preLoss = 0.0511

2022-06-16 18:15:52.475876: Epoch 114/150, Train: Loss = 0.1823, preLoss = 0.0521
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2025 0.2025 0.3267601780408828 0.4466 0.4174815550638799 0.7567
2022-06-16 18:16:41.388313: Epoch 114/150, Test: HR = 0.6116, NDCG = 0.3809
2022-06-16 18:16:43.424744: Model Saved: yelp

2022-06-16 18:17:25.133846: Epoch 115/150, Train: Loss = 0.1799, preLoss = 0.0501

2022-06-16 18:18:06.831197: Epoch 116/150, Train: Loss = 0.1808, preLoss = 0.0513

2022-06-16 18:18:48.586637: Epoch 117/150, Train: Loss = 0.1798, preLoss = 0.0506
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2059 0.2059 0.3282675959331758 0.4455 0.4199456369366586 0.7588
2022-06-16 18:19:36.897515: Epoch 117/150, Test: HR = 0.6123, NDCG = 0.3830
2022-06-16 18:19:38.801512: Model Saved: yelp

2022-06-16 18:20:20.244349: Epoch 118/150, Train: Loss = 0.1790, preLoss = 0.0502

2022-06-16 18:21:02.162391: Epoch 119/150, Train: Loss = 0.1800, preLoss = 0.0515

2022-06-16 18:21:43.798352: Epoch 120/150, Train: Loss = 0.1783, preLoss = 0.0500
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2053 0.2053 0.3268166714195762 0.4441 0.41894059947616313 0.7587
2022-06-16 18:22:32.415155: Epoch 120/150, Test: HR = 0.6126, NDCG = 0.3822
2022-06-16 18:22:34.322817: Model Saved: yelp

2022-06-16 18:23:16.417978: Epoch 121/150, Train: Loss = 0.1786, preLoss = 0.0506

2022-06-16 18:23:58.020680: Epoch 122/150, Train: Loss = 0.1778, preLoss = 0.0501

2022-06-16 18:24:39.735698: Epoch 123/150, Train: Loss = 0.1780, preLoss = 0.0506
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.204 0.204 0.3269369237885022 0.4452 0.4185740900282544 0.7585
2022-06-16 18:25:28.390344: Epoch 123/150, Test: HR = 0.6126, NDCG = 0.3818
2022-06-16 18:25:30.280746: Model Saved: yelp

2022-06-16 18:26:11.687275: Epoch 124/150, Train: Loss = 0.1764, preLoss = 0.0492

2022-06-16 18:26:53.101556: Epoch 125/150, Train: Loss = 0.1776, preLoss = 0.0507

2022-06-16 18:27:34.524708: Epoch 126/150, Train: Loss = 0.1761, preLoss = 0.0495
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2063 0.2063 0.32732737660545286 0.4449 0.41913695635748965 0.7588
2022-06-16 18:28:23.218943: Epoch 126/150, Test: HR = 0.6125, NDCG = 0.3823
2022-06-16 18:28:25.135100: Model Saved: yelp

2022-06-16 18:29:06.920913: Epoch 127/150, Train: Loss = 0.1766, preLoss = 0.0502

2022-06-16 18:29:48.948267: Epoch 128/150, Train: Loss = 0.1760, preLoss = 0.0497

2022-06-16 18:30:30.640407: Epoch 129/150, Train: Loss = 0.1758, preLoss = 0.0498
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2066 0.2066 0.3262905422987198 0.4415 0.41948726835329664 0.7597
2022-06-16 18:31:19.601402: Epoch 129/150, Test: HR = 0.6125, NDCG = 0.3824
2022-06-16 18:31:21.725754: Model Saved: yelp

2022-06-16 18:32:03.439032: Epoch 130/150, Train: Loss = 0.1760, preLoss = 0.0501

2022-06-16 18:32:45.524792: Epoch 131/150, Train: Loss = 0.1755, preLoss = 0.0498

2022-06-16 18:33:26.618428: Epoch 132/150, Train: Loss = 0.1744, preLoss = 0.0489
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.208 0.208 0.32733562304063196 0.4422 0.4203511884567066 0.7597
2022-06-16 18:34:15.376430: Epoch 132/150, Test: HR = 0.6121, NDCG = 0.3832
2022-06-16 18:34:17.328965: Model Saved: yelp

2022-06-16 18:34:59.138090: Epoch 133/150, Train: Loss = 0.1757, preLoss = 0.0504

2022-06-16 18:35:40.715470: Epoch 134/150, Train: Loss = 0.1748, preLoss = 0.0498

2022-06-16 18:36:22.336209: Epoch 135/150, Train: Loss = 0.1748, preLoss = 0.0499
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2078 0.2078 0.3273140315836661 0.442 0.4202627551810019 0.7588
2022-06-16 18:37:11.114703: Epoch 135/150, Test: HR = 0.6128, NDCG = 0.3835
2022-06-16 18:37:13.164528: Model Saved: yelp

2022-06-16 18:37:54.648223: Epoch 136/150, Train: Loss = 0.1740, preLoss = 0.0493

2022-06-16 18:38:36.310954: Epoch 137/150, Train: Loss = 0.1747, preLoss = 0.0502

2022-06-16 18:39:17.919875: Epoch 138/150, Train: Loss = 0.1739, preLoss = 0.0494
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.207 0.207 0.3270849578264427 0.442 0.41980505893461867 0.758
2022-06-16 18:40:06.776193: Epoch 138/150, Test: HR = 0.6123, NDCG = 0.3831
2022-06-16 18:40:08.986924: Model Saved: yelp

2022-06-16 18:40:50.816043: Epoch 139/150, Train: Loss = 0.1745, preLoss = 0.0502

2022-06-16 18:41:32.349941: Epoch 140/150, Train: Loss = 0.1739, preLoss = 0.0499

2022-06-16 18:42:14.314807: Epoch 141/150, Train: Loss = 0.1732, preLoss = 0.0493
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2067 0.2067 0.32804070817235215 0.444 0.4200615414769519 0.7579
2022-06-16 18:43:02.729982: Epoch 141/150, Test: HR = 0.6117, NDCG = 0.3832
2022-06-16 18:43:04.695439: Model Saved: yelp

2022-06-16 18:43:46.475217: Epoch 142/150, Train: Loss = 0.1739, preLoss = 0.0500

2022-06-16 18:44:27.890904: Epoch 143/150, Train: Loss = 0.1734, preLoss = 0.0496

2022-06-16 18:45:09.867421: Epoch 144/150, Train: Loss = 0.1730, preLoss = 0.0495
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2064 0.2064 0.3283378561502638 0.4449 0.4204041229581586 0.7595
2022-06-16 18:45:58.314325: Epoch 144/150, Test: HR = 0.6121, NDCG = 0.3833
2022-06-16 18:46:00.224857: Model Saved: yelp

2022-06-16 18:46:41.622677: Epoch 145/150, Train: Loss = 0.1725, preLoss = 0.0491

2022-06-16 18:47:22.896115: Epoch 146/150, Train: Loss = 0.1730, preLoss = 0.0497

2022-06-16 18:48:04.722089: Epoch 147/150, Train: Loss = 0.1727, preLoss = 0.0495
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2062 0.2062 0.32895041210758946 0.4467 0.4203972945966818 0.7594
2022-06-16 18:48:53.168124: Epoch 147/150, Test: HR = 0.6127, NDCG = 0.3834
2022-06-16 18:48:55.186119: Model Saved: yelp

2022-06-16 18:49:36.401660: Epoch 148/150, Train: Loss = 0.1723, preLoss = 0.0492

2022-06-16 18:50:17.825996: Epoch 149/150, Train: Loss = 0.1734, preLoss = 0.0504

epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2061 0.2061 0.3288271761531688 0.4461 0.42062876609574135 0.76
2022-06-16 18:51:05.906281: Epoch 150/150, Test: HR = 0.6120, NDCG = 0.3833
2022-06-16 18:51:07.706315: Model Saved: yelp
root@container-327e11a8ac-4a1523bb:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp  0.1 --ssl_reg 1e-6 --save_path yelp --batch 512 --epoch 150
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2022-06-16 19:14:17.652905: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-16 19:14:17.810889: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:240: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fce285b0cd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fce285b0cd0>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fcf2f097990>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fcf2f097990>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:285: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.

WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-16 19:14:40.839787: Model Prepared
2022-06-16 19:14:45.305652: Variables Inited
pred one (2246,) [0.14031921 0.0931527  0.1325419  ... 0.24967279 0.22290774 0.22290774]
pred one (2044,) [0.2723867  0.292104   0.30260095 ... 0.1819928  0.16561864 0.16561864]
pred one (2194,) [0.224051   0.10171409 0.22446615 ... 0.11251735 0.19391711 0.12265284]
pred one (2170,) [0.20763391 0.2906977  0.14932346 ... 0.18248807 0.18248807 0.14888993]
pred one (2356,) [0.14381078 0.14381078 0.18224454 ... 0.16590977 0.13335711 0.17155847]
pred one (2458,) [0.22290084 0.26089805 0.22290084 ... 0.42936707 0.39339074 0.14403133]
pred one (2242,) [0.30374688 0.182413   0.15605922 ... 0.14718658 0.04380035 0.07184312]
pred one (2228,) [0.18264931 0.38837233 0.23777847 ... 0.19195285 0.19195285 0.19657683]
pred one (2284,) [0.1919344  0.30978823 0.1919344  ... 0.22871694 0.17207989 0.15203623]
pred one (2006,) [0.05873178 0.13899162 0.5205697  ... 0.24111836 0.15433358 0.25155944]
pred one (2522,) [0.27751374 0.25066966 0.49011892 ... 0.09527422 0.26866317 0.26866317]
pred one (2140,) [0.41171315 0.07699998 0.04138967 ... 0.26879793 0.23175842 0.21014091]
pred one (2112,) [0.06835946 0.13572332 0.31433132 ... 0.349848   0.41118765 0.31395745]
pred one (1974,) [0.35372606 0.4291014  0.35372606 ... 0.26233172 0.17496735 0.67651665]
pred one (2248,) [0.3071239  0.2704019  0.3064048  ... 0.02468167 0.08826033 0.01486183]
pred one (2274,) [0.33975905 0.09392829 0.3259545  ... 0.09571664 0.11142553 0.17000069]
pred one (2314,) [0.18106872 0.19115745 0.19115745 ... 0.47641492 0.22542804 0.22542804]
pred one (2110,) [0.27807736 0.8499135  0.5071809  ... 0.11001597 0.36656845 0.25212383]
pred one (2288,) [0.6126224  0.6126224  0.51892185 ... 0.15760326 0.15615308 0.22723286]
pred one (1138,) [0.32657915 0.496503   0.32657915 ... 0.14601807 0.15039346 0.40271914]
2022-06-16 19:15:35.500014: Epoch 0/150, Train: Loss = 5.2710, preLoss = 2.2267
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0173 0.0173 0.04820153741525268 0.0795 0.0989453962455504 0.2639
2022-06-16 19:16:25.157630: Epoch 0/150, Test: HR = 0.1445, NDCG = 0.0690
WARNING:tensorflow:From /root/CLSR/model.py:524: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

2022-06-16 19:16:27.247176: Model Saved: yelp

pred one (2336,) [0.18688972 0.36622474 0.51765007 ... 0.6152169  0.59545535 0.7121196 ]
pred one (2248,) [0.22539972 0.2766143  0.28969488 ... 0.43362066 0.5914361  0.6242704 ]
pred one (2354,) [0.23192063 0.393161   0.20849507 ... 0.3179688  0.45456204 0.45456204]
pred one (2262,) [0.334084   0.334084   0.777806   ... 0.51571786 0.43966654 0.43966654]
pred one (2442,) [0.41281807 0.3981055  0.36684993 ... 0.48973912 0.3814388  0.27735603]
pred one (2476,) [0.5447127  0.90206134 0.78455746 ... 0.2859161  0.19400248 0.19400248]
pred one (2182,) [0.78688437 0.78688437 0.68600583 ... 0.39615375 0.7107042  0.55413663]
pred one (2486,) [0.06834728 0.3355465  0.47764054 ... 0.563792   0.19563515 0.6073601 ]
pred one (2304,) [0.47100985 0.43814173 0.3766408  ... 0.44346663 0.10183676 0.22287582]
pred one (2182,) [0.4445162  0.4172776  0.3698593  ... 0.12280033 0.55532134 0.45092785]
pred one (2228,) [0.7566826  0.7566826  0.60247135 ... 0.706636   0.48495173 0.46115708]
pred one (2232,) [0.33294535 0.29760307 0.39523223 ... 0.57294834 0.6481205  0.43398088]
pred one (2284,) [0.53813136 0.42526203 0.47598124 ... 0.4967643  0.52957183 0.52957183]
pred one (2354,) [-0.02808153 -0.02808153 -0.02808153 ...  0.29437196  0.42683187
  0.42683187]
pred one (2152,) [0.75652754 0.9123154  0.3889283  ... 0.9402138  0.5930051  0.13053936]
pred one (2044,) [0.3951789  0.2969458  0.52529156 ... 0.89822197 0.5295894  0.5295894 ]
pred one (2496,) [0.21656594 0.36449182 0.29777893 ... 0.3034051  0.37365803 0.01542338]
pred one (2108,) [0.3720493  0.18022715 0.3720493  ... 0.31302434 0.4670003  0.47538862]
pred one (2348,) [0.19169101 0.4615526  0.4615526  ... 0.56584305 0.5703175  0.5703175 ]
pred one (1214,) [0.98683    0.898407   0.78873396 ... 0.509346   0.74576056 0.43667597]
2022-06-16 19:17:07.678550: Epoch 1/150, Train: Loss = 9.7559, preLoss = 3.7511

pred one (2168,) [0.5771433  0.98019147 0.8335619  ... 0.6809093  0.56556046 0.33024544]
pred one (2372,) [0.35103542 0.4830822  0.58833444 ... 0.47379065 0.47379065 0.45376816]
pred one (2158,) [0.7882584  0.5328045  0.7882584  ... 0.80714136 0.68032366 0.59397864]
pred one (2268,) [0.5746738  0.5746738  0.5746738  ... 0.5125178  0.31545097 0.31545097]
pred one (2248,) [1.4258038  1.0003545  1.4258038  ... 0.506258   0.27726895 0.14228821]
pred one (2038,) [0.70689064 0.6896759  0.48403162 ... 0.86742985 0.94059587 1.0738972 ]
pred one (2196,) [0.21482073 0.21482073 0.4100498  ... 0.30785826 0.6028377  0.5073421 ]
pred one (2252,) [0.25841373 0.20675367 0.34648812 ... 0.48636314 0.38673842 0.5492448 ]
pred one (2410,) [0.55089545 0.27979124 0.5280733  ... 0.36188814 0.16157193 0.5713544 ]
pred one (2206,) [0.33701956 0.33701956 0.5679233  ... 0.2378186  0.60137415 0.5004771 ]
pred one (2386,) [0.3469596  0.3469596  0.8589512  ... 0.3655491  0.3655491  0.69735193]
pred one (2174,) [0.27733415 0.2535198  0.3924665  ... 0.21770951 0.33828107 0.44018662]
pred one (2188,) [0.88400674 1.063317   1.1582215  ... 0.15583359 0.02269903 0.02269903]
pred one (2476,) [0.51312697 0.84571415 0.75226915 ... 0.5625512  0.5625512  0.54042906]
pred one (2496,) [0.53106093 0.53106093 0.45692313 ... 0.6021543  0.47835043 0.47330078]
pred one (1998,) [0.39955893 0.8884686  0.51722914 ... 0.4998111  0.8678371  0.6547929 ]
pred one (2270,) [0.50511456 0.36789376 0.47642457 ... 0.2835523  0.2835523  0.8744432 ]
pred one (2066,) [0.13496009 0.3060139  0.13984443 ... 0.7266051  0.33949444 0.17247385]
pred one (2442,) [0.50675905 0.6324122  0.5671853  ... 0.58960485 0.7273667  0.58960485]
pred one (1036,) [0.5902231  0.55263555 0.82157934 ... 0.27861285 0.16345474 0.16660455]
2022-06-16 19:17:47.654245: Epoch 2/150, Train: Loss = 12.0426, preLoss = 4.0108

pred one (2452,) [0.30957946 0.32272387 0.12849963 ... 0.820611   0.6717355  0.820611  ]
pred one (2218,) [0.07338659 0.25240263 0.07338659 ... 0.8340453  0.8340453  0.8340453 ]
pred one (2202,) [0.30340815 0.50296855 0.30340815 ... 0.52609676 0.3925519  0.21294174]
pred one (2206,) [0.36448088 0.5946239  0.6424699  ... 0.9971615  0.5704613  0.3870257 ]
pred one (2262,) [0.663942   0.8362658  0.8954977  ... 0.69379675 0.6073733  0.5492511 ]
pred one (2218,) [0.4441061  0.49134374 0.4423347  ... 0.6840988  1.0196788  0.6840988 ]
pred one (2262,) [-0.01834435  0.00882809  0.80503464 ...  0.45013505  0.5575713
  0.18328932]
pred one (2472,) [0.27231002 0.27231002 0.78837925 ... 0.4108892  0.4108892  0.5473074 ]
pred one (2140,) [0.6446302  0.5700698  0.76847875 ... 0.7113441  0.5122251  0.38576162]
pred one (2176,) [0.6190146  0.14926293 0.72047055 ... 0.24335974 0.6188297  0.6706807 ]
pred one (2220,) [0.24727806 0.57503104 0.18453214 ... 0.2588327  0.3614409  0.3614409 ]
pred one (2188,) [0.8356808  0.9375186  0.74191546 ... 0.49500805 1.0164313  0.57680744]
pred one (2456,) [0.6849591  0.2113431  0.6138767  ... 0.15193653 0.32066387 0.24202712]
pred one (2194,) [0.45906445 0.2920617  0.5930786  ... 1.0151618  1.0151618  1.0151618 ]
pred one (2462,) [0.4446751  0.7507478  0.12951353 ... 0.44563282 0.7045645  0.66299754]
pred one (2308,) [0.36200535 0.5418185  0.4778872  ... 0.27870092 0.40924376 0.27870092]
pred one (2144,) [0.7396147  0.7396147  0.55838907 ... 0.13588354 0.17888916 0.36976108]
pred one (2222,) [1.1164621  1.0944681  1.0944681  ... 0.51921153 0.57945627 0.51921153]
pred one (2180,) [0.21123204 0.3363546  0.40693977 ... 0.5478399  0.7858771  0.71545684]
pred one (1202,) [0.3103872  0.3103872  0.37496263 ... 0.9391426  0.72667396 0.72667396]
2022-06-16 19:18:28.027796: Epoch 3/150, Train: Loss = 13.5519, preLoss = 4.0135
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0731 0.0731 0.13482500754882049 0.1972 0.20586962892283436 0.4524
2022-06-16 19:19:17.815967: Epoch 3/150, Test: HR = 0.2965, NDCG = 0.1667
2022-06-16 19:19:19.293143: Model Saved: yelp

pred one (2474,) [0.48548278 0.6126747  0.48548278 ... 0.6961353  0.6961353  0.90850574]
pred one (2386,) [0.0988069  0.0988069  0.13751966 ... 0.5371283  0.79439646 0.39398372]
pred one (2274,) [0.9559797  0.60097504 0.60097504 ... 0.3256247  0.31445056 0.34606346]
pred one (2136,) [0.8388889  0.9693699  0.950903   ... 0.97647506 0.7724189  0.7724189 ]
pred one (2334,) [0.70986414 0.6161331  0.8091684  ... 0.22574085 0.1635881  0.1635881 ]
pred one (2226,) [0.10905763 0.10905763 0.6745461  ... 0.36367378 0.753111   0.54131603]
pred one (2084,) [0.77613616 0.46183705 0.77613616 ... 0.28905085 0.33543485 0.46886593]
pred one (2258,) [0.7381783  0.6247109  0.75485075 ... 0.4247641  1.0844296  1.0844296 ]
pred one (2322,) [0.54659843 0.68958426 1.1749772  ... 0.35599643 0.21006487 0.5703735 ]
pred one (2228,) [0.79497445 0.7712188  0.6582358  ... 0.51181793 0.51181793 0.24788266]
pred one (2028,) [0.24655446 0.21320492 0.43692434 ... 0.493567   0.3554514  0.3554514 ]
pred one (2304,) [-0.03455393  0.10511273  0.5450624  ...  0.40553126  0.56578845
  0.56578845]
pred one (2606,) [0.1688756  0.3070078  0.60952336 ... 0.97157246 1.1382709  1.1382709 ]
pred one (2290,) [0.4981551  0.83474904 0.54230773 ... 0.681862   0.596501   0.7471775 ]
pred one (2232,) [0.47429243 1.0710336  0.5201526  ... 0.6080074  0.5510897  0.46889743]
pred one (2156,) [0.7315173  0.43643305 0.6768549  ... 0.5554053  0.5383109  0.5554053 ]
pred one (2386,) [0.47317967 0.6519842  0.47317967 ... 0.67600596 0.4564321  0.46081576]
pred one (2308,) [0.33348262 0.7378291  0.29998526 ... 0.3669915  0.5202205  0.63547444]
pred one (2480,) [0.7328291  0.7328291  0.8383774  ... 0.48131514 0.6962479  0.6962479 ]
pred one (1146,) [1.3757892 1.0097003 0.3488369 ... 0.8326626 0.8326626 0.8532866]
2022-06-16 19:19:59.845922: Epoch 4/150, Train: Loss = 14.1550, preLoss = 3.6222

pred one (2182,) [0.5636554  0.44860333 1.4260539  ... 0.7842605  0.6624223  0.7842605 ]
pred one (2486,) [0.69755876 0.69755876 0.28109556 ... 0.78023356 0.9334023  0.75207496]
pred one (2408,) [0.72600996 0.5709382  0.48412177 ... 0.617242   0.92457587 0.9075967 ]
pred one (2168,) [0.55686796 0.74955785 0.5285655  ... 0.1942431  0.5515811  0.35085595]
pred one (2530,) [0.01356775 0.01356775 0.7856058  ... 0.6389098  0.8621696  0.89872456]
pred one (2406,) [0.5685698  0.27958447 1.0280803  ... 0.4833853  0.19056733 0.79728675]
pred one (2320,) [0.2650239  0.2650239  0.62626827 ... 0.36236778 0.36236778 0.36236778]
pred one (2298,) [0.5519999  0.7082675  0.47461373 ... 0.3959694  0.6455351  0.6455351 ]
pred one (2428,) [0.76293707 0.7296739  0.7296739  ... 0.80758655 0.3886544  0.3886544 ]
pred one (2280,) [0.7764587  0.7764587  0.67164236 ... 0.24350213 0.33789077 0.24350213]
pred one (2208,) [0.49348137 0.373972   0.3913086  ... 0.5757335  0.20659252 0.51284033]
pred one (2180,) [0.6091981  0.6091981  0.539745   ... 0.1487038  0.11609575 0.63384724]
pred one (2278,) [0.6156993  0.6014127  0.5533341  ... 0.35835928 0.6658156  0.55832756]
pred one (2208,) [0.70229506 0.70229506 0.5754457  ... 0.5751256  0.23049808 0.23049808]
pred one (2462,) [0.7541669  1.1715674  1.0178891  ... 0.63054955 0.8020724  0.5551617 ]
pred one (2024,) [0.26008868 0.39504135 0.52190894 ... 1.070126   1.2240705  1.2240705 ]
pred one (2258,) [0.64845914 0.5153262  0.6317116  ... 0.8784639  0.53951484 0.9166385 ]
pred one (2356,) [0.42709348 0.424792   0.424792   ... 0.28307438 0.18820685 0.051934  ]
pred one (2246,) [1.1599684  0.46833587 0.4609699  ... 0.5873364  0.6755055  0.6755055 ]
pred one (1166,) [0.2067203  0.71491635 0.46780002 ... 0.6589664  0.72511554 0.9802269 ]
2022-06-16 19:20:40.010062: Epoch 5/150, Train: Loss = 14.1631, preLoss = 3.1480

pred one (2276,) [0.3742202  0.30694205 0.68391407 ... 0.7597552  0.7899405  0.54950213]
pred one (2092,) [0.6050235  0.36712334 0.64871097 ... 0.40853116 0.40853116 0.6151563 ]
pred one (2134,) [0.5450147  0.3252914  0.5450147  ... 0.47977662 0.62297326 0.36012012]
pred one (2066,) [0.69643503 0.62490785 0.62490785 ... 0.54706    0.45492688 0.54706   ]
pred one (2386,) [0.6840995  0.6840995  0.8665045  ... 0.4637934  0.27129963 0.3900123 ]
pred one (2296,) [0.63603246 0.74966234 0.84340763 ... 0.57661563 0.3804313  0.25061154]
pred one (2114,) [0.5861183  0.36110288 0.5861183  ... 0.664441   0.48545623 0.7612988 ]
pred one (2094,) [0.38658032 0.4195251  0.38658032 ... 1.0247896  0.8678174  0.9114624 ]
pred one (2186,) [0.53015554 0.53015554 0.40991738 ... 0.53956574 0.4550402  0.31133568]
pred one (2226,) [0.46408364 0.7904291  0.6348947  ... 0.4734547  0.5790179  0.5790179 ]
pred one (2220,) [0.53523624 0.66125035 0.66125035 ... 0.5779724  0.7275479  0.97709537]
pred one (2280,) [0.57220185 0.57220185 0.5946062  ... 0.22347727 0.397956   0.405362  ]
pred one (2252,) [0.67264813 0.4721514  0.42660415 ... 0.9107468  0.8688894  0.8805345 ]
pred one (2244,) [1.0039623  0.92567605 0.8996937  ... 0.5092294  0.5864756  0.6840529 ]
pred one (2176,) [0.56030655 0.51337916 0.8000691  ... 0.7590278  1.4614303  0.9951688 ]
pred one (2082,) [0.5330449  0.508594   0.48357752 ... 0.6205244  0.48659557 0.730863  ]
pred one (1908,) [0.67498696 0.67498696 0.6023938  ... 0.7261502  0.7261502  0.7261502 ]
pred one (2418,) [0.57447207 0.35760006 0.9959415  ... 0.34789705 0.7113533  0.34789705]
pred one (2064,) [0.77493167 0.77493167 0.77493167 ... 0.9783356  0.40071896 0.698244  ]
pred one (1252,) [0.6469542  0.79654676 0.55201524 ... 0.44578916 0.21862665 0.16360301]
2022-06-16 19:21:19.636052: Epoch 6/150, Train: Loss = 13.6654, preLoss = 2.6737
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0652 0.0652 0.12949343891890971 0.1929 0.20869545647724225 0.4756
2022-06-16 19:22:09.505074: Epoch 6/150, Test: HR = 0.3081, NDCG = 0.1665
2022-06-16 19:22:10.903864: Model Saved: yelp

pred one (2432,) [0.57084215 0.3770487  1.0555512  ... 0.61186016 0.950156   0.61186016]
pred one (2232,) [0.64924073 0.59755707 0.70844114 ... 0.2686655  0.6966158  0.53944695]
pred one (2156,) [0.7058952  0.7058952  0.64468336 ... 0.67650676 0.7398305  0.6609564 ]
pred one (2258,) [0.5810555  0.48952872 0.31230247 ... 0.8952577  0.46784517 0.46784517]
pred one (2206,) [0.4916318  0.52117646 0.6198015  ... 0.91753733 0.6027709  0.7181844 ]
pred one (2414,) [0.595485   0.595485   0.6588947  ... 0.43069193 0.48406437 0.47014672]
pred one (2048,) [0.6254498  0.6254498  0.6575048  ... 0.71059823 0.672831   0.9383268 ]
pred one (2206,) [0.5260277  0.5260277  0.5741845  ... 0.33057705 0.4804557  0.5555426 ]
pred one (2450,) [0.47974408 0.45634362 0.5018125  ... 0.6928388  0.9156177  0.51141673]
pred one (2244,) [0.45656765 0.57528746 0.404649   ... 0.8758298  0.7103373  0.9140012 ]
pred one (2354,) [0.46888047 0.72163457 0.6637898  ... 0.621967   0.7268261  0.76091486]
pred one (1988,) [0.6906682  0.6906682  0.41438746 ... 0.5596235  0.63304853 0.31771737]
pred one (2160,) [0.42986834 0.22428036 0.22428036 ... 0.8549347  0.7039102  0.68098795]
pred one (2282,) [0.76319754 0.89470583 0.76319754 ... 0.5221515  0.37690493 0.51108986]
pred one (2234,) [0.3569863  0.3569863  0.62131834 ... 0.42657092 0.5787769  0.28848922]
pred one (2372,) [0.555408   0.6980026  0.555408   ... 0.58643913 0.6094992  0.8134813 ]
pred one (1888,) [1.0001597  0.8302144  1.0001597  ... 0.49565965 0.8146725  0.6305896 ]
pred one (2230,) [0.65865326 0.7670907  0.65865326 ... 0.3160538  0.46260184 0.46260184]
pred one (2340,) [0.42081505 0.59850323 0.45480144 ... 0.345164   0.33092424 0.53095007]
pred one (1150,) [0.45485115 0.88409686 0.45485115 ... 0.82714546 1.0380709  0.96571255]
2022-06-16 19:22:50.943416: Epoch 7/150, Train: Loss = 12.8815, preLoss = 2.2583

pred one (2134,) [0.49140242 0.39762405 0.7007277  ... 0.75674874 0.6399964  0.79024816]
pred one (2246,) [0.53555024 0.79546165 0.4391665  ... 0.6218827  0.6218827  0.6680855 ]
pred one (2422,) [0.55686414 0.55686414 0.41120517 ... 1.3992962  1.3992962  0.967623  ]
pred one (2034,) [0.4460176  0.9212513  0.35446942 ... 0.77866936 0.786605   0.6246072 ]
pred one (2298,) [1.6476034  1.6767244  1.6767244  ... 0.46558812 0.53741086 0.53741086]
pred one (2366,) [0.5671577  0.5133442  0.5133442  ... 0.91596085 0.41455775 0.41455775]
pred one (2046,) [0.3603831  0.50202537 0.6262276  ... 0.15052328 0.40045518 0.40045518]
pred one (2050,) [0.27741256 0.27164263 0.48288888 ... 0.38209504 0.93514174 0.36250895]
pred one (2336,) [0.51864064 0.51864064 0.44365823 ... 0.4836702  0.44165254 0.44165254]
pred one (1948,) [0.21865216 0.232663   0.24837135 ... 1.1075343  0.3646004  0.3646004 ]
pred one (2476,) [0.07578549 0.22523262 0.11919455 ... 0.5687833  0.5687833  0.5911072 ]
pred one (2430,) [0.6587778  0.46122065 0.6587778  ... 0.6856384  0.7158193  0.67964536]
pred one (2174,) [0.45630002 0.6621841  0.58107376 ... 0.39602757 0.39602757 0.60375667]
pred one (2302,) [0.8700838  0.8700838  0.60416484 ... 0.9447742  1.0375559  0.8910754 ]
pred one (2130,) [0.8180293  0.68603176 0.9066307  ... 0.5352498  0.34785265 0.63412386]
pred one (2234,) [0.8478793  0.8723514  0.8478793  ... 0.68017906 0.54839134 0.54839134]
pred one (2144,) [0.5184667 0.6213306 0.6213306 ... 0.8373172 0.3610989 0.3031121]
pred one (2236,) [0.53061765 0.4707687  0.53061765 ... 0.81143606 0.53410643 0.81143606]
pred one (2472,) [0.34506205 0.35511172 0.35511172 ... 0.28350186 0.3202214  0.37531838]
pred one (1310,) [0.6354635  0.527203   0.43759844 ... 0.74919045 0.5362252  0.6667727 ]
2022-06-16 19:23:30.724604: Epoch 8/150, Train: Loss = 11.8310, preLoss = 1.8226

pred one (2276,) [0.3591001  0.5069573  0.3591001  ... 0.49196172 0.59480387 0.3436056 ]
pred one (2324,) [0.5196762  0.67195046 0.57679856 ... 0.5895684  0.67693996 0.67693996]
pred one (2230,) [0.70924145 0.6899384  0.7304599  ... 0.907037   0.98815525 0.9913609 ]
pred one (2398,) [0.7272444 0.7272444 0.7272444 ... 0.5319083 0.5022932 0.6643069]
pred one (2002,) [0.56200254 0.56200254 0.48669758 ... 0.72681165 0.63568115 0.63568115]
pred one (2496,) [0.7419479  0.7419479  0.663302   ... 0.43635094 0.7617151  0.732642  ]
pred one (2232,) [1.0383803  0.7555233  0.61173475 ... 0.76640797 0.54478747 0.54478747]
pred one (2260,) [0.5430892 0.489109  0.7120372 ... 0.6773513 0.736164  0.736164 ]
pred one (2058,) [1.0426031  1.0426031  0.9566647  ... 0.48924622 0.4463271  0.5979502 ]
pred one (2120,) [0.7636552  0.6549594  0.65425706 ... 0.8136159  0.7435732  0.9911197 ]
pred one (2130,) [0.41899204 0.5161501  0.5456199  ... 0.78487664 0.78487664 0.7550043 ]
pred one (2574,) [0.6768777  0.56254196 0.6768777  ... 0.6174344  0.39239225 0.43002737]
pred one (2128,) [0.5998041  0.59095055 0.3450033  ... 0.46766442 0.51416075 0.5764928 ]
pred one (2090,) [0.72235984 0.46756533 0.46621907 ... 0.727131   0.6919719  0.61788297]
pred one (2356,) [0.46935698 1.017861   0.6358295  ... 0.8513604  0.5762702  0.54056954]
pred one (2210,) [0.5228748  0.5158391  0.5158391  ... 0.5019145  0.37199125 0.37199125]
pred one (2148,) [0.9468039  0.9468039  0.41557714 ... 0.13129808 0.17577952 0.17577952]
pred one (1978,) [0.9024944  0.90370023 0.90370023 ... 0.62669003 0.43177462 0.44570607]
pred one (2290,) [0.378362   0.47514302 0.43735778 ... 0.6395447  0.75852513 0.5135776 ]
pred one (1114,) [0.446476   0.5854711  0.40254214 ... 0.5266887  0.70937854 0.70937854]
2022-06-16 19:24:11.089699: Epoch 9/150, Train: Loss = 10.8119, preLoss = 1.5846
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0608 0.0608 0.14171618184808155 0.2255 0.2255295076480893 0.5243
2022-06-16 19:24:59.988073: Epoch 9/150, Test: HR = 0.3504, NDCG = 0.1818
2022-06-16 19:25:01.278094: Model Saved: yelp

pred one (2288,) [0.4288301  0.43553635 0.65420914 ... 0.5014775  0.40023005 0.34982154]
pred one (2310,) [0.6098284  0.4209144  0.5088165  ... 0.8412393  0.7733538  0.38478315]
pred one (2124,) [0.57518464 0.4440174  0.45588607 ... 0.42495868 0.592061   0.592061  ]
pred one (2078,) [0.52486163 0.33591968 0.33591968 ... 0.52428925 0.73557615 0.46848226]
pred one (2396,) [0.5303695  0.6066852  0.47531724 ... 0.43145737 0.43145737 0.43145737]
pred one (2314,) [0.72493255 0.58260715 0.76149875 ... 0.28063375 0.32308495 0.60246706]
pred one (2110,) [0.39442196 0.60659415 0.2146185  ... 0.60374326 0.60374326 0.65302414]
pred one (1886,) [0.6405071  0.78289133 0.6405071  ... 0.48733136 0.48733136 0.85209274]
pred one (2070,) [0.6334282  0.6334282  0.71012366 ... 0.725329   1.1013278  0.6227347 ]
pred one (2244,) [0.22552845 0.6113901  0.27464324 ... 0.3757019  0.55167115 0.55167115]
pred one (2312,) [0.6703595  0.3538933  0.42427385 ... 0.53199667 0.43543988 0.53199667]
pred one (2254,) [0.6899148  0.15480357 0.39625806 ... 0.6382917  0.54620767 0.6382917 ]
pred one (2222,) [0.86447304 0.86447304 0.76521397 ... 0.37215978 0.50956905 0.37215978]
pred one (2070,) [0.45950162 0.42736155 0.45950162 ... 0.54090554 0.55291784 0.42028663]
pred one (2268,) [0.4290721  0.47519296 0.52306604 ... 0.444985   0.56802213 0.61546254]
pred one (2248,) [0.14340466 0.14340466 0.14340466 ... 0.422173   0.45859003 0.45859003]
pred one (1930,) [0.4539345  0.776067   0.62416726 ... 0.7321273  0.48854905 0.5774605 ]
pred one (2576,) [0.34510717 0.34510717 0.5383896  ... 0.6483079  0.32832497 0.25120127]
pred one (2112,) [0.29945728 0.3891073  0.20469081 ... 1.0891662  0.725996   1.0396057 ]
pred one (1104,) [0.84175265 0.7775847  0.7786722  ... 0.58219516 0.648276   0.58163935]
2022-06-16 19:25:41.082430: Epoch 10/150, Train: Loss = 9.7117, preLoss = 1.3124

pred one (2294,) [0.7262763  0.7958524  0.30293477 ... 0.770704   0.770704   0.770704  ]
pred one (2286,) [0.6111973  0.38385564 0.44718492 ... 0.31018686 0.43433458 0.41018045]
pred one (2352,) [0.46608764 0.46608764 0.30292037 ... 0.6165353  0.30316198 0.6165353 ]
pred one (2162,) [0.42416507 0.42416507 0.42939532 ... 0.18404785 0.44058996 0.44058996]
pred one (2060,) [0.6829847  0.6829847  0.6829847  ... 0.837803   0.58357954 0.58357954]
pred one (2194,) [0.32758427 0.4158499  0.54917896 ... 0.43997356 0.26676333 0.14887878]
pred one (2114,) [0.49346897 0.5550364  0.49346897 ... 0.41685766 0.7698933  0.5220058 ]
pred one (2342,) [0.63397837 0.63397837 0.5268355  ... 0.43550587 0.36152864 0.24850357]
pred one (2194,) [0.37619147 0.27655083 0.5763396  ... 0.4736815  0.4782843  0.6345033 ]
pred one (2274,) [0.4353398  0.4353398  0.47991902 ... 0.65973645 0.41227043 0.44694927]
pred one (2142,) [0.6789068  0.6456999  0.39218298 ... 0.5993266  0.52796304 0.6404058 ]
pred one (2362,) [0.6608442  0.7720678  0.45474893 ... 0.62289107 0.3092966  0.3092966 ]
pred one (2370,) [0.5805736  0.59252053 0.68509614 ... 0.49589252 0.47099766 0.46020263]
pred one (2334,) [0.18057828 0.44369632 1.1138341  ... 0.65989524 0.65989524 0.5984137 ]
pred one (2190,) [0.4725702  0.2998473  0.2998473  ... 0.70898414 0.79687    0.49816048]
pred one (2482,) [0.6620577  0.5049413  0.587666   ... 0.39111203 0.49506134 0.39111203]
pred one (2004,) [0.5023745  0.5023745  0.5023745  ... 0.36625546 0.29346406 0.29346406]
pred one (2446,) [0.4025767  0.54312    0.36544135 ... 0.4403028  0.568775   0.568775  ]
pred one (1830,) [0.6280527  0.56674695 0.47741205 ... 0.5671425  0.08934504 0.3433808 ]
pred one (1178,) [0.62891567 0.6663292  0.7070531  ... 0.3477522  0.23239172 0.34587568]
2022-06-16 19:26:21.129912: Epoch 11/150, Train: Loss = 8.7402, preLoss = 1.1520

pred one (2284,) [0.6823197  0.42415226 0.6823197  ... 0.3630461  0.3630461  0.4401849 ]
pred one (2076,) [0.461886   0.47115564 0.461886   ... 0.3190878  0.3177911  0.5609362 ]
pred one (2580,) [0.6324612  0.6931766  0.65942746 ... 0.35438138 0.6189872  0.4378575 ]
pred one (2456,) [0.5903598  0.6143644  0.5809195  ... 0.3568754  0.5606665  0.41854936]
pred one (2200,) [0.4383941  0.46845013 0.6172266  ... 0.08472158 0.69199103 0.43865687]
pred one (2312,) [0.3784234 0.3784234 0.5579145 ... 0.522869  0.522869  0.522869 ]
pred one (2320,) [0.4109949  0.26769653 0.3654318  ... 0.5970082  0.38718587 0.53685534]
pred one (2304,) [0.41948137 0.5076584  0.41948137 ... 0.41368744 0.25189012 0.32992393]
pred one (1940,) [0.5592123  0.18335244 0.27601206 ... 0.38110203 0.4644003  0.47512782]
pred one (2322,) [0.31756997 0.31756997 0.55167544 ... 0.62077373 0.6507546  0.40538943]
pred one (2064,) [0.3006963  0.34703723 0.4622609  ... 0.5638799  0.5638799  0.6346947 ]
pred one (2440,) [0.7415599  0.5675182  0.56492025 ... 0.6973076  0.60088575 0.65163577]
pred one (2296,) [0.69458723 0.6258001  0.5603045  ... 0.2530126  0.21332312 0.37436524]
pred one (2086,) [0.83864754 0.6069766  0.59997046 ... 0.46546918 0.52403927 0.52403927]
pred one (2370,) [0.60079014 0.58054733 0.583274   ... 0.29143947 0.29143947 0.41333485]
pred one (2104,) [0.4434847  0.3126635  0.4434847  ... 0.2676348  0.71765834 0.71765834]
pred one (2346,) [0.30037984 0.1101224  0.49412477 ... 0.39086106 0.23988217 0.2080102 ]
pred one (2146,) [0.4465735  0.24708363 0.3891868  ... 0.48941207 0.24574588 0.33632863]
pred one (2540,) [0.5365161  0.75341123 0.63408685 ... 0.52125335 0.5154942  0.35622722]
pred one (1194,) [0.6257744  0.67852855 0.70447683 ... 0.7196177  0.34183496 0.38432115]
2022-06-16 19:27:01.206513: Epoch 12/150, Train: Loss = 7.7592, preLoss = 0.9396
^CTraceback (most recent call last):/20: hit10 = 179, ndcg10 = 908
  File "main.py", line 25, in <module>
    recom.run()
  File "/root/CLSR/model.py", line 53, in run
    reses = self.testEpoch()
  File "/root/CLSR/model.py", line 457, in testEpoch
    suLocs, siLocs = self.sampleSslBatch(batIds, self.handler.subadj)
  File "/root/CLSR/model.py", line 337, in sampleSslBatch
    posset = np.reshape(np.argwhere(temLabel[k][i]!=0), [-1])
KeyboardInterrupt
root@container-327e11a8ac-4a1523bb:~/CLSR#
Remote side unexpectedly closed network connection

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

Session stopped
    - Press <return> to exit tab
    - Press R to restart session
    - Press S to save terminal output to file


Access denied
root@region-3.autodl.com's password:
     ┌────────────────────────────────────────────────────────────────────┐
     │                        • MobaXterm 20.3 •                          │
     │            (SSH client, X-server and networking tools)             │
     │                                                                    │
     │ ➤ SSH session to root@region-3.autodl.com                          │
     │   • SSH compression : ✔                                            │
     │   • SSH-browser     : ✔                                            │
     │   • X11-forwarding  : ✘  (disabled or not supported by server)     │
     │   • DISPLAY         : 192.168.1.107:0.0                            │
     │                                                                    │
     │ ➤ For more info, ctrl+click on help or visit our website           │
     └────────────────────────────────────────────────────────────────────┘

Welcome to Ubuntu 18.04.6 LTS (GNU/Linux 5.4.0-96-generic x86_64)

 * Documentation:  https://help.ubuntu.com
 * Management:     https://landscape.canonical.com
 * Support:        https://ubuntu.com/advantage
This system has been minimized by removing packages and content that are
not required on a system that users do not log into.

To restore this content, you can run the 'unminimize' command.
Last login: Thu Jun 16 10:56:43 2022 from 127.0.0.1
+--------------------------------------------------AutoDL--------------------------------------------------------+
目录说明:
╔═════════════════╦══════╦════╦═════════════════════════════════════════════════════════════════════════╗
║目录             ║名称  ║速度║说明                                                                     ║
╠═════════════════╬══════╬════╬═════════════════════════════════════════════════════════════════════════╣
║/                ║系统盘║快  ║实例关机数据不会丢失，可存放代码等。会随保存镜像一起保存。               ║
║/root/autodl-tmp ║数据盘║快  ║实例关机数据不会丢失，可存放读写IO要求高的数据。但不会随保存镜像一起保存 ║
╚═════════════════╩══════╩════╩═════════════════════════════════════════════════════════════════════════╝
CPU ：7 核心
内存：16 GB
GPU ：NVIDIA TITAN Xp, 1
存储：
  系统盘/               ：15% 2.9G/20G
  数据盘/root/autodl-tmp：0% 0/50G
+----------------------------------------------------------------------------------------------------------------+
*注意:
1.系统盘较小请将大的数据存放于数据盘或网盘中，重置系统时数据盘和网盘中的数据不受影响
2.清理系统盘请参考：https://www.autodl.com/docs/qa/
root@container-327e11a8ac-4a1523bb:~# cd ./CLSR
root@container-327e11a8ac-4a1523bb:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp  0.1 --ssl_reg 1e-6 --s         ave_path yelp --batch 512 --epoch 150
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2022-06-16 11:06:42.954602: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-16 11:06:43.039856: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:241: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("SparseTensor/values:0", shape=(33516         8,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be re         moved in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("SparseTensor/values:0", shape=(33516         8,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("SparseTensor_1/values:0", shape=(1         77378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("SparseTensor_1/values:0", shape=(1         77378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("SparseTensor_2/values:0", shape=(1         56718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("SparseTensor_2/values:0", shape=(1         56718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("SparseTensor_3/values:0", shape=(1         56674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("SparseTensor_3/values:0", shape=(1         56674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("SparseTensor_4/values:0", shape=(1         59442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("SparseTensor_4/values:0", shape=(1         59442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("SparseTensor_5/values:0", shape=(1         55622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("SparseTensor_5/values:0", shape=(1         55622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("SparseTensor_6/values:0", shape=(1         41690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("SparseTensor_6/values:0", shape=(1         41690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("SparseTensor_7/values:0", shape=(85         108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("SparseTensor_7/values:0", shape=(85         108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a fut         ure version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (         from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f80ec6e80d0>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8         0ec6e80d0>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f81e0756f90>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8         1e0756f90>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_d         ecay instead.

WARNING:tensorflow:From /root/CLSR/model.py:287: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer ins         tead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wra         pper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-16 11:07:06.585943: Model Prepared
2022-06-16 11:07:09.364054: Variables Inited
2022-06-16 11:07:59.916336: Epoch 0/150, Train: Loss = 5.2115, preLoss = 2.1704
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0205 0.0205 0.050356755708787816 0.0819 0.10344458657585726 0.2751
2022-06-16 11:08:48.885516: Epoch 0/150, Test: HR = 0.1505, NDCG = 0.0723
WARNING:tensorflow:From /root/CLSR/model.py:524: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

2022-06-16 11:08:56.473599: Model Saved: yelp

2022-06-16 11:09:37.073931: Epoch 1/150, Train: Loss = 9.6641, preLoss = 3.7048

2022-06-16 11:10:17.828999: Epoch 2/150, Train: Loss = 11.7537, preLoss = 3.8165

2022-06-16 11:10:58.976842: Epoch 3/150, Train: Loss = 13.2733, preLoss = 3.8396
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0652 0.0652 0.1323768674309008 0.1968 0.207555015933278 0.4651
2022-06-16 11:11:49.221148: Epoch 3/150, Test: HR = 0.3075, NDCG = 0.1679
2022-06-16 11:11:50.958497: Model Saved: yelp

2022-06-16 11:12:31.968628: Epoch 4/150, Train: Loss = 14.1155, preLoss = 3.7058

2022-06-16 11:13:12.766310: Epoch 5/150, Train: Loss = 14.1669, preLoss = 3.2282

2022-06-16 11:13:54.211148: Epoch 6/150, Train: Loss = 13.6248, preLoss = 2.6303
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0633 0.0633 0.13352514707378965 0.2041 0.21058514266890266 0.4793
2022-06-16 11:14:44.497439: Epoch 6/150, Test: HR = 0.3170, NDCG = 0.1698
2022-06-16 11:14:46.707902: Model Saved: yelp

2022-06-16 11:15:27.481482: Epoch 7/150, Train: Loss = 12.7945, preLoss = 2.1813

2022-06-16 11:16:07.866302: Epoch 8/150, Train: Loss = 11.8044, preLoss = 1.8386

2022-06-16 11:16:48.711259: Epoch 9/150, Train: Loss = 10.7591, preLoss = 1.5587
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0731 0.0731 0.15279175821032515 0.2313 0.23276616992154478 0.5149
2022-06-16 11:17:39.050889: Epoch 9/150, Test: HR = 0.3535, NDCG = 0.1923
2022-06-16 11:17:41.006861: Model Saved: yelp

2022-06-16 11:18:21.838909: Epoch 10/150, Train: Loss = 9.7108, preLoss = 1.3247

2022-06-16 11:19:02.400603: Epoch 11/150, Train: Loss = 8.6996, preLoss = 1.1174

2022-06-16 11:19:43.150056: Epoch 12/150, Train: Loss = 7.7320, preLoss = 0.9308
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0807 0.0807 0.1628204554632227 0.2442 0.24643169580607782 0.5401
2022-06-16 11:20:32.713902: Epoch 12/150, Test: HR = 0.3741, NDCG = 0.2046
2022-06-16 11:20:34.457389: Model Saved: yelp

2022-06-16 11:21:14.833664: Epoch 13/150, Train: Loss = 6.8686, preLoss = 0.8035

2022-06-16 11:21:54.736515: Epoch 14/150, Train: Loss = 6.0378, preLoss = 0.6474

2022-06-16 11:22:34.999324: Epoch 15/150, Train: Loss = 5.3410, preLoss = 0.5721
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0822 0.0822 0.17347176725770222 0.2611 0.25756288151455686 0.5601
2022-06-16 11:23:23.080502: Epoch 15/150, Test: HR = 0.3865, NDCG = 0.2139
2022-06-16 11:23:24.524996: Model Saved: yelp

2022-06-16 11:24:04.577097: Epoch 16/150, Train: Loss = 4.7088, preLoss = 0.4898

2022-06-16 11:24:44.887012: Epoch 17/150, Train: Loss = 4.1573, preLoss = 0.4267

2022-06-16 11:25:24.968266: Epoch 18/150, Train: Loss = 3.6466, preLoss = 0.3500
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0873 0.0873 0.17878654674143157 0.2705 0.2661817615099346 0.5788
2022-06-16 11:26:13.217351: Epoch 18/150, Test: HR = 0.4092, NDCG = 0.2235
2022-06-16 11:26:16.129004: Model Saved: yelp

2022-06-16 11:26:56.349463: Epoch 19/150, Train: Loss = 3.2126, preLoss = 0.3044

2022-06-16 11:27:36.163543: Epoch 20/150, Train: Loss = 2.8428, preLoss = 0.2730

2022-06-16 11:28:16.474315: Epoch 21/150, Train: Loss = 2.5136, preLoss = 0.2407
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0918 0.0918 0.18918830448137214 0.2854 0.2757842113122227 0.5898
2022-06-16 11:29:04.253216: Epoch 21/150, Test: HR = 0.4246, NDCG = 0.2341
2022-06-16 11:29:06.564886: Model Saved: yelp

2022-06-16 11:29:46.404403: Epoch 22/150, Train: Loss = 2.2213, preLoss = 0.2038

2022-06-16 11:30:26.107848: Epoch 23/150, Train: Loss = 1.9823, preLoss = 0.1878

2022-06-16 11:31:06.660923: Epoch 24/150, Train: Loss = 1.7667, preLoss = 0.1668
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.103 0.103 0.2004033252244601 0.2962 0.28619931211654515 0.5998
2022-06-16 11:31:54.858135: Epoch 24/150, Test: HR = 0.4278, NDCG = 0.2429
2022-06-16 11:31:56.786725: Model Saved: yelp

2022-06-16 11:32:36.842250: Epoch 25/150, Train: Loss = 1.5880, preLoss = 0.1564

2022-06-16 11:33:16.830506: Epoch 26/150, Train: Loss = 1.4275, preLoss = 0.1402

2022-06-16 11:33:57.690012: Epoch 27/150, Train: Loss = 1.2873, preLoss = 0.1271
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1099 0.1099 0.2099717999462635 0.3078 0.29878112759859016 0.6202
2022-06-16 11:34:46.095186: Epoch 27/150, Test: HR = 0.4512, NDCG = 0.2563
2022-06-16 11:34:48.318952: Model Saved: yelp

2022-06-16 11:35:29.169834: Epoch 28/150, Train: Loss = 1.1674, preLoss = 0.1198

2022-06-16 11:36:09.849388: Epoch 29/150, Train: Loss = 1.0596, preLoss = 0.1096

2022-06-16 11:36:49.932463: Epoch 30/150, Train: Loss = 0.9710, preLoss = 0.1042
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1241 0.1241 0.22727469097418002 0.3283 0.3136300806553808 0.6311
2022-06-16 11:37:38.061035: Epoch 30/150, Test: HR = 0.4667, NDCG = 0.2721
2022-06-16 11:37:39.858436: Model Saved: yelp

2022-06-16 11:38:20.435843: Epoch 31/150, Train: Loss = 0.8923, preLoss = 0.0987

2022-06-16 11:39:00.796317: Epoch 32/150, Train: Loss = 0.8236, preLoss = 0.0940

2022-06-16 11:39:40.698269: Epoch 33/150, Train: Loss = 0.7630, preLoss = 0.0898
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1265 0.1265 0.23492225633070962 0.3417 0.3202210008625575 0.6412
2022-06-16 11:40:29.021024: Epoch 33/150, Test: HR = 0.4812, NDCG = 0.2800
2022-06-16 11:40:30.767171: Model Saved: yelp

2022-06-16 11:41:10.976331: Epoch 34/150, Train: Loss = 0.7076, preLoss = 0.0840

2022-06-16 11:41:50.731776: Epoch 35/150, Train: Loss = 0.6626, preLoss = 0.0835

2022-06-16 11:42:31.107013: Epoch 36/150, Train: Loss = 0.6209, preLoss = 0.0816
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1495 0.1495 0.2524877708764763 0.3533 0.33696478360985577 0.649
2022-06-16 11:43:18.163259: Epoch 36/150, Test: HR = 0.4908, NDCG = 0.2971
2022-06-16 11:43:19.748452: Model Saved: yelp

2022-06-16 11:43:59.019927: Epoch 37/150, Train: Loss = 0.5840, preLoss = 0.0795

2022-06-16 11:44:38.672405: Epoch 38/150, Train: Loss = 0.5526, preLoss = 0.0785

2022-06-16 11:45:17.908555: Epoch 39/150, Train: Loss = 0.5223, preLoss = 0.0754
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.138 0.138 0.2474626145744438 0.3535 0.33479027489605484 0.6592
2022-06-16 11:46:05.437911: Epoch 39/150, Test: HR = 0.4985, NDCG = 0.2943
2022-06-16 11:46:07.086751: Model Saved: yelp

2022-06-16 11:46:46.852828: Epoch 40/150, Train: Loss = 0.4974, preLoss = 0.0753

2022-06-16 11:47:25.981021: Epoch 41/150, Train: Loss = 0.4713, preLoss = 0.0713

2022-06-16 11:48:05.224355: Epoch 42/150, Train: Loss = 0.4540, preLoss = 0.0741
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1671 0.1671 0.26881556513514165 0.3692 0.35638294077277294 0.6761
2022-06-16 11:48:52.469615: Epoch 42/150, Test: HR = 0.5117, NDCG = 0.3149
2022-06-16 11:48:54.153355: Model Saved: yelp

2022-06-16 11:49:33.277512: Epoch 43/150, Train: Loss = 0.4344, preLoss = 0.0727

2022-06-16 11:50:12.367162: Epoch 44/150, Train: Loss = 0.4157, preLoss = 0.0699

2022-06-16 11:50:51.515178: Epoch 45/150, Train: Loss = 0.3994, preLoss = 0.0683
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1638 0.1638 0.2675737481642501 0.3648 0.35722707882132604 0.6761
2022-06-16 11:51:38.523143: Epoch 45/150, Test: HR = 0.5152, NDCG = 0.3167
2022-06-16 11:51:40.156787: Model Saved: yelp

2022-06-16 11:52:19.425132: Epoch 46/150, Train: Loss = 0.3876, preLoss = 0.0698

2022-06-16 11:52:58.989418: Epoch 47/150, Train: Loss = 0.3730, preLoss = 0.0675

2022-06-16 11:53:38.583620: Epoch 48/150, Train: Loss = 0.3601, preLoss = 0.0661
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1544 0.1544 0.26629576732838117 0.3691 0.3589630592048899 0.6912
2022-06-16 11:54:25.582156: Epoch 48/150, Test: HR = 0.5261, NDCG = 0.3173
2022-06-16 11:54:27.267953: Model Saved: yelp

2022-06-16 11:55:06.329622: Epoch 49/150, Train: Loss = 0.3486, preLoss = 0.0653

2022-06-16 11:55:45.781834: Epoch 50/150, Train: Loss = 0.3388, preLoss = 0.0650

2022-06-16 11:56:25.195934: Epoch 51/150, Train: Loss = 0.3296, preLoss = 0.0647
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1536 0.1536 0.266643904565426 0.3694 0.3626662984677216 0.7019
2022-06-16 11:57:12.532144: Epoch 51/150, Test: HR = 0.5299, NDCG = 0.3191
2022-06-16 11:57:14.166231: Model Saved: yelp

2022-06-16 11:57:53.079604: Epoch 52/150, Train: Loss = 0.3209, preLoss = 0.0644

2022-06-16 11:58:32.276538: Epoch 53/150, Train: Loss = 0.3125, preLoss = 0.0636

2022-06-16 11:59:11.714457: Epoch 54/150, Train: Loss = 0.3053, preLoss = 0.0634
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1793 0.1793 0.28576705333004504 0.3854 0.37904872603425155 0.709
2022-06-16 11:59:59.336393: Epoch 54/150, Test: HR = 0.5442, NDCG = 0.3376
2022-06-16 12:00:01.101799: Model Saved: yelp

2022-06-16 12:00:40.343358: Epoch 55/150, Train: Loss = 0.2988, preLoss = 0.0633

2022-06-16 12:01:19.605211: Epoch 56/150, Train: Loss = 0.2935, preLoss = 0.0639

2022-06-16 12:01:59.103067: Epoch 57/150, Train: Loss = 0.2876, preLoss = 0.0635
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1996 0.1996 0.29754603345731945 0.3958 0.38881846327960334 0.7141
2022-06-16 12:02:46.496447: Epoch 57/150, Test: HR = 0.5485, NDCG = 0.3471
2022-06-16 12:02:48.328263: Model Saved: yelp

2022-06-16 12:03:27.062924: Epoch 58/150, Train: Loss = 0.2808, preLoss = 0.0619

2022-06-16 12:04:06.611536: Epoch 59/150, Train: Loss = 0.2767, preLoss = 0.0628

2022-06-16 12:04:45.599498: Epoch 60/150, Train: Loss = 0.2700, preLoss = 0.0609
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.178 0.178 0.29559788736266535 0.4065 0.3867917655915969 0.7228
2022-06-16 12:05:33.736661: Epoch 60/150, Test: HR = 0.5626, NDCG = 0.3464
2022-06-16 12:05:35.834151: Model Saved: yelp

2022-06-16 12:06:15.064240: Epoch 61/150, Train: Loss = 0.2660, preLoss = 0.0612

2022-06-16 12:06:54.530150: Epoch 62/150, Train: Loss = 0.2604, preLoss = 0.0598

2022-06-16 12:07:33.921072: Epoch 63/150, Train: Loss = 0.2573, preLoss = 0.0605
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1636 0.1636 0.2945566614165263 0.4163 0.38406423868974043 0.7262
2022-06-16 12:08:20.940937: Epoch 63/150, Test: HR = 0.5695, NDCG = 0.3445
2022-06-16 12:08:22.865004: Model Saved: yelp

2022-06-16 12:09:02.303248: Epoch 64/150, Train: Loss = 0.2523, preLoss = 0.0591

2022-06-16 12:09:42.433717: Epoch 65/150, Train: Loss = 0.2492, preLoss = 0.0595

2022-06-16 12:10:21.490543: Epoch 66/150, Train: Loss = 0.2463, preLoss = 0.0598
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1617 0.1617 0.29284851758230696 0.4158 0.3844614410035642 0.7323
2022-06-16 12:11:08.949364: Epoch 66/150, Test: HR = 0.5740, NDCG = 0.3446
2022-06-16 12:11:10.700840: Model Saved: yelp

2022-06-16 12:11:50.490917: Epoch 67/150, Train: Loss = 0.2426, preLoss = 0.0592

2022-06-16 12:12:29.929530: Epoch 68/150, Train: Loss = 0.2381, preLoss = 0.0574

2022-06-16 12:13:09.099499: Epoch 69/150, Train: Loss = 0.2366, preLoss = 0.0587
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1597 0.1597 0.2919479234522154 0.4154 0.3845108351327752 0.7345
2022-06-16 12:13:56.391932: Epoch 69/150, Test: HR = 0.5754, NDCG = 0.3444
2022-06-16 12:13:57.942370: Model Saved: yelp

2022-06-16 12:14:37.295700: Epoch 70/150, Train: Loss = 0.2329, preLoss = 0.0575

2022-06-16 12:15:16.485756: Epoch 71/150, Train: Loss = 0.2308, preLoss = 0.0579

2022-06-16 12:15:55.801769: Epoch 72/150, Train: Loss = 0.2279, preLoss = 0.0573
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1728 0.1728 0.29802062728541406 0.415 0.3913519760817861 0.7368
2022-06-16 12:16:42.494552: Epoch 72/150, Test: HR = 0.5766, NDCG = 0.3510
2022-06-16 12:16:44.196300: Model Saved: yelp

2022-06-16 12:17:23.790232: Epoch 73/150, Train: Loss = 0.2262, preLoss = 0.0578

2022-06-16 12:18:02.764618: Epoch 74/150, Train: Loss = 0.2235, preLoss = 0.0571

2022-06-16 12:18:41.896315: Epoch 75/150, Train: Loss = 0.2215, preLoss = 0.0571
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1715 0.1715 0.2987511833496506 0.4169 0.39368817526620753 0.7445
2022-06-16 12:19:29.072540: Epoch 75/150, Test: HR = 0.5800, NDCG = 0.3522
2022-06-16 12:19:30.681892: Model Saved: yelp

2022-06-16 12:20:09.782374: Epoch 76/150, Train: Loss = 0.2188, preLoss = 0.0563

2022-06-16 12:20:48.510305: Epoch 77/150, Train: Loss = 0.2176, preLoss = 0.0568

2022-06-16 12:21:27.674904: Epoch 78/150, Train: Loss = 0.2150, preLoss = 0.0559
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1764 0.1764 0.30197300496399904 0.4195 0.3966808662096745 0.7463
2022-06-16 12:22:14.104808: Epoch 78/150, Test: HR = 0.5818, NDCG = 0.3551
2022-06-16 12:22:15.758369: Model Saved: yelp

2022-06-16 12:22:54.939412: Epoch 79/150, Train: Loss = 0.2130, preLoss = 0.0556

2022-06-16 12:23:34.098578: Epoch 80/150, Train: Loss = 0.2122, preLoss = 0.0563

2022-06-16 12:24:13.611379: Epoch 81/150, Train: Loss = 0.2097, preLoss = 0.0552
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1803 0.1803 0.31064031819151233 0.4314 0.4025729475794091 0.7494
2022-06-16 12:25:00.745016: Epoch 81/150, Test: HR = 0.5890, NDCG = 0.3621
2022-06-16 12:25:02.276833: Model Saved: yelp

2022-06-16 12:25:41.246983: Epoch 82/150, Train: Loss = 0.2082, preLoss = 0.0552

2022-06-16 12:26:20.228877: Epoch 83/150, Train: Loss = 0.2066, preLoss = 0.0548

2022-06-16 12:26:59.323529: Epoch 84/150, Train: Loss = 0.2051, preLoss = 0.0546
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1896 0.1896 0.31507784393688365 0.433 0.4070925883125971 0.751
2022-06-16 12:27:46.808281: Epoch 84/150, Test: HR = 0.5899, NDCG = 0.3664
2022-06-16 12:27:48.487298: Model Saved: yelp

2022-06-16 12:28:27.698498: Epoch 85/150, Train: Loss = 0.2030, preLoss = 0.0537

2022-06-16 12:29:06.538009: Epoch 86/150, Train: Loss = 0.2022, preLoss = 0.0540

2022-06-16 12:29:45.385307: Epoch 87/150, Train: Loss = 0.2017, preLoss = 0.0547
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1876 0.1876 0.31380059025656 0.4315 0.4058434555233349 0.7485
2022-06-16 12:30:34.325327: Epoch 87/150, Test: HR = 0.5898, NDCG = 0.3658
2022-06-16 12:30:36.191668: Model Saved: yelp

2022-06-16 12:31:15.932419: Epoch 88/150, Train: Loss = 0.2008, preLoss = 0.0548

2022-06-16 12:31:54.717339: Epoch 89/150, Train: Loss = 0.1989, preLoss = 0.0539

2022-06-16 12:32:34.094753: Epoch 90/150, Train: Loss = 0.1980, preLoss = 0.0540
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1947 0.1947 0.31568743468500177 0.4287 0.4093742442898854 0.7505
2022-06-16 12:33:21.899270: Epoch 90/150, Test: HR = 0.5899, NDCG = 0.3689
2022-06-16 12:33:23.568029: Model Saved: yelp

2022-06-16 12:34:03.316115: Epoch 91/150, Train: Loss = 0.1966, preLoss = 0.0535

2022-06-16 12:34:43.477958: Epoch 92/150, Train: Loss = 0.1953, preLoss = 0.0531

2022-06-16 12:35:23.389402: Epoch 93/150, Train: Loss = 0.1947, preLoss = 0.0534
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1941 0.1941 0.31621592283676725 0.4299 0.4089105714933459 0.7486
2022-06-16 12:36:11.042711: Epoch 93/150, Test: HR = 0.5913, NDCG = 0.3693
2022-06-16 12:36:12.820232: Model Saved: yelp

2022-06-16 12:36:52.648008: Epoch 94/150, Train: Loss = 0.1930, preLoss = 0.0526

2022-06-16 12:37:32.604199: Epoch 95/150, Train: Loss = 0.1924, preLoss = 0.0527

2022-06-16 12:38:12.427882: Epoch 96/150, Train: Loss = 0.1926, preLoss = 0.0537
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2028 0.2028 0.3220067456061247 0.4336 0.4136980303231484 0.7489
2022-06-16 12:39:00.018048: Epoch 96/150, Test: HR = 0.5923, NDCG = 0.3742
2022-06-16 12:39:01.833258: Model Saved: yelp

2022-06-16 12:39:41.703437: Epoch 97/150, Train: Loss = 0.1916, preLoss = 0.0534

2022-06-16 12:40:21.360732: Epoch 98/150, Train: Loss = 0.1893, preLoss = 0.0518

2022-06-16 12:41:01.325203: Epoch 99/150, Train: Loss = 0.1897, preLoss = 0.0530
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2012 0.2012 0.3221639073529012 0.4363 0.41359701559749046 0.751
2022-06-16 12:41:49.822006: Epoch 99/150, Test: HR = 0.5934, NDCG = 0.3739
2022-06-16 12:41:51.665044: Model Saved: yelp

2022-06-16 12:42:31.735438: Epoch 100/150, Train: Loss = 0.1883, preLoss = 0.0522

2022-06-16 12:43:11.621633: Epoch 101/150, Train: Loss = 0.1878, preLoss = 0.0522

2022-06-16 12:43:51.666455: Epoch 102/150, Train: Loss = 0.1877, preLoss = 0.0528
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2061 0.2061 0.3247980242184843 0.4373 0.41689430656348003 0.7544
2022-06-16 12:44:39.592019: Epoch 102/150, Test: HR = 0.5965, NDCG = 0.3772
2022-06-16 12:44:41.461444: Model Saved: yelp

2022-06-16 12:45:21.318915: Epoch 103/150, Train: Loss = 0.1869, preLoss = 0.0524

2022-06-16 12:46:00.997521: Epoch 104/150, Train: Loss = 0.1858, preLoss = 0.0520

2022-06-16 12:46:41.550092: Epoch 105/150, Train: Loss = 0.1852, preLoss = 0.0518
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2077 0.2077 0.3251048605919633 0.4369 0.417442388614208 0.7546
2022-06-16 12:47:29.149904: Epoch 105/150, Test: HR = 0.5963, NDCG = 0.3775
2022-06-16 12:47:30.954164: Model Saved: yelp

2022-06-16 12:48:10.516481: Epoch 106/150, Train: Loss = 0.1852, preLoss = 0.0524

2022-06-16 12:48:50.093429: Epoch 107/150, Train: Loss = 0.1835, preLoss = 0.0511

2022-06-16 12:49:29.735671: Epoch 108/150, Train: Loss = 0.1832, preLoss = 0.0514
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2057 0.2057 0.3235864453138011 0.4377 0.4155143367898848 0.7545
2022-06-16 12:50:17.404293: Epoch 108/150, Test: HR = 0.5965, NDCG = 0.3757
2022-06-16 12:50:19.303717: Model Saved: yelp

2022-06-16 12:50:58.756248: Epoch 109/150, Train: Loss = 0.1828, preLoss = 0.0513

2022-06-16 12:51:38.556629: Epoch 110/150, Train: Loss = 0.1820, preLoss = 0.0510

2022-06-16 12:52:18.150871: Epoch 111/150, Train: Loss = 0.1821, preLoss = 0.0515
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2054 0.2054 0.3235565507146789 0.4364 0.41606009754679874 0.7543
2022-06-16 12:53:05.519987: Epoch 111/150, Test: HR = 0.5962, NDCG = 0.3762
2022-06-16 12:53:07.340054: Model Saved: yelp

2022-06-16 12:53:46.663044: Epoch 112/150, Train: Loss = 0.1811, preLoss = 0.0510

2022-06-16 12:54:26.480431: Epoch 113/150, Train: Loss = 0.1808, preLoss = 0.0511

2022-06-16 12:55:06.192749: Epoch 114/150, Train: Loss = 0.1800, preLoss = 0.0506
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2036 0.2036 0.32348282149902385 0.4377 0.4156349088790998 0.7543
2022-06-16 12:55:54.864649: Epoch 114/150, Test: HR = 0.5986, NDCG = 0.3764
2022-06-16 12:55:56.578805: Model Saved: yelp

2022-06-16 12:56:35.844585: Epoch 115/150, Train: Loss = 0.1793, preLoss = 0.0503

2022-06-16 12:57:15.379130: Epoch 116/150, Train: Loss = 0.1797, preLoss = 0.0510

2022-06-16 12:57:31.569634: Step 7/20: preloss = 0.05, REGLoss = 0.13                                                                                   2022-06-16 12:57:54.905199: Epoch 117/150, Train: Loss = 0.1790, preLoss = 0.0507
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2056 0.2056 0.32473122882121613 0.4395 0.4161393869728796 0.754
2022-06-16 12:58:42.400457: Epoch 117/150, Test: HR = 0.5992, NDCG = 0.3772
2022-06-16 12:58:44.330381: Model Saved: yelp

2022-06-16 12:59:24.163914: Epoch 118/150, Train: Loss = 0.1785, preLoss = 0.0505

2022-06-16 13:00:03.910717: Epoch 119/150, Train: Loss = 0.1788, preLoss = 0.0510

2022-06-16 13:00:43.395609: Epoch 120/150, Train: Loss = 0.1778, preLoss = 0.0504
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2025 0.2025 0.32368799113761876 0.4395 0.4152467107231907 0.7546
2022-06-16 13:01:31.067839: Epoch 120/150, Test: HR = 0.5985, NDCG = 0.3760
2022-06-16 13:01:32.913277: Model Saved: yelp

2022-06-16 13:02:12.575783: Epoch 121/150, Train: Loss = 0.1773, preLoss = 0.0502

2022-06-16 13:02:52.502058: Epoch 122/150, Train: Loss = 0.1777, preLoss = 0.0509

2022-06-16 13:03:32.366571: Epoch 123/150, Train: Loss = 0.1782, preLoss = 0.0516
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2028 0.2028 0.3236152580152178 0.4391 0.41538974008215357 0.7553
2022-06-16 13:04:20.339140: Epoch 123/150, Test: HR = 0.5976, NDCG = 0.3758
2022-06-16 13:04:22.239665: Model Saved: yelp

2022-06-16 13:05:02.075942: Epoch 124/150, Train: Loss = 0.1768, preLoss = 0.0505

2022-06-16 13:05:42.234574: Epoch 125/150, Train: Loss = 0.1774, preLoss = 0.0513

2022-06-16 13:06:21.854900: Epoch 126/150, Train: Loss = 0.1764, preLoss = 0.0505
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2029 0.2029 0.32394045840080293 0.4391 0.41554405872499145 0.7544
2022-06-16 13:07:09.202307: Epoch 126/150, Test: HR = 0.5974, NDCG = 0.3760
2022-06-16 13:07:11.124122: Model Saved: yelp

2022-06-16 13:07:51.733878: Epoch 127/150, Train: Loss = 0.1764, preLoss = 0.0507

2022-06-16 13:08:31.459806: Epoch 128/150, Train: Loss = 0.1762, preLoss = 0.0507

2022-06-16 13:09:11.028444: Epoch 129/150, Train: Loss = 0.1744, preLoss = 0.0492
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2021 0.2021 0.32421042695800867 0.4405 0.4155861700406056 0.7552
2022-06-16 13:09:58.836758: Epoch 129/150, Test: HR = 0.5977, NDCG = 0.3760
2022-06-16 13:10:00.740837: Model Saved: yelp

2022-06-16 13:10:40.528233: Epoch 130/150, Train: Loss = 0.1748, preLoss = 0.0498

2022-06-16 13:11:20.171559: Epoch 131/150, Train: Loss = 0.1754, preLoss = 0.0506

2022-06-16 13:11:59.871373: Epoch 132/150, Train: Loss = 0.1749, preLoss = 0.0503
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2038 0.2038 0.3242377145542892 0.4397 0.416033200088648 0.7558
2022-06-16 13:12:47.518108: Epoch 132/150, Test: HR = 0.5965, NDCG = 0.3759
2022-06-16 13:12:49.498765: Model Saved: yelp

2022-06-16 13:13:29.242998: Epoch 133/150, Train: Loss = 0.1744, preLoss = 0.0500

2022-06-16 13:14:09.478692: Epoch 134/150, Train: Loss = 0.1743, preLoss = 0.0500

2022-06-16 13:14:49.581912: Epoch 135/150, Train: Loss = 0.1743, preLoss = 0.0502
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2031 0.2031 0.32424563582004595 0.4402 0.416040911355333 0.7562
2022-06-16 13:15:37.348462: Epoch 135/150, Test: HR = 0.5971, NDCG = 0.3760
2022-06-16 13:15:39.231303: Model Saved: yelp

2022-06-16 13:16:18.964402: Epoch 136/150, Train: Loss = 0.1740, preLoss = 0.0500

2022-06-16 13:16:58.710588: Epoch 137/150, Train: Loss = 0.1748, preLoss = 0.0511

2022-06-16 13:17:38.797950: Epoch 138/150, Train: Loss = 0.1738, preLoss = 0.0502
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2024 0.2024 0.3236850257933836 0.4392 0.4162003171498207 0.7578
2022-06-16 13:18:26.709285: Epoch 138/150, Test: HR = 0.5977, NDCG = 0.3759
2022-06-16 13:18:28.606058: Model Saved: yelp

2022-06-16 13:19:08.800121: Epoch 139/150, Train: Loss = 0.1734, preLoss = 0.0499

2022-06-16 13:19:48.354456: Epoch 140/150, Train: Loss = 0.1725, preLoss = 0.0493

2022-06-16 13:20:28.291888: Epoch 141/150, Train: Loss = 0.1723, preLoss = 0.0491
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2025 0.2025 0.3239076010145283 0.4397 0.4164149043985532 0.7583
2022-06-16 13:21:15.729414: Epoch 141/150, Test: HR = 0.5980, NDCG = 0.3761
2022-06-16 13:21:17.733147: Model Saved: yelp

2022-06-16 13:21:57.756153: Epoch 142/150, Train: Loss = 0.1725, preLoss = 0.0495

2022-06-16 13:22:37.048064: Epoch 143/150, Train: Loss = 0.1724, preLoss = 0.0495

2022-06-16 13:23:16.853218: Epoch 144/150, Train: Loss = 0.1726, preLoss = 0.0498
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2021 0.2021 0.3241967924837065 0.4405 0.4166899051876836 0.759
2022-06-16 13:24:04.548387: Epoch 144/150, Test: HR = 0.5990, NDCG = 0.3764
2022-06-16 13:24:07.004104: Model Saved: yelp

2022-06-16 13:24:46.898785: Epoch 145/150, Train: Loss = 0.1719, preLoss = 0.0493

2022-06-16 13:25:27.157818: Epoch 146/150, Train: Loss = 0.1729, preLoss = 0.0504

2022-06-16 13:26:06.381196: Epoch 147/150, Train: Loss = 0.1714, preLoss = 0.0490
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2033 0.2033 0.325049107622791 0.441 0.41733588168615554 0.7587
2022-06-16 13:26:54.084894: Epoch 147/150, Test: HR = 0.5993, NDCG = 0.3772
2022-06-16 13:26:56.415445: Model Saved: yelp

2022-06-16 13:27:36.528813: Epoch 148/150, Train: Loss = 0.1719, preLoss = 0.0495

2022-06-16 13:28:16.304663: Epoch 149/150, Train: Loss = 0.1720, preLoss = 0.0498

epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.203 0.203 0.3246184283172405 0.4408 0.4171461784255156 0.7595
2022-06-16 13:29:03.751189: Epoch 150/150, Test: HR = 0.5991, NDCG = 0.3768
2022-06-16 13:29:05.654001: Model Saved: yelp
root@container-327e11a8ac-4a1523bb:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp  0.1 --ssl_reg 5e-6 --s         ave_path yelp --batch 512 --epoch 150
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2022-06-16 13:30:27.165510: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-16 13:30:27.332593: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:241: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("SparseTensor/values:0", shape=(33516         8,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be re         moved in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
drop SparseTensor(indices=Tensor("SparseTensor/indices:0", shape=(335168, 2), dtype=int64), values=Tensor("SparseTensor/values:0", shape=(33516         8,), dtype=float32), dense_shape=Tensor("SparseTensor/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("SparseTensor_1/values:0", shape=(1         77378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_1/indices:0", shape=(177378, 2), dtype=int64), values=Tensor("SparseTensor_1/values:0", shape=(1         77378,), dtype=float32), dense_shape=Tensor("SparseTensor_1/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("SparseTensor_2/values:0", shape=(1         56718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_2/indices:0", shape=(156718, 2), dtype=int64), values=Tensor("SparseTensor_2/values:0", shape=(1         56718,), dtype=float32), dense_shape=Tensor("SparseTensor_2/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("SparseTensor_3/values:0", shape=(1         56674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_3/indices:0", shape=(156674, 2), dtype=int64), values=Tensor("SparseTensor_3/values:0", shape=(1         56674,), dtype=float32), dense_shape=Tensor("SparseTensor_3/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("SparseTensor_4/values:0", shape=(1         59442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_4/indices:0", shape=(159442, 2), dtype=int64), values=Tensor("SparseTensor_4/values:0", shape=(1         59442,), dtype=float32), dense_shape=Tensor("SparseTensor_4/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("SparseTensor_5/values:0", shape=(1         55622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_5/indices:0", shape=(155622, 2), dtype=int64), values=Tensor("SparseTensor_5/values:0", shape=(1         55622,), dtype=float32), dense_shape=Tensor("SparseTensor_5/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("SparseTensor_6/values:0", shape=(1         41690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_6/indices:0", shape=(141690, 2), dtype=int64), values=Tensor("SparseTensor_6/values:0", shape=(1         41690,), dtype=float32), dense_shape=Tensor("SparseTensor_6/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("SparseTensor_7/values:0", shape=(85         108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
drop SparseTensor(indices=Tensor("SparseTensor_7/indices:0", shape=(85108, 2), dtype=int64), values=Tensor("SparseTensor_7/values:0", shape=(85         108,), dtype=float32), dense_shape=Tensor("SparseTensor_7/dense_shape:0", shape=(2,), dtype=int64))
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a fut         ure version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (         from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc6f8467310>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc         6f8467310>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc78f4f9fd0>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fc         78f4f9fd0>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_d         ecay instead.

WARNING:tensorflow:From /root/CLSR/model.py:287: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer ins         tead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wra         pper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-16 13:30:51.136360: Model Prepared
2022-06-16 13:30:55.786742: Variables Inited
2022-06-16 13:31:48.116544: Epoch 0/150, Train: Loss = 5.3270, preLoss = 2.2608
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.016 0.016 0.047224104393966 0.0805 0.10139118884015011 0.2775
2022-06-16 13:32:37.194707: Epoch 0/150, Test: HR = 0.1502, NDCG = 0.0695
WARNING:tensorflow:From /root/CLSR/model.py:524: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

2022-06-16 13:32:39.103927: Model Saved: yelp

2022-06-16 13:33:19.872755: Epoch 1/150, Train: Loss = 9.8075, preLoss = 3.7395

2022-06-16 13:34:00.222464: Epoch 2/150, Train: Loss = 11.8733, preLoss = 3.8200

2022-06-16 13:34:41.540051: Epoch 3/150, Train: Loss = 13.5228, preLoss = 4.0022
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0562 0.0562 0.12317280024937138 0.1872 0.1974734359462302 0.4545
2022-06-16 13:35:32.255426: Epoch 3/150, Test: HR = 0.2898, NDCG = 0.1561
2022-06-16 13:35:33.950004: Model Saved: yelp

2022-06-16 13:36:15.123357: Epoch 4/150, Train: Loss = 14.2485, preLoss = 3.6842

2022-06-16 13:36:55.912382: Epoch 5/150, Train: Loss = 14.2899, preLoss = 3.2361

2022-06-16 13:37:36.927246: Epoch 6/150, Train: Loss = 13.6175, preLoss = 2.5989
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0696 0.0696 0.14080879735355722 0.211 0.21655279453479334 0.4824
2022-06-16 13:38:28.536325: Epoch 6/150, Test: HR = 0.3203, NDCG = 0.1760
2022-06-16 13:38:30.030424: Model Saved: yelp

2022-06-16 13:39:11.066007: Epoch 7/150, Train: Loss = 12.7843, preLoss = 2.1964

2022-06-16 13:39:52.918599: Epoch 8/150, Train: Loss = 11.7425, preLoss = 1.8176

2022-06-16 13:40:34.477849: Epoch 9/150, Train: Loss = 10.6623, preLoss = 1.5146
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0807 0.0807 0.1573040158870021 0.2317 0.24026726804700899 0.5279
2022-06-16 13:41:25.604314: Epoch 9/150, Test: HR = 0.3543, NDCG = 0.1966
2022-06-16 13:41:27.060926: Model Saved: yelp

2022-06-16 13:42:08.273513: Epoch 10/150, Train: Loss = 9.5865, preLoss = 1.2990

2022-06-16 13:42:49.881458: Epoch 11/150, Train: Loss = 8.5762, preLoss = 1.1172

2022-06-16 13:43:31.125256: Epoch 12/150, Train: Loss = 7.5562, preLoss = 0.8611
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0888 0.0888 0.1796447094690883 0.2656 0.26177719235574837 0.5579
2022-06-16 13:44:21.032160: Epoch 12/150, Test: HR = 0.3859, NDCG = 0.2183
2022-06-16 13:44:22.578013: Model Saved: yelp

2022-06-16 13:45:04.315320: Epoch 13/150, Train: Loss = 6.7482, preLoss = 0.7762

2022-06-16 13:45:45.847066: Epoch 14/150, Train: Loss = 5.9679, preLoss = 0.6679

2022-06-16 13:46:26.807642: Epoch 15/150, Train: Loss = 5.2321, preLoss = 0.5440
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0803 0.0803 0.17959470108081613 0.2754 0.2629623297622879 0.5715
2022-06-16 13:47:17.092862: Epoch 15/150, Test: HR = 0.4014, NDCG = 0.2202
2022-06-16 13:47:18.508392: Model Saved: yelp

2022-06-16 13:47:59.461194: Epoch 16/150, Train: Loss = 4.6088, preLoss = 0.4682

2022-06-16 13:48:40.840935: Epoch 17/150, Train: Loss = 4.0528, preLoss = 0.3981

2022-06-16 13:49:22.030357: Epoch 18/150, Train: Loss = 3.5678, preLoss = 0.3423
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0776 0.0776 0.1782437797211101 0.2762 0.2664555818816948 0.588
2022-06-16 13:50:11.649063: Epoch 18/150, Test: HR = 0.4155, NDCG = 0.2231
2022-06-16 13:50:13.312555: Model Saved: yelp

2022-06-16 13:50:55.219491: Epoch 19/150, Train: Loss = 3.1371, preLoss = 0.2940

2022-06-16 13:51:36.906136: Epoch 20/150, Train: Loss = 2.7685, preLoss = 0.2579

2022-06-16 13:52:18.154448: Epoch 21/150, Train: Loss = 2.4542, preLoss = 0.2316
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0886 0.0886 0.18807596069941165 0.2849 0.2769220274503792 0.5988
2022-06-16 13:53:07.759733: Epoch 21/150, Test: HR = 0.4245, NDCG = 0.2331
2022-06-16 13:53:09.270159: Model Saved: yelp

2022-06-16 13:53:50.127231: Epoch 22/150, Train: Loss = 2.1818, preLoss = 0.2072

2022-06-16 13:54:31.059929: Epoch 23/150, Train: Loss = 1.9422, preLoss = 0.1820

2022-06-16 13:55:12.198932: Epoch 24/150, Train: Loss = 1.7405, preLoss = 0.1674
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1006 0.1006 0.2041780715571803 0.3052 0.29236978087453563 0.6166
2022-06-16 13:56:01.734974: Epoch 24/150, Test: HR = 0.4432, NDCG = 0.2487
2022-06-16 13:56:03.231833: Model Saved: yelp

2022-06-16 13:56:43.690242: Epoch 25/150, Train: Loss = 1.5570, preLoss = 0.1476

2022-06-16 13:57:24.932747: Epoch 26/150, Train: Loss = 1.4002, preLoss = 0.1332

2022-06-16 13:58:06.350676: Epoch 27/150, Train: Loss = 1.2675, preLoss = 0.1250
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0969 0.0969 0.20766163651990754 0.3157 0.29649995743148033 0.6287
2022-06-16 13:58:55.388457: Epoch 27/150, Test: HR = 0.4563, NDCG = 0.2531
2022-06-16 13:58:56.932082: Model Saved: yelp

2022-06-16 13:59:38.125904: Epoch 28/150, Train: Loss = 1.1529, preLoss = 0.1180

2022-06-16 14:00:19.005693: Epoch 29/150, Train: Loss = 1.0534, preLoss = 0.1117

2022-06-16 14:01:00.087374: Epoch 30/150, Train: Loss = 0.9618, preLoss = 0.1013
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1213 0.1213 0.23014207095205216 0.3367 0.3159579424149041 0.6398
2022-06-16 14:01:49.728590: Epoch 30/150, Test: HR = 0.4698, NDCG = 0.2731
2022-06-16 14:01:51.282389: Model Saved: yelp

2022-06-16 14:02:31.225307: Epoch 31/150, Train: Loss = 0.8860, preLoss = 0.0969

2022-06-16 14:03:12.556591: Epoch 32/150, Train: Loss = 0.8211, preLoss = 0.0935

2022-06-16 14:03:53.981533: Epoch 33/150, Train: Loss = 0.7630, preLoss = 0.0893
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1168 0.1168 0.22505890956467312 0.3289 0.31602747564868816 0.6486
2022-06-16 14:04:43.052463: Epoch 33/150, Test: HR = 0.4747, NDCG = 0.2722
2022-06-16 14:04:44.577091: Model Saved: yelp

2022-06-16 14:05:25.678178: Epoch 34/150, Train: Loss = 0.7105, preLoss = 0.0850

2022-06-16 14:06:06.711003: Epoch 35/150, Train: Loss = 0.6660, preLoss = 0.0832

2022-06-16 14:06:47.471005: Epoch 36/150, Train: Loss = 0.6279, preLoss = 0.0835
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.123 0.123 0.23452332701330098 0.3404 0.32472448562472067 0.6554
2022-06-16 14:07:35.776703: Epoch 36/150, Test: HR = 0.4888, NDCG = 0.2827
2022-06-16 14:07:37.317574: Model Saved: yelp

2022-06-16 14:08:18.073606: Epoch 37/150, Train: Loss = 0.5900, preLoss = 0.0791

2022-06-16 14:08:59.115783: Epoch 38/150, Train: Loss = 0.5572, preLoss = 0.0757

2022-06-16 14:09:40.064044: Epoch 39/150, Train: Loss = 0.5309, preLoss = 0.0766
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1346 0.1346 0.24925350359860335 0.3615 0.3366329712616756 0.666
2022-06-16 14:10:28.963129: Epoch 39/150, Test: HR = 0.5071, NDCG = 0.2965
2022-06-16 14:10:30.536613: Model Saved: yelp

2022-06-16 14:11:11.505364: Epoch 40/150, Train: Loss = 0.5033, preLoss = 0.0731

2022-06-16 14:11:52.593516: Epoch 41/150, Train: Loss = 0.4799, preLoss = 0.0719

2022-06-16 14:12:33.506841: Epoch 42/150, Train: Loss = 0.4597, preLoss = 0.0716
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1541 0.1541 0.2666604348844976 0.3739 0.3549796170845414 0.6804
2022-06-16 14:13:22.453027: Epoch 42/150, Test: HR = 0.5267, NDCG = 0.3163
2022-06-16 14:13:24.043959: Model Saved: yelp

2022-06-16 14:14:04.465917: Epoch 43/150, Train: Loss = 0.4420, preLoss = 0.0720

2022-06-16 14:14:45.594848: Epoch 44/150, Train: Loss = 0.4231, preLoss = 0.0689

2022-06-16 14:15:26.980648: Epoch 45/150, Train: Loss = 0.4086, preLoss = 0.0693
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.15 0.15 0.26963696721854336 0.3858 0.35711825090806504 0.6913
2022-06-16 14:16:16.227755: Epoch 45/150, Test: HR = 0.5342, NDCG = 0.3176
2022-06-16 14:16:17.845101: Model Saved: yelp

2022-06-16 14:16:58.214818: Epoch 46/150, Train: Loss = 0.3967, preLoss = 0.0707

2022-06-16 14:17:39.470846: Epoch 47/150, Train: Loss = 0.3825, preLoss = 0.0679

2022-06-16 14:18:20.385083: Epoch 48/150, Train: Loss = 0.3711, preLoss = 0.0674
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1598 0.1598 0.2870091111902865 0.4078 0.3723121891762618 0.7042
2022-06-16 14:19:09.283126: Epoch 48/150, Test: HR = 0.5531, NDCG = 0.3342
2022-06-16 14:19:10.919801: Model Saved: yelp

2022-06-16 14:19:51.920843: Epoch 49/150, Train: Loss = 0.3604, preLoss = 0.0672

2022-06-16 14:20:33.208284: Epoch 50/150, Train: Loss = 0.3501, preLoss = 0.0662

2022-06-16 14:21:14.180576: Epoch 51/150, Train: Loss = 0.3405, preLoss = 0.0659
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1534 0.1534 0.2819095173213837 0.4023 0.3697801497959335 0.7066
2022-06-16 14:22:02.912254: Epoch 51/150, Test: HR = 0.5556, NDCG = 0.3317
2022-06-16 14:22:04.513018: Model Saved: yelp

2022-06-16 14:22:45.386982: Epoch 52/150, Train: Loss = 0.3319, preLoss = 0.0649

2022-06-16 14:23:26.439386: Epoch 53/150, Train: Loss = 0.3236, preLoss = 0.0642

2022-06-16 14:24:07.042874: Epoch 54/150, Train: Loss = 0.3159, preLoss = 0.0636
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1645 0.1645 0.2887717024702494 0.4093 0.3782216231579072 0.7171
2022-06-16 14:24:56.468897: Epoch 54/150, Test: HR = 0.5679, NDCG = 0.3406
2022-06-16 14:24:58.190121: Model Saved: yelp

2022-06-16 14:25:39.447557: Epoch 55/150, Train: Loss = 0.3081, preLoss = 0.0622

2022-06-16 14:26:19.851172: Epoch 56/150, Train: Loss = 0.3035, preLoss = 0.0635

2022-06-16 14:27:00.580064: Epoch 57/150, Train: Loss = 0.2964, preLoss = 0.0619
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1546 0.1546 0.2831816710402336 0.4091 0.3757305314254666 0.7252
2022-06-16 14:27:49.570866: Epoch 57/150, Test: HR = 0.5763, NDCG = 0.3381
2022-06-16 14:27:51.207808: Model Saved: yelp

2022-06-16 14:28:31.867700: Epoch 58/150, Train: Loss = 0.2914, preLoss = 0.0622

2022-06-16 14:29:12.524325: Epoch 59/150, Train: Loss = 0.2876, preLoss = 0.0632

2022-06-16 14:29:53.166331: Epoch 60/150, Train: Loss = 0.2815, preLoss = 0.0618
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1576 0.1576 0.2874275132049934 0.4117 0.38017483810939223 0.7276
2022-06-16 14:30:41.845267: Epoch 60/150, Test: HR = 0.5827, NDCG = 0.3437
2022-06-16 14:30:43.501210: Model Saved: yelp

2022-06-16 14:31:23.924535: Epoch 61/150, Train: Loss = 0.2769, preLoss = 0.0615

2022-06-16 14:32:04.460787: Epoch 62/150, Train: Loss = 0.2727, preLoss = 0.0612

2022-06-16 14:32:44.955276: Epoch 63/150, Train: Loss = 0.2676, preLoss = 0.0602
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1636 0.1636 0.29072886623429406 0.4116 0.385326777968988 0.7339
2022-06-16 14:33:33.661949: Epoch 63/150, Test: HR = 0.5839, NDCG = 0.3476
2022-06-16 14:33:35.359222: Model Saved: yelp

2022-06-16 14:34:16.370078: Epoch 64/150, Train: Loss = 0.2642, preLoss = 0.0602

2022-06-16 14:34:57.450053: Epoch 65/150, Train: Loss = 0.2602, preLoss = 0.0598

2022-06-16 14:35:38.391035: Epoch 66/150, Train: Loss = 0.2581, preLoss = 0.0608
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1695 0.1695 0.29852561739265154 0.4249 0.3887336972175291 0.7341
2022-06-16 14:36:27.251101: Epoch 66/150, Test: HR = 0.5840, NDCG = 0.3509
2022-06-16 14:36:28.906396: Model Saved: yelp

2022-06-16 14:37:09.549301: Epoch 67/150, Train: Loss = 0.2530, preLoss = 0.0588

2022-06-16 14:37:50.020835: Epoch 68/150, Train: Loss = 0.2496, preLoss = 0.0583

2022-06-16 14:38:30.941610: Epoch 69/150, Train: Loss = 0.2471, preLoss = 0.0585
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1802 0.1802 0.3042406763287346 0.4245 0.39484596669517086 0.735
2022-06-16 14:39:19.903279: Epoch 69/150, Test: HR = 0.5844, NDCG = 0.3569
2022-06-16 14:39:21.586898: Model Saved: yelp

2022-06-16 14:40:01.975101: Epoch 70/150, Train: Loss = 0.2430, preLoss = 0.0570

2022-06-16 14:40:42.446404: Epoch 71/150, Train: Loss = 0.2418, preLoss = 0.0582

2022-06-16 14:41:22.746534: Epoch 72/150, Train: Loss = 0.2393, preLoss = 0.0582
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.175 0.175 0.30191502833176564 0.423 0.39496888765098703 0.7408
2022-06-16 14:42:11.416854: Epoch 72/150, Test: HR = 0.5902, NDCG = 0.3571
2022-06-16 14:42:13.088387: Model Saved: yelp

2022-06-16 14:42:53.659477: Epoch 73/150, Train: Loss = 0.2364, preLoss = 0.0575

2022-06-16 14:43:34.119085: Epoch 74/150, Train: Loss = 0.2334, preLoss = 0.0564

2022-06-16 14:44:14.538246: Epoch 75/150, Train: Loss = 0.2318, preLoss = 0.0568
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1849 0.1849 0.3064981473139575 0.4223 0.3991887668397113 0.7398
2022-06-16 14:45:03.176573: Epoch 75/150, Test: HR = 0.5884, NDCG = 0.3611
2022-06-16 14:45:05.192638: Model Saved: yelp

2022-06-16 14:45:45.781386: Epoch 76/150, Train: Loss = 0.2298, preLoss = 0.0568

2022-06-16 14:46:26.136929: Epoch 77/150, Train: Loss = 0.2271, preLoss = 0.0557

2022-06-16 14:47:07.223195: Epoch 78/150, Train: Loss = 0.2261, preLoss = 0.0562
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1843 0.1843 0.3067504727786258 0.4239 0.3997060442721497 0.7421
2022-06-16 14:47:55.481121: Epoch 78/150, Test: HR = 0.5890, NDCG = 0.3610
2022-06-16 14:47:57.208681: Model Saved: yelp

2022-06-16 14:48:37.583770: Epoch 79/150, Train: Loss = 0.2230, preLoss = 0.0550

2022-06-16 14:49:17.892085: Epoch 80/150, Train: Loss = 0.2216, preLoss = 0.0554

2022-06-16 14:49:58.613261: Epoch 81/150, Train: Loss = 0.2197, preLoss = 0.0548
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1866 0.1866 0.3100332919030578 0.4272 0.4025535337747361 0.7436
2022-06-16 14:50:46.801239: Epoch 81/150, Test: HR = 0.5928, NDCG = 0.3645
2022-06-16 14:50:48.451230: Model Saved: yelp

2022-06-16 14:51:28.876176: Epoch 82/150, Train: Loss = 0.2193, preLoss = 0.0557

2022-06-16 14:52:09.363026: Epoch 83/150, Train: Loss = 0.2164, preLoss = 0.0545

2022-06-16 14:52:50.100398: Epoch 84/150, Train: Loss = 0.2164, preLoss = 0.0557
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1903 0.1903 0.31708117499941124 0.4396 0.40633301145099404 0.7468
2022-06-16 14:53:38.309645: Epoch 84/150, Test: HR = 0.5957, NDCG = 0.3683
2022-06-16 14:53:39.979418: Model Saved: yelp

2022-06-16 14:54:20.510179: Epoch 85/150, Train: Loss = 0.2145, preLoss = 0.0549

2022-06-16 14:55:01.134553: Epoch 86/150, Train: Loss = 0.2133, preLoss = 0.0547

2022-06-16 14:55:41.416955: Epoch 87/150, Train: Loss = 0.2104, preLoss = 0.0535
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1888 0.1888 0.3168530159578711 0.4423 0.405236066387645 0.7471
2022-06-16 14:56:29.463737: Epoch 87/150, Test: HR = 0.5964, NDCG = 0.3672
2022-06-16 14:56:31.366271: Model Saved: yelp

2022-06-16 14:57:11.666543: Epoch 88/150, Train: Loss = 0.2106, preLoss = 0.0544

2022-06-16 14:57:52.389177: Epoch 89/150, Train: Loss = 0.2089, preLoss = 0.0538

2022-06-16 14:58:32.602547: Epoch 90/150, Train: Loss = 0.2072, preLoss = 0.0532
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1848 0.1848 0.31550055458391074 0.4414 0.4052395525965781 0.7503
2022-06-16 14:59:21.045883: Epoch 90/150, Test: HR = 0.6016, NDCG = 0.3678
2022-06-16 14:59:22.758898: Model Saved: yelp

2022-06-16 15:00:03.225011: Epoch 91/150, Train: Loss = 0.2066, preLoss = 0.0534

2022-06-16 15:00:43.583405: Epoch 92/150, Train: Loss = 0.2069, preLoss = 0.0545

2022-06-16 15:01:24.167508: Epoch 93/150, Train: Loss = 0.2049, preLoss = 0.0537
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1845 0.1845 0.3136832228683389 0.4354 0.4055921291840844 0.7501
2022-06-16 15:02:12.578704: Epoch 93/150, Test: HR = 0.5991, NDCG = 0.3676
2022-06-16 15:02:14.312158: Model Saved: yelp

2022-06-16 15:02:54.836468: Epoch 94/150, Train: Loss = 0.2034, preLoss = 0.0531

2022-06-16 15:03:35.363669: Epoch 95/150, Train: Loss = 0.2029, preLoss = 0.0534

2022-06-16 15:04:15.849388: Epoch 96/150, Train: Loss = 0.2014, preLoss = 0.0523
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1865 0.1865 0.31338024102709405 0.4333 0.40603798469297536 0.7509
2022-06-16 15:05:04.538712: Epoch 96/150, Test: HR = 0.5974, NDCG = 0.3674
2022-06-16 15:05:06.456935: Model Saved: yelp

2022-06-16 15:05:46.884523: Epoch 97/150, Train: Loss = 0.2012, preLoss = 0.0530

2022-06-16 15:06:27.239249: Epoch 98/150, Train: Loss = 0.2009, preLoss = 0.0533

2022-06-16 15:07:07.452215: Epoch 99/150, Train: Loss = 0.1996, preLoss = 0.0527
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1892 0.1892 0.32026611509250735 0.4452 0.4104845299925525 0.7555
2022-06-16 15:07:56.273016: Epoch 99/150, Test: HR = 0.6046, NDCG = 0.3725
2022-06-16 15:07:57.993558: Model Saved: yelp

2022-06-16 15:08:38.254050: Epoch 100/150, Train: Loss = 0.1988, preLoss = 0.0528

^CTraceback (most recent call last):20: preloss = 0.06, REGLoss = 0.14
  File "main.py", line 25, in <module>
    recom.run()
  File "/root/CLSR/model.py", line 50, in run
    reses = self.trainEpoch()
  File "/root/CLSR/model.py", line 391, in trainEpoch
    suLocs, siLocs = self.sampleSslBatch(batIds, self.handler.subadj)
  File "/root/CLSR/model.py", line 338, in sampleSslBatch
    posset = np.reshape(np.argwhere(temLabel[k][i]!=0), [-1])
KeyboardInterrupt
root@container-327e11a8ac-4a1523bb:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp  0.1 --ssl_reg 5e-6 --s         ave_path yelp --batch 512 --epoch 150
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2022-06-16 15:45:56.925666: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-16 15:45:57.095169: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:240: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be re         moved in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a fut         ure version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (         from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f851029cc10>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8         51029cc10>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f860b220990>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f8         60b220990>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
Traceback (most recent call last):
  File "main.py", line 25, in <module>
    recom.run()
  File "/root/CLSR/model.py", line 38, in run
    self.prepareModel()
  File "/root/CLSR/model.py", line 275, in prepareModel
    self.preds, self.sslloss = self.ours()
  File "/root/CLSR/model.py", line 223, in ours
    S_final += tf.reduce_sum(Activate(get_cos_distance(pckUlat,pckIlat,sampNum), self.actFunc),axis=-1) + pckUlat + pckIlat
UnboundLocalError: local variable 'S_final' referenced before assignment
root@container-327e11a8ac-4a1523bb:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp  0.1 --ssl_reg 5e-6 --s         ave_path yelp --batch 512 --epoch 150
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2022-06-16 15:50:36.221009: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-16 15:50:36.298501: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:240: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be re         moved in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a fut         ure version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (         from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7410132cd0>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7         410132cd0>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f74107bf990>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7f7         4107bf990>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:285: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_d         ecay instead.

WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer ins         tead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wra         pper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-16 15:51:00.672792: Model Prepared
2022-06-16 15:51:04.906536: Variables Inited
2022-06-16 15:51:56.862919: Epoch 0/150, Train: Loss = 15.4438, preLoss = 3.8697
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0092 0.0092 0.026933385667651747 0.0459 0.06760627969874504 0.1952
2022-06-16 15:52:46.269369: Epoch 0/150, Test: HR = 0.0939, NDCG = 0.0423
WARNING:tensorflow:From /root/CLSR/model.py:523: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

2022-06-16 15:52:48.312831: Model Saved: yelp

^CTraceback (most recent call last):20: preloss = 10.26, REGLoss = 11.72
  File "main.py", line 25, in <module>
    recom.run()
  File "/root/CLSR/model.py", line 50, in run
    reses = self.trainEpoch()
  File "/root/CLSR/model.py", line 389, in trainEpoch
    uLocs, iLocs, timeLocs = self.sampleTrainBatch(batIds, self.handler.trnMat, self.handler.timeMat, sample_num_list[s])
  File "/root/CLSR/model.py", line 307, in sampleTrainBatch
    neglocs = negSamp(temLabel[i], sampNum, args.item, trnPos[i])
  File "/root/CLSR/DataHandler.py", line 28, in negSamp
    rdmItm = np.random.choice(nodeNum)
  File "mtrand.pyx", line 962, in numpy.random.mtrand.RandomState.choice
  File "mtrand.pyx", line 748, in numpy.random.mtrand.RandomState.randint
  File "_bounded_integers.pyx", line 1228, in numpy.random._bounded_integers._rand_int64
  File "<__array_function__ internals>", line 6, in prod
  File "/root/miniconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py", line 3052, in prod
    keepdims=keepdims, initial=initial, where=where)
  File "/root/miniconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py", line 86, in _wrapreduction
    return ufunc.reduce(obj, axis, dtype, out, **passkwargs)
KeyboardInterrupt

root@container-327e11a8ac-4a1523bb:~/CLSR#
root@container-327e11a8ac-4a1523bb:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp  0.1 --ssl_reg 1e-6 --s         ave_path yelp --batch 512 --epoch 150
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synony         m of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a          synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2022-06-16 15:53:37.070782: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-16 15:53:37.148861: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:240: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be re         moved in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a fut         ure version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (         from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fba143b1b50>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb         a143b1b50>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fbb016aa090>> could not be transformed          and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAP         H_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb         b016aa090>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:285: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_d         ecay instead.

WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer ins         tead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wra         pper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-16 15:54:01.511107: Model Prepared
2022-06-16 15:54:04.362708: Variables Inited
2022-06-16 15:54:54.926964: Epoch 0/150, Train: Loss = 8.1165, preLoss = 3.4116
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0094 0.0094 0.03067423154263865 0.0527 0.07346515299918464 0.2098
2022-06-16 15:55:46.266961: Epoch 0/150, Test: HR = 0.1030, NDCG = 0.0467
WARNING:tensorflow:From /root/CLSR/model.py:523: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

2022-06-16 15:55:48.304975: Model Saved: yelp

2022-06-16 15:56:28.913016: Epoch 1/150, Train: Loss = 14.6297, preLoss = 6.7447

2022-06-16 15:57:09.674593: Epoch 2/150, Train: Loss = 16.3265, preLoss = 6.1315

2022-06-16 15:57:50.618217: Epoch 3/150, Train: Loss = 17.2418, preLoss = 5.3665
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0404 0.0404 0.10587458312139315 0.1684 0.17576801904395692 0.4194
2022-06-16 15:58:41.954948: Epoch 3/150, Test: HR = 0.2675, NDCG = 0.1376
2022-06-16 15:58:43.432153: Model Saved: yelp

2022-06-16 15:59:24.930584: Epoch 4/150, Train: Loss = 18.0229, preLoss = 5.1636

2022-06-16 16:00:06.168710: Epoch 5/150, Train: Loss = 18.0775, preLoss = 4.7547

2022-06-16 16:00:47.185861: Epoch 6/150, Train: Loss = 17.5577, preLoss = 4.1150
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0597 0.0597 0.1414529742130407 0.2176 0.2159797412953324 0.4835
2022-06-16 16:01:37.897676: Epoch 6/150, Test: HR = 0.3249, NDCG = 0.1761
2022-06-16 16:01:39.363435: Model Saved: yelp

2022-06-16 16:02:19.732217: Epoch 7/150, Train: Loss = 16.5806, preLoss = 3.3839

2022-06-16 16:03:00.659353: Epoch 8/150, Train: Loss = 15.5812, preLoss = 2.8357

2022-06-16 16:03:41.282120: Epoch 9/150, Train: Loss = 14.4651, preLoss = 2.4134
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0791 0.0791 0.16414891486723002 0.2486 0.24550130750275562 0.539
2022-06-16 16:04:31.474770: Epoch 9/150, Test: HR = 0.3698, NDCG = 0.2029
2022-06-16 16:04:32.966798: Model Saved: yelp

2022-06-16 16:05:14.257385: Epoch 10/150, Train: Loss = 13.3477, preLoss = 2.0580

2022-06-16 16:05:55.425067: Epoch 11/150, Train: Loss = 12.2267, preLoss = 1.7452

2022-06-16 16:06:35.844423: Epoch 12/150, Train: Loss = 11.1180, preLoss = 1.4542
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0971 0.0971 0.1858155906142714 0.2725 0.26983874147048753 0.5691
2022-06-16 16:07:26.575765: Epoch 12/150, Test: HR = 0.4052, NDCG = 0.2286
2022-06-16 16:07:28.102803: Model Saved: yelp

2022-06-16 16:08:09.238800: Epoch 13/150, Train: Loss = 10.1376, preLoss = 1.2658

2022-06-16 16:08:50.055532: Epoch 14/150, Train: Loss = 9.1781, preLoss = 1.0521

2022-06-16 16:09:30.706387: Epoch 15/150, Train: Loss = 8.3496, preLoss = 0.9203
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0935 0.0935 0.18415301902813588 0.2718 0.2700562589724866 0.5759
2022-06-16 16:10:19.484308: Epoch 15/150, Test: HR = 0.4105, NDCG = 0.2285
2022-06-16 16:10:20.993055: Model Saved: yelp

2022-06-16 16:11:01.213388: Epoch 16/150, Train: Loss = 7.5766, preLoss = 0.7925

2022-06-16 16:11:41.612754: Epoch 17/150, Train: Loss = 6.8926, preLoss = 0.7031

2022-06-16 16:12:21.939157: Epoch 18/150, Train: Loss = 6.2739, preLoss = 0.6210
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0837 0.0837 0.17881852552319938 0.2708 0.2657233940794077 0.5783
2022-06-16 16:13:10.785911: Epoch 18/150, Test: HR = 0.4056, NDCG = 0.2221
2022-06-16 16:13:12.313071: Model Saved: yelp

2022-06-16 16:13:52.918381: Epoch 19/150, Train: Loss = 5.6969, preLoss = 0.5279

2022-06-16 16:14:33.377544: Epoch 20/150, Train: Loss = 5.2206, preLoss = 0.4760

2022-06-16 16:15:13.936684: Epoch 21/150, Train: Loss = 4.7774, preLoss = 0.4244
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.084 0.084 0.17917527457481272 0.2713 0.2674186116940548 0.583
2022-06-16 16:16:02.749210: Epoch 21/150, Test: HR = 0.4115, NDCG = 0.2243
2022-06-16 16:16:04.308046: Model Saved: yelp

2022-06-16 16:16:45.055056: Epoch 22/150, Train: Loss = 4.3714, preLoss = 0.3676

2022-06-16 16:17:26.351649: Epoch 23/150, Train: Loss = 4.0310, preLoss = 0.3468

2022-06-16 16:18:07.333274: Epoch 24/150, Train: Loss = 3.7156, preLoss = 0.3139
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.084 0.084 0.17731409939131856 0.2678 0.26744775711095214 0.587
2022-06-16 16:18:56.088034: Epoch 24/150, Test: HR = 0.4066, NDCG = 0.2219
2022-06-16 16:18:57.633658: Model Saved: yelp

2022-06-16 16:19:38.324057: Epoch 25/150, Train: Loss = 3.4256, preLoss = 0.2662

2022-06-16 16:20:18.880918: Epoch 26/150, Train: Loss = 3.1807, preLoss = 0.2547

2022-06-16 16:20:59.344331: Epoch 27/150, Train: Loss = 2.9585, preLoss = 0.2367
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.087 0.087 0.18113093639308772 0.274 0.27189111463228305 0.595
2022-06-16 16:21:47.525498: Epoch 27/150, Test: HR = 0.4138, NDCG = 0.2262
2022-06-16 16:21:49.092826: Model Saved: yelp

2022-06-16 16:22:29.768265: Epoch 28/150, Train: Loss = 2.7542, preLoss = 0.2052

2022-06-16 16:23:10.203019: Epoch 29/150, Train: Loss = 2.5834, preLoss = 0.1963

2022-06-16 16:23:51.445789: Epoch 30/150, Train: Loss = 2.4281, preLoss = 0.1852
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0907 0.0907 0.18063131564489068 0.2687 0.27230805081466475 0.5915
2022-06-16 16:24:40.033535: Epoch 30/150, Test: HR = 0.4157, NDCG = 0.2281
2022-06-16 16:24:41.822130: Model Saved: yelp

2022-06-16 16:25:23.611961: Epoch 31/150, Train: Loss = 2.2800, preLoss = 0.1696

2022-06-16 16:26:04.156474: Epoch 32/150, Train: Loss = 2.1616, preLoss = 0.1621

2022-06-16 16:26:44.587385: Epoch 33/150, Train: Loss = 2.0401, preLoss = 0.1534
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0866 0.0866 0.17967066172640378 0.271 0.268785878964743 0.5855
2022-06-16 16:27:33.576205: Epoch 33/150, Test: HR = 0.4108, NDCG = 0.2247
2022-06-16 16:27:36.315163: Model Saved: yelp

2022-06-16 16:28:18.313314: Epoch 34/150, Train: Loss = 1.9287, preLoss = 0.1374

2022-06-16 16:28:59.078758: Epoch 35/150, Train: Loss = 1.8397, preLoss = 0.1373

2022-06-16 16:29:39.756119: Epoch 36/150, Train: Loss = 1.7499, preLoss = 0.1270
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0912 0.0912 0.1846040530456638 0.2771 0.27352867514067614 0.5919
2022-06-16 16:30:28.252961: Epoch 36/150, Test: HR = 0.4170, NDCG = 0.2296
2022-06-16 16:30:30.757132: Model Saved: yelp

2022-06-16 16:31:11.494439: Epoch 37/150, Train: Loss = 1.6832, preLoss = 0.1276

2022-06-16 16:31:52.632915: Epoch 38/150, Train: Loss = 1.6081, preLoss = 0.1171

2022-06-16 16:32:33.458392: Epoch 39/150, Train: Loss = 1.5501, preLoss = 0.1127
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0893 0.0893 0.18396034912596665 0.2758 0.27472649109908054 0.5961
2022-06-16 16:33:21.887730: Epoch 39/150, Test: HR = 0.4173, NDCG = 0.2297
2022-06-16 16:33:24.038601: Model Saved: yelp

2022-06-16 16:34:05.398573: Epoch 40/150, Train: Loss = 1.4817, preLoss = 0.1057

2022-06-16 16:34:46.346863: Epoch 41/150, Train: Loss = 1.4301, preLoss = 0.1049

2022-06-16 16:35:27.815030: Epoch 42/150, Train: Loss = 1.3770, preLoss = 0.0964
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0919 0.0919 0.188362883409565 0.2821 0.27778362266001944 0.5991
2022-06-16 16:36:16.743053: Epoch 42/150, Test: HR = 0.4189, NDCG = 0.2324
2022-06-16 16:36:18.734009: Model Saved: yelp

2022-06-16 16:36:59.095390: Epoch 43/150, Train: Loss = 1.3305, preLoss = 0.0937

2022-06-16 16:37:39.550218: Epoch 44/150, Train: Loss = 1.2926, preLoss = 0.0930

2022-06-16 16:38:20.317227: Epoch 45/150, Train: Loss = 1.2514, preLoss = 0.0902
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0982 0.0982 0.19256289648262592 0.2842 0.2816910350776391 0.599
2022-06-16 16:39:09.239393: Epoch 45/150, Test: HR = 0.4227, NDCG = 0.2372
2022-06-16 16:39:11.240762: Model Saved: yelp

2022-06-16 16:39:51.961782: Epoch 46/150, Train: Loss = 1.2146, preLoss = 0.0862

2022-06-16 16:40:32.850014: Epoch 47/150, Train: Loss = 1.1823, preLoss = 0.0858

2022-06-16 16:41:13.334284: Epoch 48/150, Train: Loss = 1.1424, preLoss = 0.0819
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0999 0.0999 0.19445524857879123 0.2864 0.28469797377228195 0.605
2022-06-16 16:42:02.665655: Epoch 48/150, Test: HR = 0.4294, NDCG = 0.2405
2022-06-16 16:42:04.962612: Model Saved: yelp

2022-06-16 16:42:45.558879: Epoch 49/150, Train: Loss = 1.1104, preLoss = 0.0808

2022-06-16 16:43:26.298020: Epoch 50/150, Train: Loss = 1.0772, preLoss = 0.0760

2022-06-16 16:44:07.334500: Epoch 51/150, Train: Loss = 1.0509, preLoss = 0.0770
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0949 0.0949 0.19220568734174906 0.2873 0.2834256403892027 0.6088
2022-06-16 16:44:56.665350: Epoch 51/150, Test: HR = 0.4309, NDCG = 0.2385
2022-06-16 16:44:58.474819: Model Saved: yelp

2022-06-16 16:45:38.863661: Epoch 52/150, Train: Loss = 1.0248, preLoss = 0.0748

2022-06-16 16:46:19.232447: Epoch 53/150, Train: Loss = 0.9985, preLoss = 0.0721

2022-06-16 16:46:59.585386: Epoch 54/150, Train: Loss = 0.9711, preLoss = 0.0683
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0968 0.0968 0.19442299569407923 0.2896 0.28668805775081224 0.6152
2022-06-16 16:47:47.932642: Epoch 54/150, Test: HR = 0.4361, NDCG = 0.2416
2022-06-16 16:47:50.623182: Model Saved: yelp

2022-06-16 16:48:31.007435: Epoch 55/150, Train: Loss = 0.9443, preLoss = 0.0680

2022-06-16 16:49:11.853185: Epoch 56/150, Train: Loss = 0.9294, preLoss = 0.0652

2022-06-16 16:49:52.267820: Epoch 57/150, Train: Loss = 0.9065, preLoss = 0.0651
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0971 0.0971 0.19507193954955962 0.2908 0.28752246134066456 0.6168
2022-06-16 16:50:40.284700: Epoch 57/150, Test: HR = 0.4389, NDCG = 0.2428
2022-06-16 16:50:42.143570: Model Saved: yelp

2022-06-16 16:51:22.479844: Epoch 58/150, Train: Loss = 0.8955, preLoss = 0.0635

2022-06-16 16:52:02.968283: Epoch 59/150, Train: Loss = 0.8712, preLoss = 0.0634

2022-06-16 16:52:43.270933: Epoch 60/150, Train: Loss = 0.8510, preLoss = 0.0626
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1008 0.1008 0.2009515147321221 0.2973 0.29289510835615706 0.6223
2022-06-16 16:53:31.442462: Epoch 60/150, Test: HR = 0.4426, NDCG = 0.2476
2022-06-16 16:53:33.807753: Model Saved: yelp

2022-06-16 16:54:14.058933: Epoch 61/150, Train: Loss = 0.8344, preLoss = 0.0607

2022-06-16 16:54:54.619812: Epoch 62/150, Train: Loss = 0.8197, preLoss = 0.0592

2022-06-16 16:55:34.960273: Epoch 63/150, Train: Loss = 0.8040, preLoss = 0.0594
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1031 0.1031 0.20337571347279823 0.3006 0.2957592986603146 0.6273
2022-06-16 16:56:23.786573: Epoch 63/150, Test: HR = 0.4473, NDCG = 0.2505
2022-06-16 16:56:25.723258: Model Saved: yelp

2022-06-16 16:57:06.125585: Epoch 64/150, Train: Loss = 0.7878, preLoss = 0.0589

2022-06-16 16:57:46.827947: Epoch 65/150, Train: Loss = 0.7760, preLoss = 0.0579

2022-06-16 16:58:27.174073: Epoch 66/150, Train: Loss = 0.7592, preLoss = 0.0561
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.108 0.108 0.2097196839878101 0.3084 0.3014155034540011 0.6324
2022-06-16 16:59:15.299626: Epoch 66/150, Test: HR = 0.4535, NDCG = 0.2564
2022-06-16 16:59:17.434531: Model Saved: yelp

2022-06-16 16:59:58.036058: Epoch 67/150, Train: Loss = 0.7443, preLoss = 0.0548

2022-06-16 17:00:38.416897: Epoch 68/150, Train: Loss = 0.7364, preLoss = 0.0550

2022-06-16 17:01:18.993247: Epoch 69/150, Train: Loss = 0.7275, preLoss = 0.0543
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1122 0.1122 0.21435162276185588 0.3135 0.30499130002420494 0.6339
2022-06-16 17:02:06.486289: Epoch 69/150, Test: HR = 0.4585, NDCG = 0.2608
2022-06-16 17:02:08.542020: Model Saved: yelp

2022-06-16 17:02:48.574364: Epoch 70/150, Train: Loss = 0.7123, preLoss = 0.0534

2022-06-16 17:03:28.969578: Epoch 71/150, Train: Loss = 0.7036, preLoss = 0.0530

2022-06-16 17:04:10.155835: Epoch 72/150, Train: Loss = 0.6949, preLoss = 0.0533
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1111 0.1111 0.21457833788243988 0.3144 0.30683764308362177 0.6397
2022-06-16 17:04:59.195621: Epoch 72/150, Test: HR = 0.4617, NDCG = 0.2620
2022-06-16 17:05:01.168427: Model Saved: yelp

2022-06-16 17:05:41.659459: Epoch 73/150, Train: Loss = 0.6798, preLoss = 0.0509

2022-06-16 17:06:22.231102: Epoch 74/150, Train: Loss = 0.6724, preLoss = 0.0505

2022-06-16 17:07:02.462425: Epoch 75/150, Train: Loss = 0.6616, preLoss = 0.0504
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1101 0.1101 0.21565134182182719 0.3173 0.3072755203486021 0.6399
2022-06-16 17:07:50.644762: Epoch 75/150, Test: HR = 0.4634, NDCG = 0.2628
2022-06-16 17:07:52.507797: Model Saved: yelp

2022-06-16 17:08:33.027441: Epoch 76/150, Train: Loss = 0.6523, preLoss = 0.0498

2022-06-16 17:09:13.144524: Epoch 77/150, Train: Loss = 0.6459, preLoss = 0.0490

2022-06-16 17:09:54.142673: Epoch 78/150, Train: Loss = 0.6390, preLoss = 0.0489
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1084 0.1084 0.21462654475234813 0.3165 0.30774366907377226 0.6435
2022-06-16 17:10:42.739177: Epoch 78/150, Test: HR = 0.4686, NDCG = 0.2637
2022-06-16 17:10:44.629028: Model Saved: yelp

2022-06-16 17:11:25.199484: Epoch 79/150, Train: Loss = 0.6267, preLoss = 0.0477

2022-06-16 17:12:06.039683: Epoch 80/150, Train: Loss = 0.6237, preLoss = 0.0482

2022-06-16 17:12:46.301753: Epoch 81/150, Train: Loss = 0.6164, preLoss = 0.0483
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1095 0.1095 0.21694441738249692 0.32 0.3099825875304906 0.6469
2022-06-16 17:13:34.890800: Epoch 81/150, Test: HR = 0.4697, NDCG = 0.2653
2022-06-16 17:13:36.851505: Model Saved: yelp

2022-06-16 17:14:17.184622: Epoch 82/150, Train: Loss = 0.6076, preLoss = 0.0474

2022-06-16 17:14:57.731290: Epoch 83/150, Train: Loss = 0.6004, preLoss = 0.0461

2022-06-16 17:15:38.533146: Epoch 84/150, Train: Loss = 0.5973, preLoss = 0.0477
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1105 0.1105 0.21990188150900886 0.3242 0.31262348607206447 0.6497
2022-06-16 17:16:26.507336: Epoch 84/150, Test: HR = 0.4733, NDCG = 0.2681
2022-06-16 17:16:28.400890: Model Saved: yelp

2022-06-16 17:17:08.665869: Epoch 85/150, Train: Loss = 0.5865, preLoss = 0.0460

2022-06-16 17:17:49.141267: Epoch 86/150, Train: Loss = 0.5833, preLoss = 0.0456

2022-06-16 17:18:30.090607: Epoch 87/150, Train: Loss = 0.5781, preLoss = 0.0443
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1116 0.1116 0.21978153068113407 0.3231 0.3140769326741365 0.6537
2022-06-16 17:19:18.379720: Epoch 87/150, Test: HR = 0.4752, NDCG = 0.2691
2022-06-16 17:19:20.304941: Model Saved: yelp

2022-06-16 17:20:01.219441: Epoch 88/150, Train: Loss = 0.5726, preLoss = 0.0456

2022-06-16 17:20:41.477688: Epoch 89/150, Train: Loss = 0.5655, preLoss = 0.0449

2022-06-16 17:21:21.460179: Epoch 90/150, Train: Loss = 0.5600, preLoss = 0.0443
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1164 0.1164 0.22411562911372954 0.3282 0.3171746840654056 0.655
2022-06-16 17:22:09.794322: Epoch 90/150, Test: HR = 0.4779, NDCG = 0.2725
2022-06-16 17:22:11.685885: Model Saved: yelp

2022-06-16 17:22:51.797741: Epoch 91/150, Train: Loss = 0.5509, preLoss = 0.0425

2022-06-16 17:23:32.579734: Epoch 92/150, Train: Loss = 0.5514, preLoss = 0.0441

2022-06-16 17:24:12.895460: Epoch 93/150, Train: Loss = 0.5465, preLoss = 0.0420
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1184 0.1184 0.2273247920226182 0.3316 0.3197576401330587 0.6556
2022-06-16 17:25:00.780890: Epoch 93/150, Test: HR = 0.4831, NDCG = 0.2763
2022-06-16 17:25:02.704116: Model Saved: yelp

2022-06-16 17:25:43.202846: Epoch 94/150, Train: Loss = 0.5426, preLoss = 0.0436

2022-06-16 17:26:23.707094: Epoch 95/150, Train: Loss = 0.5399, preLoss = 0.0434

2022-06-16 17:27:04.742718: Epoch 96/150, Train: Loss = 0.5365, preLoss = 0.0436
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1201 0.1201 0.22901153391761428 0.3336 0.3212710228456792 0.6566
2022-06-16 17:27:52.817903: Epoch 96/150, Test: HR = 0.4842, NDCG = 0.2778
2022-06-16 17:27:54.974450: Model Saved: yelp

2022-06-16 17:28:35.613639: Epoch 97/150, Train: Loss = 0.5313, preLoss = 0.0435

2022-06-16 17:29:16.131847: Epoch 98/150, Train: Loss = 0.5242, preLoss = 0.0416

^CTraceback (most recent call last):20: preloss = 0.04, REGLoss = 0.48
  File "main.py", line 25, in <module>
    recom.run()
  File "/root/CLSR/model.py", line 50, in run
    reses = self.trainEpoch()
  File "/root/CLSR/model.py", line 390, in trainEpoch
    suLocs, siLocs = self.sampleSslBatch(batIds, self.handler.subadj)
  File "/root/CLSR/model.py", line 337, in sampleSslBatch
    posset = np.reshape(np.argwhere(temLabel[k][i]!=0), [-1])
KeyboardInterrupt

^[[A^[[A
root@container-327e11a8ac-4a1523bb:~/CLSR#
root@container-327e11a8ac-4a1523bb:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp  0.1 --ssl_reg 5e-6 --save_path yelp --batch 512 --epoch 150
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2022-06-16 17:29:50.073790: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-16 17:29:50.247968: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

^CTraceback (most recent call last):
  File "main.py", line 23, in <module>
    with tf.Session(config=config) as sess:
  File "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 1570, in __init__
    super(Session, self).__init__(target, graph, config=config)
  File "/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py", line 693, in __init__
    self._session = tf_session.TF_NewSessionRef(self._graph._c_graph, opts)
KeyboardInterrupt
root@container-327e11a8ac-4a1523bb:~/CLSR# CUDA_VISIBLE_DEVICES=0 python main.py --data yelp --reg 1e-2          --temp  0.1 --ssl_reg 1e-5 --save_path yelp --batch 512 --epoch 150
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/root/miniconda3/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
WARNING:tensorflow:From main.py:15: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.

2022-06-16 17:34:19.400768: Start
tstInt [None None None ... None 8044 None]
tstStat [False False False ... False  True False] 34306
tstUsrs [    5     6     8 ... 34293 34297 34304] 10000
trnMat   (0, 0) 1.0
  (0, 1)        1.0
  (0, 2)        1.0
  (0, 3)        1.0
  (0, 4)        1.0
  (0, 5)        1.0
  (0, 6)        1.0
  (0, 7)        1.0
  (0, 8)        2.0
  (0, 9)        1.0
  (1, 10)       1.0
  (1, 11)       1.0
  (1, 12)       1.0
  (1, 13)       1.0
  (1, 14)       1.0
  (1, 15)       1.0
  (1, 16)       1.0
  (1, 17)       1.0
  (1, 18)       1.0
  (1, 19)       1.0
  (1, 20)       1.0
  (2, 21)       1.0
  (2, 22)       1.0
  (2, 23)       1.0
  (2, 24)       1.0
  :     :
  (34303, 14502)        1.0
  (34303, 15810)        1.0
  (34303, 15826)        1.0
  (34303, 21557)        1.0
  (34303, 21953)        1.0
  (34303, 35239)        1.0
  (34304, 258)  1.0
  (34304, 6211) 1.0
  (34304, 9161) 1.0
  (34304, 18943)        1.0
  (34304, 18957)        1.0
  (34304, 19006)        1.0
  (34304, 19872)        1.0
  (34304, 25815)        1.0
  (34304, 41723)        1.0
  (34305, 264)  1.0
  (34305, 1207) 1.0
  (34305, 3229) 1.0
  (34305, 4340) 1.0
  (34305, 4344) 1.0
  (34305, 5847) 1.0
  (34305, 9852) 1.0
  (34305, 18942)        1.0
  (34305, 23483)        1.0
  (34305, 40666)        1.0   (0, 34306)        1.0
  (0, 34307)    1.0
  (0, 34308)    1.0
  (0, 34309)    1.0
  (0, 34311)    1.0
  (0, 34313)    1.0
  (0, 34314)    1.0
  (0, 34315)    1.0
  (1, 34320)    1.0
  (1, 34321)    1.0
  (1, 34326)    1.0
  (2, 34327)    1.0
  (2, 34331)    1.0
  (2, 34337)    1.0
  (2, 34342)    1.0
  (2, 34346)    1.0
  (3, 34367)    1.0
  (3, 34369)    1.0
  (3, 34370)    1.0
  (3, 34372)    1.0
  (3, 34376)    1.0
  (3, 34378)    1.0
  (3, 34386)    1.0
  (3, 34396)    1.0
  (3, 34409)    1.0
  :     :
  (80280, 33393)        1.0
  (80282, 32716)        1.0
  (80287, 32812)        1.0
  (80291, 32853)        2.0
  (80291, 33050)        2.0
  (80293, 32862)        1.0
  (80302, 33141)        1.0
  (80303, 33142)        1.0
  (80306, 33163)        1.0
  (80307, 33173)        1.0
  (80309, 33197)        2.0
  (80310, 33206)        1.0
  (80311, 33956)        2.0
  (80313, 33229)        1.0
  (80317, 33314)        1.0
  (80319, 33331)        1.0
  (80321, 33354)        1.0
  (80328, 33399)        1.0
  (80336, 33535)        1.0
  (80338, 33576)        1.0
  (80350, 33781)        1.0
  (80358, 33946)        1.0
  (80359, 33955)        1.0
  (80362, 34076)        1.0
  (80365, 34120)        1.0   (1, 34316)        1.0
  (1, 34317)    1.0
  (1, 34318)    1.0
  (1, 34322)    1.0
  (1, 34324)    1.0
  (1, 34325)    1.0
  (2, 34334)    1.0
  (2, 34335)    1.0
  (2, 34339)    1.0
  (2, 34349)    1.0
  (3, 34339)    1.0
  (3, 34352)    1.0
  (3, 34353)    1.0
  (3, 34354)    1.0
  (3, 34360)    1.0
  (3, 34361)    1.0
  (3, 34362)    1.0
  (3, 34368)    1.0
  (3, 34379)    1.0
  (3, 34381)    1.0
  (3, 34385)    1.0
  (3, 34390)    1.0
  (3, 34391)    1.0
  (3, 34392)    1.0
  (3, 34393)    1.0
  :     :
  (80204, 31379)        1.0
  (80206, 31422)        1.0
  (80239, 32026)        1.0
  (80243, 32055)        1.0
  (80252, 32250)        1.0
  (80257, 32342)        1.0
  (80261, 32372)        1.0
  (80266, 32450)        1.0
  (80270, 32474)        1.0
  (80279, 32642)        1.0
  (80286, 32803)        1.0
  (80289, 32818)        1.0
  (80296, 32959)        2.0
  (80297, 33036)        1.0
  (80298, 33068)        1.0
  (80311, 33207)        1.0
  (80322, 33356)        1.0
  (80325, 33383)        1.0
  (80345, 33691)        1.0
  (80347, 33737)        1.0
  (80349, 33775)        1.0
  (80354, 33846)        1.0
  (80357, 33867)        1.0
  (80360, 34013)        1.0
  (80369, 34192)        1.0   (0, 4)    3
  (0, 6)        2
  (1, 10)       1
  (1, 11)       1
  (1, 12)       1
  (1, 13)       3
  (1, 16)       1
  (1, 17)       5
  (1, 18)       1
  (1, 19)       1
  (2, 22)       3
  (2, 23)       5
  (2, 24)       5
  (2, 26)       5
  (2, 27)       7
  (2, 28)       1
  (2, 29)       1
  (2, 30)       6
  (2, 32)       5
  (2, 33)       1
  (2, 34)       3
  (2, 35)       5
  (2, 37)       5
  (2, 38)       5
  (2, 39)       5
  :     :
  (34303, 35239)        6
  (34303, 6345) 5
  (34303, 15810)        7
  (34303, 14502)        5
  (34303, 3797) 3
  (34303, 21953)        6
  (34303, 492)  3
  (34303, 15826)        6
  (34303, 1934) 6
  (34304, 6211) 2
  (34304, 258)  2
  (34304, 18943)        4
  (34304, 18957)        2
  (34304, 9161) 2
  (34304, 25815)        2
  (34304, 19872)        2
  (34304, 19006)        2
  (34304, 41723)        4
  (34305, 4340) 5
  (34305, 3229) 1
  (34305, 5847) 4
  (34305, 40666)        7
  (34305, 1207) 4
  (34305, 4344) 4
  (34305, 264)  7
[46068 43634 43633 ...   458    51  6084]
2022-06-16 17:34:19.480537: Load Data
WARNING:tensorflow:From main.py:23: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.

USER 34306 ITEM 46069
WARNING:tensorflow:From /root/CLSR/model.py:240: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

WARNING:tensorflow:From /root/CLSR/Utils/NNLayers.py:48: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.

WARNING:tensorflow:From /root/CLSR/model.py:95: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
WARNING:tensorflow:From /root/CLSR/Utils/attention.py:9: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.

WARNING:tensorflow:From /root/CLSR/Utils/attention.py:19: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.
Instructions for updating:
Use keras.layers.dense instead.
WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.
Instructions for updating:
Call initializer instance with the dtype argument instead of passing it to the constructor
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb74c071e90>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb74c071e90>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose:0", shape=(34306, 8, 64), dtype=float32)
WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb819b438d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7fb819b438d0>>: AttributeError: module 'gast' has no attribute 'Index'
candidate_vector Tensor("transpose_1:0", shape=(46069, 8, 64), dtype=float32)
WARNING:tensorflow:From /root/CLSR/model.py:285: The name tf.train.exponential_decay is deprecated. Please use tf.compat.v1.train.exponential_decay instead.

WARNING:tensorflow:From /root/CLSR/model.py:286: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.

WARNING:tensorflow:From /root/miniconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
2022-06-16 17:34:42.562155: Model Prepared
2022-06-16 17:34:46.633901: Variables Inited
2022-06-16 17:35:37.258854: Epoch 0/150, Train: Loss = 5.3545, preLoss = 2.2707
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0216 0.0216 0.05447447392914313 0.0881 0.10784133331957303 0.2819
2022-06-16 17:36:27.894917: Epoch 0/150, Test: HR = 0.1578, NDCG = 0.0768
WARNING:tensorflow:From /root/CLSR/model.py:523: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.

2022-06-16 17:36:30.950818: Model Saved: yelp

2022-06-16 17:37:12.317037: Epoch 1/150, Train: Loss = 9.7305, preLoss = 3.6374

2022-06-16 17:37:53.586995: Epoch 2/150, Train: Loss = 11.9143, preLoss = 3.8394

2022-06-16 17:38:35.339939: Epoch 3/150, Train: Loss = 13.4682, preLoss = 3.9101
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0618 0.0618 0.12106760588214836 0.18 0.19307498344252313 0.4382
2022-06-16 17:39:28.030865: Epoch 3/150, Test: HR = 0.2856, NDCG = 0.1548
2022-06-16 17:39:29.626991: Model Saved: yelp

2022-06-16 17:40:11.364368: Epoch 4/150, Train: Loss = 14.1177, preLoss = 3.6032

2022-06-16 17:40:52.914386: Epoch 5/150, Train: Loss = 14.0590, preLoss = 3.1033

2022-06-16 17:41:34.519378: Epoch 6/150, Train: Loss = 13.5327, preLoss = 2.6021
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.065 0.065 0.14429534073358063 0.2228 0.22112483956532084 0.4968
2022-06-16 17:42:26.830568: Epoch 6/150, Test: HR = 0.3352, NDCG = 0.1804
2022-06-16 17:42:28.732044: Model Saved: yelp

2022-06-16 17:43:10.625628: Epoch 7/150, Train: Loss = 12.6998, preLoss = 2.1821

2022-06-16 17:43:52.617500: Epoch 8/150, Train: Loss = 11.6788, preLoss = 1.7969

2022-06-16 17:44:34.538334: Epoch 9/150, Train: Loss = 10.6082, preLoss = 1.5165
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.078 0.078 0.1599213329311042 0.241 0.24047166908059786 0.527
2022-06-16 17:45:26.082241: Epoch 9/150, Test: HR = 0.3624, NDCG = 0.1990
2022-06-16 17:45:28.191684: Model Saved: yelp

2022-06-16 17:46:09.738304: Epoch 10/150, Train: Loss = 9.5221, preLoss = 1.2717

2022-06-16 17:46:51.348700: Epoch 11/150, Train: Loss = 8.4744, preLoss = 1.0623

2022-06-16 17:47:33.396422: Epoch 12/150, Train: Loss = 7.5289, preLoss = 0.9109
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0898 0.0898 0.17598457662388048 0.2584 0.2573359117662307 0.5478
2022-06-16 17:48:24.983278: Epoch 12/150, Test: HR = 0.3812, NDCG = 0.2155
2022-06-16 17:48:27.236100: Model Saved: yelp

2022-06-16 17:49:09.064275: Epoch 13/150, Train: Loss = 6.6180, preLoss = 0.7333

2022-06-16 17:49:51.043082: Epoch 14/150, Train: Loss = 5.8236, preLoss = 0.6211

2022-06-16 17:50:32.725334: Epoch 15/150, Train: Loss = 5.1214, preLoss = 0.5312
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.103 0.103 0.1906695704847484 0.2765 0.27162267845530624 0.5633
2022-06-16 17:51:23.150931: Epoch 15/150, Test: HR = 0.4002, NDCG = 0.2306
2022-06-16 17:51:25.081862: Model Saved: yelp

2022-06-16 17:52:06.355291: Epoch 16/150, Train: Loss = 4.5079, preLoss = 0.4563

2022-06-16 17:52:48.061605: Epoch 17/150, Train: Loss = 3.9570, preLoss = 0.3858

2022-06-16 17:53:29.445094: Epoch 18/150, Train: Loss = 3.4740, preLoss = 0.3273
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0763 0.0763 0.1776116915596285 0.2757 0.26437226680784154 0.5802
2022-06-16 17:54:19.935567: Epoch 18/150, Test: HR = 0.4147, NDCG = 0.2226
2022-06-16 17:54:22.120052: Model Saved: yelp

2022-06-16 17:55:03.673111: Epoch 19/150, Train: Loss = 3.0699, preLoss = 0.2954

2022-06-16 17:55:45.339329: Epoch 20/150, Train: Loss = 2.7070, preLoss = 0.2537

2022-06-16 17:56:26.701581: Epoch 21/150, Train: Loss = 2.3997, preLoss = 0.2233
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0819 0.0819 0.18441587059992345 0.2826 0.2723775275177863 0.5924
2022-06-16 17:57:16.962223: Epoch 21/150, Test: HR = 0.4215, NDCG = 0.2293
2022-06-16 17:57:18.891317: Model Saved: yelp

2022-06-16 17:58:00.529030: Epoch 22/150, Train: Loss = 2.1343, preLoss = 0.2015

2022-06-16 17:58:41.839395: Epoch 23/150, Train: Loss = 1.9011, preLoss = 0.1778

2022-06-16 17:59:23.448356: Epoch 24/150, Train: Loss = 1.7014, preLoss = 0.1613
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.0948 0.0948 0.19840415048138832 0.2967 0.287539056788509 0.6112
2022-06-16 18:00:13.754139: Epoch 24/150, Test: HR = 0.4356, NDCG = 0.2432
2022-06-16 18:00:15.526594: Model Saved: yelp

2022-06-16 18:00:56.928710: Epoch 25/150, Train: Loss = 1.5262, preLoss = 0.1464

2022-06-16 18:01:38.236405: Epoch 26/150, Train: Loss = 1.3790, preLoss = 0.1375

2022-06-16 18:02:19.485039: Epoch 27/150, Train: Loss = 1.2420, preLoss = 0.1204
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1004 0.1004 0.20831696633493524 0.3074 0.29730495116008804 0.6198
2022-06-16 18:03:09.063026: Epoch 27/150, Test: HR = 0.4526, NDCG = 0.2551
2022-06-16 18:03:10.597600: Model Saved: yelp

2022-06-16 18:03:52.397419: Epoch 28/150, Train: Loss = 1.1330, preLoss = 0.1177

2022-06-16 18:04:33.440070: Epoch 29/150, Train: Loss = 1.0326, preLoss = 0.1084

2022-06-16 18:05:14.615304: Epoch 30/150, Train: Loss = 0.9502, preLoss = 0.1040
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1098 0.1098 0.21999998557408235 0.3234 0.30843936036015485 0.6343
2022-06-16 18:06:04.630893: Epoch 30/150, Test: HR = 0.4659, NDCG = 0.2660
2022-06-16 18:06:06.258031: Model Saved: yelp

2022-06-16 18:06:47.852513: Epoch 31/150, Train: Loss = 0.8726, preLoss = 0.0959

2022-06-16 18:07:29.250133: Epoch 32/150, Train: Loss = 0.8086, preLoss = 0.0928

2022-06-16 18:08:10.658409: Epoch 33/150, Train: Loss = 0.7513, preLoss = 0.0894
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.129 0.129 0.23233067079018724 0.3339 0.3228071867312002 0.6491
2022-06-16 18:09:01.027691: Epoch 33/150, Test: HR = 0.4851, NDCG = 0.2815
2022-06-16 18:09:02.517344: Model Saved: yelp

2022-06-16 18:09:43.711128: Epoch 34/150, Train: Loss = 0.7016, preLoss = 0.0872

2022-06-16 18:10:24.612622: Epoch 35/150, Train: Loss = 0.6588, preLoss = 0.0851

2022-06-16 18:11:05.495737: Epoch 36/150, Train: Loss = 0.6216, preLoss = 0.0834
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1354 0.1354 0.243763783444299 0.3512 0.3312974329086536 0.6557
2022-06-16 18:11:55.409908: Epoch 36/150, Test: HR = 0.5014, NDCG = 0.2924
2022-06-16 18:11:56.986758: Model Saved: yelp

2022-06-16 18:12:38.862319: Epoch 37/150, Train: Loss = 0.5872, preLoss = 0.0815

2022-06-16 18:13:20.596840: Epoch 38/150, Train: Loss = 0.5564, preLoss = 0.0798

2022-06-16 18:14:01.862071: Epoch 39/150, Train: Loss = 0.5277, preLoss = 0.0768
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1456 0.1456 0.2599269789560654 0.3696 0.3476110705498782 0.6746
2022-06-16 18:14:51.947881: Epoch 39/150, Test: HR = 0.5196, NDCG = 0.3086
2022-06-16 18:14:53.620803: Model Saved: yelp

2022-06-16 18:15:35.552896: Epoch 40/150, Train: Loss = 0.5044, preLoss = 0.0762

2022-06-16 18:16:17.102150: Epoch 41/150, Train: Loss = 0.4810, preLoss = 0.0736

2022-06-16 18:16:58.622433: Epoch 42/150, Train: Loss = 0.4640, preLoss = 0.0754
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1569 0.1569 0.27791998353255787 0.3886 0.363667676052395 0.6885
2022-06-16 18:17:48.709165: Epoch 42/150, Test: HR = 0.5291, NDCG = 0.3234
2022-06-16 18:17:50.251949: Model Saved: yelp

2022-06-16 18:18:31.832650: Epoch 43/150, Train: Loss = 0.4444, preLoss = 0.0730

2022-06-16 18:19:13.082809: Epoch 44/150, Train: Loss = 0.4282, preLoss = 0.0724

2022-06-16 18:19:54.081784: Epoch 45/150, Train: Loss = 0.4125, preLoss = 0.0701
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1502 0.1502 0.2806403102882797 0.3965 0.36559046428641107 0.693
2022-06-16 18:20:44.785657: Epoch 45/150, Test: HR = 0.5352, NDCG = 0.3258
2022-06-16 18:20:46.632466: Model Saved: yelp

2022-06-16 18:21:28.052809: Epoch 46/150, Train: Loss = 0.3998, preLoss = 0.0709

2022-06-16 18:22:09.301346: Epoch 47/150, Train: Loss = 0.3871, preLoss = 0.0703

2022-06-16 18:22:50.683246: Epoch 48/150, Train: Loss = 0.3761, preLoss = 0.0700
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1588 0.1588 0.28177485279624265 0.3979 0.3673755945892038 0.6959
2022-06-16 18:23:41.050360: Epoch 48/150, Test: HR = 0.5415, NDCG = 0.3285
2022-06-16 18:23:42.648025: Model Saved: yelp

2022-06-16 18:24:24.012811: Epoch 49/150, Train: Loss = 0.3650, preLoss = 0.0691

2022-06-16 18:25:05.060473: Epoch 50/150, Train: Loss = 0.3541, preLoss = 0.0676

2022-06-16 18:25:46.401839: Epoch 51/150, Train: Loss = 0.3446, preLoss = 0.0666
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1623 0.1623 0.2793972400383897 0.394 0.36773732757913213 0.7011
2022-06-16 18:26:36.550677: Epoch 51/150, Test: HR = 0.5401, NDCG = 0.3271
2022-06-16 18:26:38.130678: Model Saved: yelp

2022-06-16 18:27:19.183490: Epoch 52/150, Train: Loss = 0.3363, preLoss = 0.0662

2022-06-16 18:28:00.625840: Epoch 53/150, Train: Loss = 0.3277, preLoss = 0.0650

2022-06-16 18:28:41.768903: Epoch 54/150, Train: Loss = 0.3223, preLoss = 0.0662
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1663 0.1663 0.2871294154571867 0.4028 0.37653846302704763 0.7131
2022-06-16 18:29:31.617262: Epoch 54/150, Test: HR = 0.5534, NDCG = 0.3362
2022-06-16 18:29:33.230200: Model Saved: yelp

2022-06-16 18:30:14.219762: Epoch 55/150, Train: Loss = 0.3144, preLoss = 0.0651

2022-06-16 18:30:55.682169: Epoch 56/150, Train: Loss = 0.3075, preLoss = 0.0643

2022-06-16 18:31:36.731982: Epoch 57/150, Train: Loss = 0.3019, preLoss = 0.0646
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1846 0.1846 0.3012687418999432 0.416 0.3879471910984909 0.7178
2022-06-16 18:32:26.384575: Epoch 57/150, Test: HR = 0.5611, NDCG = 0.3484
2022-06-16 18:32:27.978142: Model Saved: yelp

2022-06-16 18:33:09.174444: Epoch 58/150, Train: Loss = 0.2962, preLoss = 0.0640

2022-06-16 18:33:50.362604: Epoch 59/150, Train: Loss = 0.2910, preLoss = 0.0634

2022-06-16 18:34:31.421646: Epoch 60/150, Train: Loss = 0.2849, preLoss = 0.0617
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1846 0.1846 0.30296144764323374 0.4189 0.38999389682402036 0.7221
2022-06-16 18:35:21.324378: Epoch 60/150, Test: HR = 0.5626, NDCG = 0.3498
2022-06-16 18:35:22.920982: Model Saved: yelp

2022-06-16 18:36:04.209483: Epoch 61/150, Train: Loss = 0.2831, preLoss = 0.0644

2022-06-16 18:36:45.642395: Epoch 62/150, Train: Loss = 0.2773, preLoss = 0.0626

2022-06-16 18:37:26.812042: Epoch 63/150, Train: Loss = 0.2730, preLoss = 0.0620
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1779 0.1779 0.2968265437979713 0.4082 0.38818932982227344 0.7237
2022-06-16 18:38:16.166729: Epoch 63/150, Test: HR = 0.5647, NDCG = 0.3481
2022-06-16 18:38:17.702802: Model Saved: yelp

2022-06-16 18:38:59.056192: Epoch 64/150, Train: Loss = 0.2675, preLoss = 0.0604

2022-06-16 18:39:40.559075: Epoch 65/150, Train: Loss = 0.2653, preLoss = 0.0612

2022-06-16 18:40:21.775849: Epoch 66/150, Train: Loss = 0.2612, preLoss = 0.0605
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.18 0.18 0.2993555148040972 0.4137 0.3891498209902933 0.7254
2022-06-16 18:41:11.659027: Epoch 66/150, Test: HR = 0.5656, NDCG = 0.3489
2022-06-16 18:41:13.307716: Model Saved: yelp

2022-06-16 18:41:54.882765: Epoch 67/150, Train: Loss = 0.2587, preLoss = 0.0610

2022-06-16 18:42:35.859194: Epoch 68/150, Train: Loss = 0.2554, preLoss = 0.0607

2022-06-16 18:43:16.860636: Epoch 69/150, Train: Loss = 0.2517, preLoss = 0.0596
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1777 0.1777 0.29888431131662974 0.4144 0.3901723957090603 0.7301
2022-06-16 18:44:06.501985: Epoch 69/150, Test: HR = 0.5700, NDCG = 0.3498
2022-06-16 18:44:08.041339: Model Saved: yelp

2022-06-16 18:44:49.202130: Epoch 70/150, Train: Loss = 0.2502, preLoss = 0.0606

2022-06-16 18:45:30.738739: Epoch 71/150, Train: Loss = 0.2471, preLoss = 0.0596

2022-06-16 18:46:12.324734: Epoch 72/150, Train: Loss = 0.2438, preLoss = 0.0587
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1721 0.1721 0.2967408839418957 0.4197 0.38625526558813833 0.7316
2022-06-16 18:47:02.688393: Epoch 72/150, Test: HR = 0.5639, NDCG = 0.3440
2022-06-16 18:47:04.461842: Model Saved: yelp

2022-06-16 18:47:46.209581: Epoch 73/150, Train: Loss = 0.2417, preLoss = 0.0591

2022-06-16 18:48:28.040999: Epoch 74/150, Train: Loss = 0.2388, preLoss = 0.0580

2022-06-16 18:49:09.765394: Epoch 75/150, Train: Loss = 0.2376, preLoss = 0.0587
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1836 0.1836 0.3039551399673238 0.415 0.39730583423592514 0.7384
2022-06-16 18:49:59.587632: Epoch 75/150, Test: HR = 0.5699, NDCG = 0.3549
2022-06-16 18:50:01.334143: Model Saved: yelp

2022-06-16 18:50:42.418148: Epoch 76/150, Train: Loss = 0.2345, preLoss = 0.0581

2022-06-16 18:51:23.798051: Epoch 77/150, Train: Loss = 0.2323, preLoss = 0.0575

2022-06-16 18:52:05.136029: Epoch 78/150, Train: Loss = 0.2301, preLoss = 0.0567
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1841 0.1841 0.3055796090523895 0.4197 0.3991733630400322 0.7422
2022-06-16 18:52:55.444117: Epoch 78/150, Test: HR = 0.5784, NDCG = 0.3579
2022-06-16 18:52:57.084587: Model Saved: yelp

2022-06-16 18:53:38.281250: Epoch 79/150, Train: Loss = 0.2299, preLoss = 0.0578

2022-06-16 18:54:19.580921: Epoch 80/150, Train: Loss = 0.2279, preLoss = 0.0579

2022-06-16 18:55:00.755827: Epoch 81/150, Train: Loss = 0.2242, preLoss = 0.0559
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.2032 0.2032 0.313168307225259 0.4174 0.40780346450897825 0.743
2022-06-16 18:55:50.624982: Epoch 81/150, Test: HR = 0.5780, NDCG = 0.3662
2022-06-16 18:55:52.233994: Model Saved: yelp

2022-06-16 18:56:33.449071: Epoch 82/150, Train: Loss = 0.2235, preLoss = 0.0563

2022-06-16 18:57:14.398528: Epoch 83/150, Train: Loss = 0.2218, preLoss = 0.0561

2022-06-16 18:57:55.677393: Epoch 84/150, Train: Loss = 0.2213, preLoss = 0.0567
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1964 0.1964 0.30941449784435543 0.4156 0.40538518547829144 0.7452
2022-06-16 18:58:46.019366: Epoch 84/150, Test: HR = 0.5794, NDCG = 0.3636
2022-06-16 18:58:47.684806: Model Saved: yelp

2022-06-16 18:59:28.640350: Epoch 85/150, Train: Loss = 0.2190, preLoss = 0.0557

2022-06-16 19:00:09.981815: Epoch 86/150, Train: Loss = 0.2177, preLoss = 0.0555

2022-06-16 19:00:51.258842: Epoch 87/150, Train: Loss = 0.2176, preLoss = 0.0566
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1934 0.1934 0.3099040059029554 0.4211 0.4038430252273524 0.7455
2022-06-16 19:01:40.951901: Epoch 87/150, Test: HR = 0.5792, NDCG = 0.3619
2022-06-16 19:01:42.667907: Model Saved: yelp

2022-06-16 19:02:24.062596: Epoch 88/150, Train: Loss = 0.2154, preLoss = 0.0555

2022-06-16 19:03:05.353732: Epoch 89/150, Train: Loss = 0.2146, preLoss = 0.0554

2022-06-16 19:03:46.363462: Epoch 90/150, Train: Loss = 0.2134, preLoss = 0.0557
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1875 0.1875 0.30758878971779413 0.4191 0.4025324411016464 0.7462
2022-06-16 19:04:36.322772: Epoch 90/150, Test: HR = 0.5810, NDCG = 0.3609
2022-06-16 19:04:37.988247: Model Saved: yelp

2022-06-16 19:05:19.204669: Epoch 91/150, Train: Loss = 0.2111, preLoss = 0.0541

2022-06-16 19:06:00.444031: Epoch 92/150, Train: Loss = 0.2108, preLoss = 0.0548

2022-06-16 19:06:41.799517: Epoch 93/150, Train: Loss = 0.2090, preLoss = 0.0540
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1851 0.1851 0.30979433028743353 0.4268 0.40376743252904146 0.7498
2022-06-16 19:07:31.528878: Epoch 93/150, Test: HR = 0.5897, NDCG = 0.3634
2022-06-16 19:07:33.238611: Model Saved: yelp

2022-06-16 19:08:14.679442: Epoch 94/150, Train: Loss = 0.2092, preLoss = 0.0545

2022-06-16 19:08:55.842763: Epoch 95/150, Train: Loss = 0.2081, preLoss = 0.0544

2022-06-16 19:09:37.311834: Epoch 96/150, Train: Loss = 0.2075, preLoss = 0.0546
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1879 0.1879 0.3103057373617467 0.4264 0.40525081491940945 0.752
2022-06-16 19:10:27.218186: Epoch 96/150, Test: HR = 0.5922, NDCG = 0.3650
2022-06-16 19:10:28.829106: Model Saved: yelp

2022-06-16 19:11:09.944309: Epoch 97/150, Train: Loss = 0.2061, preLoss = 0.0543

2022-06-16 19:11:51.330619: Epoch 98/150, Train: Loss = 0.2049, preLoss = 0.0536

2022-06-16 19:12:32.615925: Epoch 99/150, Train: Loss = 0.2047, preLoss = 0.0542
epochNdcg1,epochHit1,epochNdcg5,epochHit5,epochNdcg20,epochHit20 0.1952 0.1952 0.31537247238099364 0.4295 0.4092195887582287 0.7516
2022-06-16 19:13:23.044412: Epoch 99/150, Test: HR = 0.5949, NDCG = 0.3697
2022-06-16 19:13:24.732062: Model Saved: yelp

2022-06-16 19:14:06.183364: Epoch 100/150, Train: Loss = 0.2033, preLoss = 0.0534

2022-06-16 19:14:47.961961: Epoch 101/150, Train: Loss = 0.2024, preLoss = 0.0532

^CTraceback (most recent call last):0: preloss = 0.06, REGLoss = 0.15
  File "main.py", line 25, in <module>
    recom.run()
  File "/root/CLSR/model.py", line 50, in run
    reses = self.trainEpoch()
  File "/root/CLSR/model.py", line 389, in trainEpoch
    uLocs, iLocs, timeLocs = self.sampleTrainBatch(batIds, self.handler.trnMat, self.handler.timeMat, sample_num_list[s])
  File "/root/CLSR/model.py", line 315, in sampleTrainBatch
    timeLocs[cur+temlen//2] = timeMat[batIds[i],negloc]
  File "/root/miniconda3/lib/python3.7/site-packages/scipy/sparse/_index.py", line 37, in __getitem__
    return self._get_intXint(row, col)
  File "/root/miniconda3/lib/python3.7/site-packages/scipy/sparse/compressed.py", line 643, in _get_intXint
    def _get_intXint(self, row, col):
KeyboardInterrupt

root@container-327e11a8ac-4a1523bb:~/CLSR#
root@container-327e11a8ac-4a1523bb:~/CLSR#
Remote side unexpectedly closed network connection

────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

Session stopped
    - Press <return> to exit tab
    - Press R to restart session
    - Press S to save terminal output to file
